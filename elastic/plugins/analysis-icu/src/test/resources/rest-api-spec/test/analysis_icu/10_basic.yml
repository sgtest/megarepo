# Integration tests for ICU analysis components
#
"Tokenizer":
    - do:
        indices.analyze:
          body:
            text:         Foo Bar
            tokenizer:    icu_tokenizer
    - length: { tokens: 2 }
    - match:  { tokens.0.token: Foo }
    - match:  { tokens.1.token: Bar }
---
"Normalization filter":
    - do:
        indices.analyze:
          body:
            filter:       [icu_normalizer]
            text:         Foo Bar Ruß
            tokenizer:    keyword
    - length: { tokens: 1 }
    - match:  { tokens.0.token: foo bar russ }
---
"Normalization charfilter":
    - do:
        indices.analyze:
          body:
            char_filter:  [icu_normalizer]
            text:         Foo Bar Ruß
            tokenizer:    keyword
    - length: { tokens: 1 }
    - match:  { tokens.0.token: foo bar russ }
---
"Folding filter":
    - do:
        indices.analyze:
          body:
            filter:       [icu_folding]
            text:         Foo Bar résumé
            tokenizer:    keyword
    - length: { tokens: 1 }
    - match:  { tokens.0.token: foo bar resume }
---
"Normalization with a UnicodeSet Filter":
    - do:
        indices.create:
            index:  test
            body:
                settings:
                    index:
                        analysis:
                            char_filter:
                                charfilter_icu_normalizer:
                                    type: icu_normalizer
                                    unicodeSetFilter: "[^ß]"
                            filter:
                                tokenfilter_icu_normalizer:
                                    type: icu_normalizer
                                    unicodeSetFilter: "[^ßB]"
                                tokenfilter_icu_folding:
                                    type: icu_folding
                                    unicodeSetFilter: "[^â]"
    - do:
        indices.analyze:
          index:    test
          body:
            char_filter: ["charfilter_icu_normalizer"]
            tokenizer:  keyword
            text:     charfilter Föo Bâr Ruß
    - length: { tokens: 1 }
    - match:  { tokens.0.token: charfilter föo bâr ruß }
    - do:
        indices.analyze:
          index:    test
          body:
            tokenizer:  keyword
            filter: ["tokenfilter_icu_normalizer"]
            text:     tokenfilter Föo Bâr Ruß
    - length: { tokens: 1 }
    - match:  { tokens.0.token: tokenfilter föo Bâr ruß }
    - do:
        indices.analyze:
          index:    test
          body:
            tokenizer:  keyword
            filter: ["tokenfilter_icu_folding"]
            text:     icufolding Föo Bâr Ruß
    - length: { tokens: 1 }
    - match:  { tokens.0.token: icufolding foo bâr russ }
