[[analysis-normalization-tokenfilter]]
=== Normalization token filters
++++
<titleabbrev>Normalization</titleabbrev>
++++

There are several token filters available which try to normalize special
characters of a certain language.

[horizontal]
Arabic::

{lucene-analysis-docs}/ar/ArabicNormalizer.html[`arabic_normalization`]

German::

{lucene-analysis-docs}/de/GermanNormalizationFilter.html[`german_normalization`]

Hindi::

{lucene-analysis-docs}/hi/HindiNormalizer.html[`hindi_normalization`]

Indic::

{lucene-analysis-docs}/in/IndicNormalizer.html[`indic_normalization`]

Kurdish (Sorani)::

{lucene-analysis-docs}/ckb/SoraniNormalizer.html[`sorani_normalization`]

Persian::

{lucene-analysis-docs}/fa/PersianNormalizer.html[`persian_normalization`]

Scandinavian::

{lucene-analysis-docs}/miscellaneous/ScandinavianNormalizationFilter.html[`scandinavian_normalization`],
{lucene-analysis-docs}/miscellaneous/ScandinavianFoldingFilter.html[`scandinavian_folding`]

Serbian::

{lucene-analysis-docs}/sr/SerbianNormalizationFilter.html[`serbian_normalization`]

