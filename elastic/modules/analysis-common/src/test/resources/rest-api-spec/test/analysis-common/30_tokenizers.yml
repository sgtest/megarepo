## Smoke tests for tokenizers included in the analysis-common module

"keyword":
    - do:
        indices.analyze:
          body:
            text:      Foo Bar!
            tokenizer: keyword
    - length:    { tokens: 1 }
    - match:     { tokens.0.token: Foo Bar! }

---
"nGram":
    - do:
        indices.analyze:
          body:
            text: good
            explain: true
            tokenizer:
              type: nGram
              min_gram: 2
              max_gram: 2
    - length: { detail.tokenizer.tokens: 3 }
    - match:  { detail.tokenizer.name: _anonymous_tokenizer }
    - match:  { detail.tokenizer.tokens.0.token: go }
    - match:  { detail.tokenizer.tokens.1.token: oo }
    - match:  { detail.tokenizer.tokens.2.token: od }

---
"nGram_exception":
    - skip:
        version: " - 6.99.99"
        reason: only starting from version 7.x this throws an error
    - do:
        catch: /The difference between max_gram and min_gram in NGram Tokenizer must be less than or equal to[:] \[1\] but was \[2\]\. This limit can be set by changing the \[index.max_ngram_diff\] index level setting\./
        indices.analyze:
          body:
            text: good
            explain: true
            tokenizer:
              type: nGram
              min_gram: 2
              max_gram: 4
---
"simple_pattern":
    - do:
        indices.analyze:
          body:
            text: "a6bf fooo ff61"
            explain: true
            tokenizer:
              type: simple_pattern
              pattern: "[abcdef0123456789]{4}"
    - length: { detail.tokenizer.tokens: 2 }
    - match:  { detail.tokenizer.name: _anonymous_tokenizer }
    - match:  { detail.tokenizer.tokens.0.token: a6bf }
    - match:  { detail.tokenizer.tokens.1.token: ff61 }

---
"simple_pattern_split":
    - do:
        indices.analyze:
          body:
            text: "foo==bar"
            explain: true
            tokenizer:
              type: simple_pattern_split
              pattern: ==
    - length: { detail.tokenizer.tokens: 2 }
    - match:  { detail.tokenizer.name: _anonymous_tokenizer }
    - match:  { detail.tokenizer.tokens.0.token: foo }
    - match:  { detail.tokenizer.tokens.1.token: bar }
