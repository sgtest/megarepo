// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb | FileCheck %s --check-prefixes=CHECK,CHECK-%{PTX}

HloModule m, is_scheduled=true

add {
  a = f64[] parameter(0)
  b = f64[] parameter(1)
  ROOT out = f64[] add(a, b)
}

fused_computation {
  p1 = f64[1024,1024]{1,0} parameter(0)
  p2 = f64[1024,1024]{1,0} parameter(1)
  s = pred[1024,1024]{1,0} parameter(2)
  p = f64[1024,1024]{1,0} select(s, p1, p2)
  z = f64[] constant(0)
  ROOT out = f64[1024]{0} reduce(p, z), to_apply=add, dimensions={0}
}

ENTRY e {
  p1 = f64[1024,1024]{1,0} parameter(0)
  p2 = f64[1024,1024]{1,0} parameter(1)
  s = pred[1024,1024]{1,0} parameter(2)
  ROOT f = f64[1024]{0} fusion(p1, p2, s), kind=kInput, calls=fused_computation
}

// CHECK: @shared_cache = private addrspace(3) global [32 x [33 x double]]

// CHECK-LABEL: entry:
// CHECK:         %[[VAL_0:.*]] = alloca double, align 8
// CHECK:         %[[VAL_1:.*]] = alloca double, align 8
// CHECK:         %[[VAL_2:.*]] = alloca double, align 8
// CHECK:         %[[VAL_3:.*]] = alloca double, align 8
// CHECK:         %[[VAL_4:.*]] = alloca double, align 8
// CHECK:         %[[VAL_5:.*]] = alloca double, align 8
// CHECK:         %[[VAL_6:.*]] = alloca double, align 8
// CHECK:         %[[VAL_7:.*]] = alloca double, align 8
// CHECK:         %[[VAL_8:.*]] = alloca double, align 8
// CHECK:         %[[VAL_9:.*]] = alloca double, align 8
// CHECK:         %[[VAL_10:.*]] = alloca double, align 8
// CHECK:         %[[VAL_11:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_12:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_13:.*]] = alloca double, align 8
// CHECK:         %[[VAL_14:.*]] = alloca double, align 8
// CHECK-PTX:     %[[VAL_15:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(), !range !2
// CHECK-GCN:     %[[VAL_15:.*]] = call i32 @llvm.amdgcn.workgroup.id.y
// CHECK:         %[[VAL_16:.*]] = icmp eq i32 %[[VAL_15]], 0
// CHECK:         br i1 %[[VAL_16]], label %[[VAL_17:.*]], label %[[VAL_18:.*]]
// CHECK:       reduce-group-0-after:                             ; preds = %[[VAL_19:.*]], %[[VAL_20:.*]]
// CHECK:         ret void
// CHECK:       reduce-group-0-true:                              ; preds = %[[VAL_20]]
// CHECK:         %[[VAL_21:.*]] = load double, ptr addrspace(1) @0, align 8
// CHECK:         store double %[[VAL_21]], ptr %[[VAL_13]], align 8
// CHECK-PTX:     %[[VAL_22:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !3
// CHECK-GCN:     %[[VAL_22:.*]] = call i32 @llvm.amdgcn.workitem.id.x
// CHECK-PTX:     %[[VAL_23:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !4
// CHECK-GCN:     %[[VAL_23:.*]] = call i32 @llvm.amdgcn.workgroup.id.x
// CHECK:         %[[VAL_24:.*]] = udiv i32 %[[VAL_22]], 1024
// CHECK:         %[[VAL_25:.*]] = urem i32 %[[VAL_22]], 1024
// CHECK:         %[[VAL_26:.*]] = udiv i32 %[[VAL_25]], 32
// CHECK:         %[[VAL_27:.*]] = urem i32 %[[VAL_22]], 32
// CHECK:         %[[VAL_28:.*]] = mul i32 %[[VAL_27]], 1
// CHECK:         %[[VAL_29:.*]] = urem i32 %[[VAL_22]], 32
// CHECK:         %[[VAL_30:.*]] = udiv i32 %[[VAL_23]], 1
// CHECK:         %[[VAL_31:.*]] = urem i32 %[[VAL_30]], 32
// CHECK:         %[[VAL_32:.*]] = udiv i32 %[[VAL_23]], 32
// CHECK:         %[[VAL_33:.*]] = urem i32 %[[VAL_32]], 1
// CHECK:         %[[VAL_34:.*]] = udiv i32 %[[VAL_23]], 32
// CHECK:         %[[VAL_35:.*]] = icmp eq i32 %[[VAL_33]], 0
// CHECK:         %[[VAL_36:.*]] = select i1 %[[VAL_35]], i32 1024, i32 4096
// CHECK:         %[[VAL_37:.*]] = mul i32 %[[VAL_34]], 1
// CHECK:         %[[VAL_38:.*]] = mul i32 %[[VAL_33]], 4096
// CHECK:         %[[VAL_39:.*]] = mul i32 %[[VAL_31]], 32
// CHECK:         store i32 %[[VAL_26]], ptr %[[VAL_12]], align 4
// CHECK:         br label %[[VAL_40:.*]]
// CHECK:       loop1.loop_header:                                ; preds = %[[VAL_41:.*]], %[[VAL_17]]
// CHECK:         %[[VAL_42:.*]] = load i32, ptr %[[VAL_12]], align 4
// CHECK:         %[[VAL_43:.*]] = icmp uge i32 %[[VAL_42]], %[[VAL_36]]
// CHECK:         br i1 %[[VAL_43]], label %[[VAL_44:.*]], label %[[VAL_45:.*]]
// CHECK:       loop1.loop_body:                                  ; preds = %[[VAL_40]]
// CHECK:         %[[VAL_46:.*]] = add nuw nsw i32 %[[VAL_42]], 32
// CHECK:         store i32 %[[VAL_46]], ptr %[[VAL_12]], align 4
// CHECK:         %[[VAL_47:.*]] = icmp eq i32 %[[VAL_42]], %[[VAL_26]]
// CHECK:         store i32 0, ptr %[[VAL_11]], align 4
// CHECK:         br label %[[VAL_48:.*]]
// CHECK:       loop2.loop_header:                                ; preds = %[[VAL_49:.*]], %[[VAL_45]]
// CHECK:         %[[VAL_50:.*]] = load i32, ptr %[[VAL_11]], align 4
// CHECK:         %[[VAL_51:.*]] = icmp uge i32 %[[VAL_50]], 1
// CHECK:         br i1 %[[VAL_51]], label %[[VAL_41]], label %[[VAL_52:.*]]
// CHECK:       loop2.loop_body:                                  ; preds = %[[VAL_48]]
// CHECK:         %[[VAL_53:.*]] = add nuw nsw i32 %[[VAL_50]], 1
// CHECK:         store i32 %[[VAL_53]], ptr %[[VAL_11]], align 4
// CHECK:         %[[VAL_54:.*]] = icmp eq i32 %[[VAL_50]], 0
// CHECK:         %[[VAL_55:.*]] = mul i32 %[[VAL_50]], 32
// CHECK:         %[[VAL_56:.*]] = add i32 %[[VAL_55]], 0
// CHECK:         %[[VAL_57:.*]] = add i32 %[[VAL_56]], %[[VAL_28]]
// CHECK:         %[[VAL_58:.*]] = icmp ult i32 %[[VAL_57]], 32
// CHECK:         br i1 %[[VAL_58]], label %[[VAL_59:.*]], label %[[VAL_49]]
// CHECK:       x_in_tile-after:                                  ; preds = %[[VAL_59]], %[[VAL_52]]
// CHECK:         br label %[[VAL_48]], !llvm.loop !5
// CHECK:       loop2.loop_exit:                                  ; preds = %[[VAL_48]]
// CHECK:         br label %[[VAL_40]], !llvm.loop !8
// CHECK:       loop1.loop_exit:                                  ; preds = %[[VAL_40]]
// CHECK:         %[[VAL_60:.*]] = load double, ptr %[[VAL_13]], align 8
// CHECK:         %[[VAL_61:.*]] = getelementptr inbounds [32 x [33 x double]], ptr addrspace(3) @shared_cache, i32 0, i32 %[[VAL_27]], i32 %[[VAL_26]]
// CHECK:         %[[VAL_62:.*]] = addrspacecast ptr addrspace(3) %[[VAL_61]] to ptr
// CHECK:         store double %[[VAL_60]], ptr %[[VAL_62]], align 8
// CHECK:         call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_63:.*]] = getelementptr inbounds [32 x [33 x double]], ptr addrspace(3) @shared_cache, i32 0, i32 %[[VAL_26]], i32 %[[VAL_27]]
// CHECK:         %[[VAL_64:.*]] = addrspacecast ptr addrspace(3) %[[VAL_63]] to ptr
// CHECK:         %[[VAL_65:.*]] = load double, ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_66:.*]] = bitcast double %[[VAL_65]] to i64
// CHECK:         %[[VAL_67:.*]] = bitcast i64 %[[VAL_66]] to <2 x i32>
// CHECK:         %[[VAL_68:.*]] = extractelement <2 x i32> %[[VAL_67]], i64 0
// CHECK:         %[[VAL_69:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_68]], i32 16, i32 31)
// CHECK:         %[[VAL_70:.*]] = insertelement <2 x i32> %[[VAL_67]], i32 %[[VAL_69]], i64 0
// CHECK:         %[[VAL_71:.*]] = extractelement <2 x i32> %[[VAL_70]], i64 1
// CHECK:         %[[VAL_72:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_71]], i32 16, i32 31)
// CHECK:         %[[VAL_73:.*]] = insertelement <2 x i32> %[[VAL_70]], i32 %[[VAL_72]], i64 1
// CHECK:         %[[VAL_74:.*]] = bitcast <2 x i32> %[[VAL_73]] to i64
// CHECK:         %[[VAL_75:.*]] = bitcast i64 %[[VAL_74]] to double
// CHECK:         store double %[[VAL_75]], ptr %[[VAL_9]], align 8
// CHECK:         call void @[[ADD:add.*]](ptr %[[VAL_64]], ptr %[[VAL_9]], ptr %[[VAL_8]])
// CHECK:         %[[VAL_76:.*]] = load double, ptr %[[VAL_8]], align 8
// CHECK:         store double %[[VAL_76]], ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_77:.*]] = load double, ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_78:.*]] = bitcast double %[[VAL_77]] to i64
// CHECK:         %[[VAL_79:.*]] = bitcast i64 %[[VAL_78]] to <2 x i32>
// CHECK:         %[[VAL_80:.*]] = extractelement <2 x i32> %[[VAL_79]], i64 0
// CHECK:         %[[VAL_81:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_80]], i32 8, i32 31)
// CHECK:         %[[VAL_82:.*]] = insertelement <2 x i32> %[[VAL_79]], i32 %[[VAL_81]], i64 0
// CHECK:         %[[VAL_83:.*]] = extractelement <2 x i32> %[[VAL_82]], i64 1
// CHECK:         %[[VAL_84:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_83]], i32 8, i32 31)
// CHECK:         %[[VAL_85:.*]] = insertelement <2 x i32> %[[VAL_82]], i32 %[[VAL_84]], i64 1
// CHECK:         %[[VAL_86:.*]] = bitcast <2 x i32> %[[VAL_85]] to i64
// CHECK:         %[[VAL_87:.*]] = bitcast i64 %[[VAL_86]] to double
// CHECK:         store double %[[VAL_87]], ptr %[[VAL_7]], align 8
// CHECK:         call void @[[ADD]](ptr %[[VAL_64]], ptr %[[VAL_7]], ptr %[[VAL_6]])
// CHECK:         %[[VAL_88:.*]] = load double, ptr %[[VAL_6]], align 8
// CHECK:         store double %[[VAL_88]], ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_89:.*]] = load double, ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_90:.*]] = bitcast double %[[VAL_89]] to i64
// CHECK:         %[[VAL_91:.*]] = bitcast i64 %[[VAL_90]] to <2 x i32>
// CHECK:         %[[VAL_92:.*]] = extractelement <2 x i32> %[[VAL_91]], i64 0
// CHECK:         %[[VAL_93:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_92]], i32 4, i32 31)
// CHECK:         %[[VAL_94:.*]] = insertelement <2 x i32> %[[VAL_91]], i32 %[[VAL_93]], i64 0
// CHECK:         %[[VAL_95:.*]] = extractelement <2 x i32> %[[VAL_94]], i64 1
// CHECK:         %[[VAL_96:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_95]], i32 4, i32 31)
// CHECK:         %[[VAL_97:.*]] = insertelement <2 x i32> %[[VAL_94]], i32 %[[VAL_96]], i64 1
// CHECK:         %[[VAL_98:.*]] = bitcast <2 x i32> %[[VAL_97]] to i64
// CHECK:         %[[VAL_99:.*]] = bitcast i64 %[[VAL_98]] to double
// CHECK:         store double %[[VAL_99]], ptr %[[VAL_5]], align 8
// CHECK:         call void @[[ADD]](ptr %[[VAL_64]], ptr %[[VAL_5]], ptr %[[VAL_4]])
// CHECK:         %[[VAL_100:.*]] = load double, ptr %[[VAL_4]], align 8
// CHECK:         store double %[[VAL_100]], ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_101:.*]] = load double, ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_102:.*]] = bitcast double %[[VAL_101]] to i64
// CHECK:         %[[VAL_103:.*]] = bitcast i64 %[[VAL_102]] to <2 x i32>
// CHECK:         %[[VAL_104:.*]] = extractelement <2 x i32> %[[VAL_103]], i64 0
// CHECK:         %[[VAL_105:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_104]], i32 2, i32 31)
// CHECK:         %[[VAL_106:.*]] = insertelement <2 x i32> %[[VAL_103]], i32 %[[VAL_105]], i64 0
// CHECK:         %[[VAL_107:.*]] = extractelement <2 x i32> %[[VAL_106]], i64 1
// CHECK:         %[[VAL_108:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_107]], i32 2, i32 31)
// CHECK:         %[[VAL_109:.*]] = insertelement <2 x i32> %[[VAL_106]], i32 %[[VAL_108]], i64 1
// CHECK:         %[[VAL_110:.*]] = bitcast <2 x i32> %[[VAL_109]] to i64
// CHECK:         %[[VAL_111:.*]] = bitcast i64 %[[VAL_110]] to double
// CHECK:         store double %[[VAL_111]], ptr %[[VAL_3]], align 8
// CHECK:         call void @[[ADD]](ptr %[[VAL_64]], ptr %[[VAL_3]], ptr %[[VAL_2]])
// CHECK:         %[[VAL_112:.*]] = load double, ptr %[[VAL_2]], align 8
// CHECK:         store double %[[VAL_112]], ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_113:.*]] = load double, ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_114:.*]] = bitcast double %[[VAL_113]] to i64
// CHECK:         %[[VAL_115:.*]] = bitcast i64 %[[VAL_114]] to <2 x i32>
// CHECK:         %[[VAL_116:.*]] = extractelement <2 x i32> %[[VAL_115]], i64 0
// CHECK:         %[[VAL_117:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_116]], i32 1, i32 31)
// CHECK:         %[[VAL_118:.*]] = insertelement <2 x i32> %[[VAL_115]], i32 %[[VAL_117]], i64 0
// CHECK:         %[[VAL_119:.*]] = extractelement <2 x i32> %[[VAL_118]], i64 1
// CHECK:         %[[VAL_120:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_119]], i32 1, i32 31)
// CHECK:         %[[VAL_121:.*]] = insertelement <2 x i32> %[[VAL_118]], i32 %[[VAL_120]], i64 1
// CHECK:         %[[VAL_122:.*]] = bitcast <2 x i32> %[[VAL_121]] to i64
// CHECK:         %[[VAL_123:.*]] = bitcast i64 %[[VAL_122]] to double
// CHECK:         store double %[[VAL_123]], ptr %[[VAL_1]], align 8
// CHECK:         call void @[[ADD]](ptr %[[VAL_64]], ptr %[[VAL_1]], ptr %[[VAL_0]])
// CHECK:         %[[VAL_124:.*]] = load double, ptr %[[VAL_0]], align 8
// CHECK:         store double %[[VAL_124]], ptr %[[VAL_64]], align 8
// CHECK:         %[[VAL_126:.*]] = icmp ult i32 %[[VAL_26]], 32
// CHECK:         %[[VAL_127:.*]] = icmp ult i32 %[[VAL_27]], %[[VAL_36]]
// CHECK:         %[[VAL_128:.*]] = and i1 %[[VAL_126]], %[[VAL_127]]
// CHECK:         %[[VAL_130:.*]] = icmp eq i32 %[[VAL_29]], 0
// CHECK:         %[[VAL_131:.*]] = and i1 %[[VAL_128]], %[[VAL_130]]
// CHECK:         br i1 %[[VAL_131]], label %[[VAL_132:.*]], label %[[VAL_19]]
// CHECK:       reduction_write_output-after:                     ; preds = %[[VAL_132]], %[[VAL_44]]
// CHECK:         br label %[[VAL_18]]
// CHECK:       x_in_tile-true:                                   ; preds = %[[VAL_52]]
// CHECK:         %[[VAL_133:.*]] = add i32 %[[VAL_37]], 0
// CHECK:         %[[VAL_134:.*]] = add i32 %[[VAL_38]], %[[VAL_42]]
// CHECK:         %[[VAL_135:.*]] = add i32 %[[VAL_39]], %[[VAL_57]]
// CHECK:         %[[VAL_137:.*]] = getelementptr inbounds [1024 x [1024 x i8]], ptr %[[VAL_138:.*]], i32 0, i32 %[[VAL_134]], i32 %[[VAL_135]]
// CHECK:         %[[VAL_139:.*]] = load i8, ptr %[[VAL_137]], align 1, !invariant.load !9
// CHECK:         %[[VAL_140:.*]] = getelementptr inbounds [1024 x [1024 x double]], ptr %[[VAL_141:.*]], i32 0, i32 %[[VAL_134]], i32 %[[VAL_135]]
// CHECK:         %[[VAL_142:.*]] = load double, ptr %[[VAL_140]], align 8, !invariant.load !9
// CHECK:         %[[VAL_143:.*]] = getelementptr inbounds [1024 x [1024 x double]], ptr %[[VAL_144:.*]], i32 0, i32 %[[VAL_134]], i32 %[[VAL_135]]
// CHECK:         %[[VAL_145:.*]] = load double, ptr %[[VAL_143]], align 8, !invariant.load !9
// CHECK:         %[[VAL_146:.*]] = trunc i8 %[[VAL_139]] to i1
// CHECK:         %[[VAL_147:.*]] = select i1 %[[VAL_146]], double %[[VAL_142]], double %[[VAL_145]]
// CHECK:         store double %[[VAL_147]], ptr %[[VAL_14]], align 8
// CHECK:         call void @[[ADD]](ptr %[[VAL_13]], ptr %[[VAL_14]], ptr %[[VAL_10]])
// CHECK:         %[[VAL_149:.*]] = load double, ptr %[[VAL_10]], align 8
// CHECK:         store double %[[VAL_149]], ptr %[[VAL_13]], align 8
// CHECK:         br label %[[VAL_49]]
// CHECK:       reduction_write_output-true:                      ; preds = %[[VAL_44]]
// CHECK:         %[[VAL_150:.*]] = add i32 %[[VAL_37]], %[[VAL_24]]
// CHECK:         %[[VAL_151:.*]] = add i32 %[[VAL_38]], %[[VAL_28]]
// CHECK:         %[[VAL_152:.*]] = add i32 %[[VAL_39]], %[[VAL_26]]
// CHECK:         %[[VAL_153:.*]] = mul i32 %[[VAL_150]], 1024
// CHECK:         %[[VAL_154:.*]] = add i32 %[[VAL_153]], %[[VAL_152]]
// CHECK:         %[[VAL_155:.*]] = udiv i32 %[[VAL_154]], 1
// CHECK:         %[[VAL_156:.*]] = getelementptr inbounds [1024 x double], ptr %[[VAL_157:.*]], i32 0, i32 %[[VAL_155]]
// CHECK:         %[[VAL_158:.*]] = load double, ptr %[[VAL_64]], align 8
// CHECK:         store double %[[VAL_158]], ptr %[[VAL_156]], align 8
// CHECK:         br label %[[VAL_19]]
// CHECK:       entry:
// CHECK:         %[[VAL_159:.*]] = alloca double, align 8
// CHECK:         %[[VAL_160:.*]] = load double, ptr %[[VAL_161:.*]], align 8
// CHECK:         %[[VAL_162:.*]] = load double, ptr %[[VAL_163:.*]], align 8
// CHECK:         %[[VAL_164:.*]] = fadd double %[[VAL_160]], %[[VAL_162]]
// CHECK:         store double %[[VAL_164]], ptr %[[VAL_159]], align 8
// CHECK:         %[[VAL_165:.*]] = load double, ptr %[[VAL_159]], align 8
// CHECK:         store double %[[VAL_165]], ptr %[[VAL_166:.*]], align 8
// CHECK:         ret void

// CHECK-PTX: !3 = !{i32 0, i32 1024}
// CHECK-PTX: !4 = !{i32 0, i32 32}
