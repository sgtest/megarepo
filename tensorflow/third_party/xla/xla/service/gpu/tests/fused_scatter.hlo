// RUN: hlo_to_llvm_ir %s | FileCheck %{IR_SUBST} %s

// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// CHECK-LABEL: entry:
// CHECK:         %[[VAL_0:.*]] = call i32 [[CTAIDX]]
// CHECK:         %[[VAL_1:.*]] = call i32 [[TIDX]]
// CHECK:         %[[VAL_2:.*]] = mul nuw nsw i32 %[[VAL_0]], 2
// CHECK:         %[[VAL_3:.*]] = add nuw nsw i32 %[[VAL_2]], %[[VAL_1]]
// CHECK:         %[[VAL_4:.*]] = icmp ult i32 %[[VAL_3]], 2
// CHECK:         call void @llvm.assume(i1 %[[VAL_4]])
// CHECK:         %[[VAL_5:.*]] = add nuw nsw i32 %[[VAL_3]], 0
// CHECK:         %[[VAL_6:.*]] = udiv i32 %[[VAL_5]], 1
// CHECK:         %[[VAL_7:.*]] = icmp ult i32 %[[VAL_3]], 2
// CHECK:         br i1 %[[VAL_7]], label %[[VAL_8:.*]], label %[[VAL_9:.*]]
// CHECK:       wrapped_indices.in_bounds-after:                  ; preds = %[[VAL_8]], %[[VAL_10:.*]]
// CHECK:         ret void
// CHECK:       wrapped_indices.in_bounds-true:                   ; preds = %[[VAL_10]]
// CHECK:         %[[VAL_11:.*]] = getelementptr i32, ptr %[[VAL_12:.*]], i32 %[[VAL_3]]
// CHECK:         %[[VAL_13:.*]] = getelementptr inbounds i32, ptr %[[VAL_11]], i32 0
// CHECK:         %[[VAL_14:.*]] = load i32, ptr %[[VAL_13]], align 4, !invariant.load
// CHECK:         %[[VAL_15:.*]] = getelementptr i32, ptr %[[VAL_12]], i32 %[[VAL_3]]
// CHECK:         %[[VAL_16:.*]] = getelementptr inbounds i32, ptr %[[VAL_15]], i32 0
// CHECK:         %[[VAL_17:.*]] = load i32, ptr %[[VAL_16]], align 4, !invariant.load
// CHECK:         %[[VAL_18:.*]] = add i32 %[[VAL_14]], %[[VAL_17]]
// CHECK:         %[[VAL_19:.*]] = getelementptr i32, ptr %[[VAL_20:.*]], i32 %[[VAL_3]]
// CHECK:         %[[VAL_21:.*]] = getelementptr inbounds i32, ptr %[[VAL_19]], i32 0
// CHECK:         store i32 %[[VAL_18]], ptr %[[VAL_21]], align 4
// CHECK:         br label %[[VAL_9]]
// CHECK:       entry:
// CHECK:         %[[VAL_22:.*]] = call i32 [[CTAIDX]]
// CHECK:         %[[VAL_23:.*]] = call i32 [[TIDX]]
// CHECK:         %[[VAL_24:.*]] = mul nuw nsw i32 %[[VAL_22]], 6
// CHECK:         %[[VAL_25:.*]] = add nuw nsw i32 %[[VAL_24]], %[[VAL_23]]
// CHECK:         %[[VAL_26:.*]] = icmp ult i32 %[[VAL_25]], 6
// CHECK:         call void @llvm.assume(i1 %[[VAL_26]])
// CHECK:         %[[VAL_27:.*]] = add nuw nsw i32 %[[VAL_25]], 0
// CHECK:         %[[VAL_28:.*]] = udiv i32 %[[VAL_27]], 1
// CHECK:         %[[VAL_29:.*]] = urem i32 %[[VAL_28]], 3
// CHECK:         %[[VAL_30:.*]] = udiv i32 %[[VAL_27]], 3
// CHECK:         %[[VAL_31:.*]] = icmp ult i32 %[[VAL_25]], 6
// CHECK:         br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK:       wrapped_updates.in_bounds-after:                  ; preds = %[[VAL_32]], %[[VAL_34:.*]]
// CHECK:         ret void
// CHECK:       wrapped_updates.in_bounds-true:                   ; preds = %[[VAL_34]]
// CHECK:         %[[VAL_35:.*]] = getelementptr i32, ptr %[[VAL_36:.*]], i32 %[[VAL_25]]
// CHECK:         %[[VAL_37:.*]] = getelementptr inbounds i32, ptr %[[VAL_35]], i32 0
// CHECK:         %[[VAL_38:.*]] = load i32, ptr %[[VAL_37]], align 4, !invariant.load
// CHECK:         %[[VAL_39:.*]] = getelementptr i32, ptr %[[VAL_36]], i32 %[[VAL_25]]
// CHECK:         %[[VAL_40:.*]] = getelementptr inbounds i32, ptr %[[VAL_39]], i32 0
// CHECK:         %[[VAL_41:.*]] = load i32, ptr %[[VAL_40]], align 4, !invariant.load
// CHECK:         %[[VAL_42:.*]] = add i32 %[[VAL_38]], %[[VAL_41]]
// CHECK:         %[[VAL_43:.*]] = getelementptr i32, ptr %[[VAL_44:.*]], i32 %[[VAL_25]]
// CHECK:         %[[VAL_45:.*]] = getelementptr inbounds i32, ptr %[[VAL_43]], i32 0
// CHECK:         store i32 %[[VAL_42]], ptr %[[VAL_45]], align 4
// CHECK:         br label %[[VAL_33]]
// CHECK:       entry:
// CHECK:         %[[VAL_46:.*]] = call i32 [[CTAIDX]]
// CHECK:         %[[VAL_47:.*]] = call i32 [[TIDX]]
// CHECK:         %[[VAL_48:.*]] = mul nuw nsw i32 %[[VAL_46]], 9
// CHECK:         %[[VAL_49:.*]] = add nuw nsw i32 %[[VAL_48]], %[[VAL_47]]
// CHECK:         %[[VAL_50:.*]] = icmp ult i32 %[[VAL_49]], 9
// CHECK:         call void @llvm.assume(i1 %[[VAL_50]])
// CHECK:         %[[VAL_51:.*]] = add nuw nsw i32 %[[VAL_49]], 0
// CHECK:         %[[VAL_52:.*]] = udiv i32 %[[VAL_51]], 1
// CHECK:         %[[VAL_53:.*]] = urem i32 %[[VAL_52]], 3
// CHECK:         %[[VAL_54:.*]] = udiv i32 %[[VAL_51]], 3
// CHECK:         %[[VAL_55:.*]] = icmp ult i32 %[[VAL_49]], 9
// CHECK:         br i1 %[[VAL_55]], label %[[VAL_56:.*]], label %[[VAL_57:.*]]
// CHECK:       wrapped_operand.in_bounds-after:                  ; preds = %[[VAL_56]], %[[VAL_58:.*]]
// CHECK:         ret void
// CHECK:       wrapped_operand.in_bounds-true:                   ; preds = %[[VAL_58]]
// CHECK:         %[[VAL_59:.*]] = getelementptr i32, ptr %[[VAL_60:.*]], i32 %[[VAL_49]]
// CHECK:         %[[VAL_61:.*]] = getelementptr inbounds i32, ptr %[[VAL_59]], i32 0
// CHECK:         %[[VAL_62:.*]] = load i32, ptr %[[VAL_61]], align 4, !invariant.load
// CHECK:         %[[VAL_63:.*]] = getelementptr i32, ptr %[[VAL_60]], i32 %[[VAL_49]]
// CHECK:         %[[VAL_64:.*]] = getelementptr inbounds i32, ptr %[[VAL_63]], i32 0
// CHECK:         %[[VAL_65:.*]] = load i32, ptr %[[VAL_64]], align 4, !invariant.load
// CHECK:         %[[VAL_66:.*]] = add i32 %[[VAL_62]], %[[VAL_65]]
// CHECK:         %[[VAL_67:.*]] = getelementptr i32, ptr %[[VAL_68:.*]], i32 %[[VAL_49]]
// CHECK:         %[[VAL_69:.*]] = getelementptr inbounds i32, ptr %[[VAL_67]], i32 0
// CHECK:         store i32 %[[VAL_66]], ptr %[[VAL_69]], align 4
// CHECK:         br label %[[VAL_57]]
// CHECK:       entry:
// CHECK:         %[[VAL_70:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_71:.*]] = call i32 [[CTAIDX]]
// CHECK:         %[[VAL_72:.*]] = call i32 [[TIDX]]
// CHECK:         %[[VAL_73:.*]] = mul nuw nsw i32 %[[VAL_71]], 6
// CHECK:         %[[VAL_74:.*]] = add nuw nsw i32 %[[VAL_73]], %[[VAL_72]]
// CHECK:         %[[VAL_75:.*]] = icmp ult i32 %[[VAL_74]], 6
// CHECK:         call void @llvm.assume(i1 %[[VAL_75]])
// CHECK:         %[[VAL_76:.*]] = add nuw nsw i32 %[[VAL_74]], 0
// CHECK:         %[[VAL_77:.*]] = udiv i32 %[[VAL_76]], 1
// CHECK:         %[[VAL_78:.*]] = urem i32 %[[VAL_77]], 3
// CHECK:         %[[VAL_79:.*]] = udiv i32 %[[VAL_76]], 3
// CHECK:         %[[VAL_80:.*]] = icmp ult i32 %[[VAL_74]], 6
// CHECK:         br i1 %[[VAL_80]], label %[[VAL_81:.*]], label %[[VAL_82:.*]]
// CHECK:       scatter.in_bounds-after:                          ; preds = %[[VAL_83:.*]], %[[VAL_84:.*]]
// CHECK:         ret void
// CHECK:       scatter.in_bounds-true:                           ; preds = %[[VAL_84]]
// CHECK:         %[[VAL_85:.*]] = getelementptr inbounds [2 x i32], ptr %[[VAL_86:.*]], i32 0, i32 %[[VAL_79]]
// CHECK:         %[[VAL_87:.*]] = load i32, ptr %[[VAL_85]], align 4, !invariant.load
// CHECK:         %[[VAL_88:.*]] = add i32 0, %[[VAL_87]]
// CHECK:         %[[VAL_89:.*]] = icmp ult i32 %[[VAL_87]], 3
// CHECK:         %[[VAL_90:.*]] = and i1 true, %[[VAL_89]]
// CHECK:         br i1 %[[VAL_90]], label %[[VAL_91:.*]], label %[[VAL_83]]
// CHECK:       scatter.in_bounds-after3:                         ; preds = %[[VAL_91]], %[[VAL_81]]
// CHECK:         br label %[[VAL_82]]
// CHECK:       scatter.in_bounds-true2:                          ; preds = %[[VAL_81]]
// CHECK:         %[[VAL_92:.*]] = getelementptr inbounds [3 x [3 x i32]], ptr %[[VAL_93:.*]], i32 0, i32 %[[VAL_88]], i32 %[[VAL_78]]
// CHECK:         %[[VAL_94:.*]] = getelementptr i32, ptr %[[VAL_95:.*]], i32 %[[VAL_74]]
// CHECK:         %[[VAL_96:.*]] = getelementptr inbounds i32, ptr %[[VAL_94]], i32 0
// CHECK:         %[[VAL_97:.*]] = load i32, ptr %[[VAL_96]], align 4, !invariant.load
// CHECK:         store i32 %[[VAL_97]], ptr [[ADDRSPACE_ANNOTATION]]%[[VAL_70]], align 4
// CHECK:         %[[VAL_98:.*]] = load i32, ptr [[ADDRSPACE_ANNOTATION]]%[[VAL_70]], align 4
// CHECK:         store atomic i32 %[[VAL_98]], ptr %[[VAL_92]] unordered, align 4
// CHECK:         br label %[[VAL_83]]

HloModule TensorFlowScatterV1, is_scheduled=true

update_s32 (lhs: s32[], rhs: s32[]) -> s32[] {
  lhs = s32[] parameter(0)
  ROOT rhs = s32[] parameter(1)
}

fused_computation {
  param_0 = s32[3,3]{1,0} parameter(0)
  ROOT operand.1 = s32[3,3]{1,0} add(s32[3,3]{1,0} param_0, s32[3,3]{1,0} param_0)
}

fused_computation.1 {
  param_0.1 = s32[2]{0} parameter(0)
  ROOT indices.1 = s32[2]{0} add(s32[2]{0} param_0.1, s32[2]{0} param_0.1)
}

fused_computation.2 {
  param_0.2 = s32[2,3]{1,0} parameter(0)
  ROOT updates.1 = s32[2,3]{1,0} add(s32[2,3]{1,0} param_0.2, s32[2,3]{1,0} param_0.2)
}

fused_computation.3 {
  operand = s32[3,3]{1,0} parameter(0)
  indices = s32[2]{0} parameter(1)
  updates = s32[2,3]{1,0} parameter(2)
  ROOT scatter = s32[3,3] scatter(operand, indices, updates),
      to_apply=update_s32,
      update_window_dims={1},
      inserted_window_dims={0},
      scatter_dims_to_operand_dims={0},
      index_vector_dim=1
}

ENTRY main {
  p1 = s32[2] parameter(1)
  wrapped_indices = s32[2]{0} fusion(s32[2]{0} p1), kind=kLoop, calls=fused_computation.1
  p2 = s32[2,3] parameter(2)
  wrapped_updates = s32[2,3]{1,0} fusion(s32[2,3]{1,0} p2), kind=kLoop, calls=fused_computation.2
  p0 = s32[3,3] parameter(0)
  wrapped_operand = s32[3,3]{1,0} fusion(s32[3,3]{1,0} p0), kind=kLoop, calls=fused_computation
  ROOT wrapped_scatter = s32[3,3] fusion(wrapped_operand, wrapped_indices, wrapped_updates), kind=kInput, calls=fused_computation.3
}
