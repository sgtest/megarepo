diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel
--- stablehlo/BUILD.bazel
+++ stablehlo/BUILD.bazel
@@ -279,6 +279,24 @@
 )
 
 cc_library(
+    name = "experimental_ops",
+    srcs = [
+        "stablehlo/dialect/ExperimentalOps.cpp",
+    ],
+    hdrs = [
+        "stablehlo/dialect/ExperimentalOps.h",
+    ],
+    strip_include_prefix = ".",
+    deps = [
+        ":stablehlo_ops",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_library(
     name = "interpreter_ops",
     srcs = [
         "stablehlo/reference/InterpreterOps.cpp",
@@ -763,6 +781,7 @@
     deps = [
         ":base",
         ":chlo_ops",
+        ":experimental_ops",
         ":stablehlo_ops",
         ":stablehlo_ops_inc_gen",
         ":stablehlo_pass_inc_gen",
diff --ruN a/stablehlo/CMakeLists.txt b/stablehlo/CMakeLists.txt
--- stablehlo/CMakeLists.txt
+++ stablehlo/CMakeLists.txt
@@ -13,135 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-cmake_minimum_required(VERSION 3.15.0)
 
-if(POLICY CMP0068)
-  cmake_policy(SET CMP0068 NEW)
-  set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)
-endif()
-
-if(POLICY CMP0075)
-  cmake_policy(SET CMP0075 NEW)
-endif()
-
-if(POLICY CMP0077)
-  cmake_policy(SET CMP0077 NEW)
-endif()
-
-# CMP0116: Ninja generators transform `DEPFILE`s from `add_custom_command()`
-# New in CMake 3.20. https://cmake.org/cmake/help/latest/policy/CMP0116.html
-if(POLICY CMP0116)
-  cmake_policy(SET CMP0116 OLD)
-endif()
+# This build of StableHLO is meant to be embedded in MLIR-HLO.
+# As a result, its root CMakeLists.txt is different from the original
+# CMakeLists.txt from https://github.com/openxla/stablehlo.
+# All other files of this build of StableHLO except for this one are the same
+# as the original files.
+# To get access to a standalone build of StableHLO, check out the
+# openxla/stablehlo repository.
 
 #-------------------------------------------------------------------------------
 # Options and settings
 #-------------------------------------------------------------------------------
-option(STABLEHLO_BUILD_EMBEDDED "Build StableHLO as part of another project" OFF)
-option(STABLEHLO_ENABLE_BINDINGS_PYTHON "Enables StableHLO Python bindings" OFF)
-option(STABLEHLO_ENABLE_STRICT_BUILD "Build StableHLO with strict warnings and warnings as errors" OFF)
 
-#-------------------------------------------------------------------------------
-# Project setup and globals
-#-------------------------------------------------------------------------------
-set(STABLEHLO_EXTERNAL_PROJECT_BUILD OFF)
-
-if(NOT (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR) AND NOT MLIR_BINARY_DIR)
-  # Building as part of LLVM via the external project mechanism.
-  set(STABLEHLO_EXTERNAL_PROJECT_BUILD ON)
-else()
-  # Building standalone.
-  project(stablehlo LANGUAGES CXX C)
-  set(CMAKE_C_STANDARD 11)
-  set(CMAKE_CXX_STANDARD 17)
-endif()
-
-# Build with ccache if the package is present
-set(LLVM_CCACHE_BUILD OFF CACHE BOOL "Set to ON for a ccache enabled build")
-if(LLVM_CCACHE_BUILD)
-  find_program(CCACHE_PROGRAM ccache)
-  if(CCACHE_PROGRAM)
-      set(LLVM_CCACHE_MAXSIZE "" CACHE STRING "Size of ccache")
-      set(LLVM_CCACHE_DIR "" CACHE STRING "Directory to keep ccached data")
-      set(LLVM_CCACHE_PARAMS "CCACHE_CPP2=yes CCACHE_HASHDIR=yes"
-          CACHE STRING "Parameters to pass through to ccache")
-
-      set(CCACHE_PROGRAM "${LLVM_CCACHE_PARAMS} ${CCACHE_PROGRAM}")
-      if (LLVM_CCACHE_MAXSIZE)
-        set(CCACHE_PROGRAM "CCACHE_MAXSIZE=${LLVM_CCACHE_MAXSIZE} ${CCACHE_PROGRAM}")
-      endif()
-      if (LLVM_CCACHE_DIR)
-        set(CCACHE_PROGRAM "CCACHE_DIR=${LLVM_CCACHE_DIR} ${CCACHE_PROGRAM}")
-      endif()
-      set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM})
-  else()
-    message(FATAL_ERROR "Unable to find the program ccache. Set LLVM_CCACHE_BUILD to OFF")
-  endif()
-endif()
-
-#-------------------------------------------------------------------------------
-# MLIR/LLVM Configuration
-#-------------------------------------------------------------------------------
-if (STABLEHLO_ENABLE_STRICT_BUILD)
-  set(LLVM_ENABLE_WARNINGS ON)
-  set(LLVM_ENABLE_WERROR ON)
-  set(LLVM_ENABLE_PEDANTIC ON)
-endif()
-
-# Find MLIR to install if we are building standalone. If building as part of
-# another project, let it handle the MLIR dependency. The dependent project
-# might use a bundled version of MLIR instead of installing, for instance.
-if(STABLEHLO_EXTERNAL_PROJECT_BUILD)
-  message(STATUS "Building StableHLO as an external LLVM project")
-  set(MLIR_MAIN_SRC_DIR ${LLVM_MAIN_SRC_DIR}/../mlir ) # --src-root
-  set(MLIR_INCLUDE_DIR ${MLIR_MAIN_SRC_DIR}/include ) # --includedir
-  set(MLIR_GENERATED_INCLUDE_DIR ${LLVM_BINARY_DIR}/tools/mlir/include)
-  include_directories(SYSTEM ${MLIR_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_GENERATED_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_TABLEGEN_OUTPUT_DIR})
-
-  set(BACKEND_PACKAGE_STRING "${PACKAGE_STRING}")
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_MAIN_SRC_DIR}/cmake/modules")
-elseif(NOT STABLEHLO_BUILD_EMBEDDED)
-  message(STATUS "Building StableHLO with an installed MLIR")
-  find_package(MLIR REQUIRED CONFIG)
-  message(STATUS "Using MLIRConfig.cmake in: ${MLIR_DIR}")
-  message(STATUS "Using LLVMConfig.cmake in: ${LLVM_DIR}")
-  set(LLVM_RUNTIME_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/bin)
-  set(LLVM_LIBRARY_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/lib)
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_CMAKE_DIR}")
-  list(APPEND CMAKE_MODULE_PATH "${LLVM_CMAKE_DIR}")
-else()
-  message(STATUS "Building StableHLO embedded in another project")
-endif()
-
-if(LLVM_ENABLE_ZLIB)
-  find_package(ZLIB)
-endif()
-
-include(TableGen)
-include(AddLLVM)
-include(AddMLIR)
-include(HandleLLVMOptions)
-include_directories(${LLVM_INCLUDE_DIRS})
-include_directories(${MLIR_INCLUDE_DIRS})
-include_directories(${CMAKE_CURRENT_SOURCE_DIR})
-include_directories(${CMAKE_CURRENT_BINARY_DIR})
-link_directories(${LLVM_BUILD_LIBRARY_DIR})
-add_definitions(${LLVM_DEFINITIONS})
-
-#-------------------------------------------------------------------------------
-# Python configuration
-#-------------------------------------------------------------------------------
-
-if(STABLEHLO_ENABLE_BINDINGS_PYTHON)
-  if(NOT STABLEHLO_EXTERNAL_PROJECT_BUILD)
-    message(WARNING "StableHLO Python bindings are not supported in standalone mode")
-  endif()
-
-  include(MLIRDetectPythonEnv)
-  mlir_configure_python_dev_packages()
-endif()
+set(STABLEHLO_ENABLE_BINDINGS_PYTHON ${MHLO_ENABLE_BINDINGS_PYTHON})
 
 #-------------------------------------------------------------------------------
 # Directory setup
diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp
--- stablehlo/stablehlo/dialect/Base.cpp
+++ stablehlo/stablehlo/dialect/Base.cpp
@@ -600,5 +600,18 @@
   return UnrankedTensorType::get(components.getElementType());
 }
 
+DenseIntElementsAttr getPaddingAttr(MLIRContext* context,
+                                    ArrayRef<int64_t> values) {
+  return DenseIntElementsAttr::get(
+      RankedTensorType::get({static_cast<int64_t>(values.size()) / 2, 2},
+                            IntegerType::get(context, 64)),
+      values);
+}
+
+DenseIntElementsAttr getPaddingAttr(Builder* builder,
+                                    ArrayRef<int64_t> values) {
+  return getPaddingAttr(builder->getContext(), values);
+}
+
 }  // namespace hlo
 }  // namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h
--- stablehlo/stablehlo/dialect/Base.h
+++ stablehlo/stablehlo/dialect/Base.h
@@ -194,6 +194,10 @@
 
 ShapedType createShapedType(ShapedTypeComponents components);
 
+DenseIntElementsAttr getPaddingAttr(MLIRContext *context,
+                                    ArrayRef<int64_t> value);
+DenseIntElementsAttr getPaddingAttr(Builder *builder, ArrayRef<int64_t> value);
+
 // This interface is implemented by both StableHLO and MHLO dialects
 // and is used as the foundation for sharing verification, type inference and
 // prettyprinting logic between them.
diff --ruN a/stablehlo/stablehlo/dialect/CMakeLists.txt b/stablehlo/stablehlo/dialect/CMakeLists.txt
--- stablehlo/stablehlo/dialect/CMakeLists.txt
+++ stablehlo/stablehlo/dialect/CMakeLists.txt
@@ -77,6 +77,20 @@
 target_include_directories(ChloOps INTERFACE
   $<BUILD_INTERFACE:${STABLEHLO_SOURCE_DIR}>
   $<BUILD_INTERFACE:${STABLEHLO_BINARY_DIR}>
+)
+
+add_mlir_dialect_library(ExperimentalOps
+  PARTIAL_SOURCES_INTENDED
+  ExperimentalOps.cpp
+
+  DEPENDS
+  StablehloOpsIncGen
+
+  LINK_LIBS PUBLIC
+  MLIRFuncDialect
+  MLIRIR
+  MLIRSupport
+  StablehloOps
 )
 
 add_mlir_dialect_library(StablehloRegister
diff --ruN a/stablehlo/stablehlo/dialect/ExperimentalOps.cpp b/stablehlo/stablehlo/dialect/ExperimentalOps.cpp
--- stablehlo/stablehlo/dialect/ExperimentalOps.cpp
+++ stablehlo/stablehlo/dialect/ExperimentalOps.cpp
@@ -0,0 +1,504 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/dialect/ExperimentalOps.h"
+
+#include <optional>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/Types.h"
+
+namespace mlir {
+namespace stablehlo {
+
+LogicalResult DynamicReduceWindowOpAdaptor::verify() {
+  // Before checking the constraints inherited from ReduceWindowOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2 * op_->getNumResults() + 5)
+    return op_.emitError("expects size(operands) = 2 * size(results) + 5");
+  if (op_->getNumResults() == 0)
+    return op_.emitError("expects size(results) > 0");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_reduce_window".
+    // called_computations carries the body.
+    if (attr.getName() != "api_version" &&
+        attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "called_computations")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_reduce_window")
+    return op_.emitError() << "expects @stablehlo.dynamic_reduce_window";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto numInputs = getInputs().size();
+  auto inputs = op_.getInputs().slice(0, numInputs);
+  auto initValues = op_.getInputs().slice(numInputs, numInputs);
+  auto windowDimensions = op_.getInputs()[op_.getInputs().size() - 5];
+  auto windowStrides = op_.getInputs()[op_.getInputs().size() - 4];
+  auto baseDilations = op_.getInputs()[op_.getInputs().size() - 3];
+  auto windowDilations = op_.getInputs()[op_.getInputs().size() - 2];
+  auto padding = op_.getInputs()[op_.getInputs().size() - 1];
+  auto results = op_.getResults();
+
+  // reduce_window_c1
+  // This constraint hold automatically thanks to the checks that we have
+  // performed above.
+
+  // reduce_window_i1
+  SmallVector<ShapedType> inputTypes;
+  for (auto [index, input] : llvm::enumerate(inputs)) {
+    auto inputType = input.getType().dyn_cast<ShapedType>();
+    inputTypes.push_back(inputType);
+    if (!inputType)
+      return op_.emitError()
+             << "expects inputs (e.g. operand #" << index << ") to be tensors";
+  }
+
+  // reduce_window_i2
+  SmallVector<ShapedType> initValueTypes;
+  for (auto [index, initValue] : llvm::enumerate(initValues)) {
+    auto initValueType = initValue.getType().dyn_cast<ShapedType>();
+    initValueTypes.push_back(initValueType);
+    if (!initValueType || !initValueType.hasRank() ||
+        initValueType.getRank() != 0)
+      return op_.emitError() << "expects init_values (e.g. operand #"
+                             << numInputs + index << ") "
+                             << "to be 0-dimensional tensors";
+  }
+
+  // reduce_window_i3...reduce_window_i7
+  auto checkRank = [&](StringRef name, int64_t index, Value dynamicAttr,
+                       int64_t expectedRank) -> LogicalResult {
+    auto type = dynamicAttr.getType().dyn_cast<ShapedType>();
+    if (!type || !type.hasRank() || type.getRank() != expectedRank ||
+        !type.getElementType().isIntOrIndex()) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to be a " << expectedRank << "-dimensional tensor "
+             << "of integer or index type";
+    }
+    return success();
+  };
+  if (failed(checkRank("window_dimensions", -5, windowDimensions, 1)) ||
+      failed(checkRank("window_strides", -4, windowStrides, 1)) ||
+      failed(checkRank("base_dilations", -3, baseDilations, 1)) ||
+      failed(checkRank("window_dilations", -2, windowDilations, 1)) ||
+      failed(checkRank("padding", -1, padding, 2)))
+    return failure();
+
+  // reduce_window_i7
+  auto paddingType = getPadding().getType().dyn_cast<ShapedType>();
+  if (!paddingType || !paddingType.hasRank() || paddingType.getRank() != 2 ||
+      paddingType.getDimSize(1) != 2 ||
+      !paddingType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects padding_type (operand #" << op_.getNumOperands() - 1
+           << ") to be a 2-dimensional tensor of integer or index type";
+
+  // reduce_window_c2
+  std::optional<ArrayRef<int64_t>> inputShape;
+  for (auto inputType : inputTypes) {
+    if (!inputType.hasRank()) continue;
+    if (!inputShape) inputShape = inputType.getShape();
+    if (failed(verifyCompatibleShape(inputType.getShape(), *inputShape)))
+      return op_.emitError() << "expects all inputs (operands 0.." << numInputs
+                             << ") to have compatible shapes";
+  }
+
+  // reduce_window_c3
+  for (auto [inputType, initValueType] :
+       llvm::zip(inputTypes, initValueTypes)) {
+    if (inputType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects inputs (operands 0.." << numInputs
+                             << ") and init_values (operands " << numInputs
+                             << ".." << numInputs * 2 << ") to have pairwise "
+                             << "the same element types";
+  }
+
+  // reduce_window_c4...reduce_window_c12
+  // In this range, we only verify the constraints with even numbers.
+  // Verifying the constraints with odd numbers would require knowing the
+  // actual values of window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+  auto checkShape = [&](StringRef name, int64_t index, Value dynamicAttr,
+                        ArrayRef<int64_t> expectedShape) -> LogicalResult {
+    auto type = dynamicAttr.getType().cast<ShapedType>();
+    if (type.getShape() != expectedShape) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to have shape [" << expectedShape << "]";
+    }
+    return success();
+  };
+  if (inputShape) {
+    auto inputRank = static_cast<int64_t>(inputShape->size());
+    if (failed(checkShape("window_dimensions", -5, windowDimensions,
+                          {inputRank})) ||
+        failed(checkShape("window_strides", -4, windowStrides, {inputRank})) ||
+        failed(checkShape("base_dilations", -3, baseDilations, {inputRank})) ||
+        failed(
+            checkShape("window_dilations", -2, windowDilations, {inputRank})) ||
+        failed(checkShape("padding", -1, padding, {inputRank, 2})))
+      return failure();
+  }
+
+  // reduce_window_c13
+  if (op_.getCalledComputations().size() != 1)
+    return op_.emitError() << "expects called_computations to have 1 element";
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  if (!bodyFunc)
+    return op_.emitError() << "expects called_computations to refer to "
+                           << "a function that exists within a parent module";
+
+  // reduce_window_c13
+  SmallVector<Type> expectedBodyInputs;
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  SmallVector<Type> expectedBodyOutputs;
+  llvm::append_range(expectedBodyOutputs, initValueTypes);
+  auto expectedBodyType = FunctionType::get(
+      op_.getContext(), expectedBodyInputs, expectedBodyOutputs);
+  if (bodyFunc.getFunctionType() != expectedBodyType)
+    return op_.emitError() << "expects body to have type " << expectedBodyType;
+
+  // reduce_window_c14
+  SmallVector<ShapedType> resultTypes;
+  std::optional<ArrayRef<int64_t>> resultShape;
+  for (auto result : results) {
+    auto resultType = result.getType().dyn_cast<ShapedType>();
+    resultTypes.push_back(resultType);
+    if (!resultType) return op_.emitError() << "expects results to be tensors";
+
+    if (!resultType.hasRank()) continue;
+    if (!resultShape) resultShape = resultType.getShape();
+    if (failed(verifyCompatibleShape(resultType.getShape(), *resultShape)))
+      return op_.emitError() << "expects all results to have compatible shapes";
+  }
+
+  // reduce_window_c15
+  // Verifying this constraint would require knowing the actual values of
+  // window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+
+  // reduce_window_c16
+  for (auto [resultType, initValueType] :
+       llvm::zip(resultTypes, initValueTypes)) {
+    if (resultType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects results and init_values (operands "
+                             << numInputs << ".." << numInputs * 2 << ") "
+                             << "to have pairwise the same element types";
+  }
+
+  return success();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInputs() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(0, numInputs);
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInitValues() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(numInputs, numInputs);
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDimensions() {
+  return op_.getInputs()[op_.getInputs().size() - 5]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowStrides() {
+  return op_.getInputs()[op_.getInputs().size() - 4]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getBaseDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 3]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 2]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getPadding() {
+  return op_.getInputs()[op_.getInputs().size() - 1]
+      .cast<TypedValue<ShapedType>>();
+}
+
+Region& DynamicReduceWindowOpAdaptor::getBody() {
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  return bodyFunc.getBody();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getResults() {
+  return op_.getResults();
+}
+
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_reduce_window") return {};
+  return DynamicReduceWindowOpAdaptor(op);
+}
+
+LogicalResult DynamicRngBitGeneratorOpAdaptor::verify() {
+  // Before checking the constraints inherited from RngBitGeneratorOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_rng_bit_generator".
+    // rng_algorithm comes from the operation.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "rng_algorithm")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return op_.emitError() << "expects @stablehlo.dynamic_rng_bit_generator";
+  if (!op_->hasAttr("rng_algorithm"))
+    return op_.emitError() << "expects an rng_algorithm";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto rngAlgorithmAttr = op_->getAttr("rng_algorithm");
+  auto initialState = op_.getInputs()[0];
+  auto outputShape = op_.getInputs()[1];
+  auto outputState = op_.getResults()[0];
+  auto output = op_.getResults()[1];
+
+  // dynamic_rng_bit_generator_i1
+  if (!rngAlgorithmAttr.isa<RngAlgorithmAttr>())
+    return op_.emitError()
+           << "expects a #stablehlo<rng_algorithm ...> rng_algorithm";
+
+  // dynamic_rng_bit_generator_i2
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto initialStateType = initialState.getType().dyn_cast<ShapedType>();
+  if (!initialStateType || !initialStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects initial_state (operand #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_i3
+  auto outputShapeType = outputShape.getType().dyn_cast<ShapedType>();
+  if (!outputShapeType || !outputShapeType.hasRank() ||
+      outputShapeType.getRank() != 1 ||
+      !outputShapeType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects output_shape (operand #1) "
+           << "to be a 1-dimensional tensor of integer or index type";
+
+  // dynamic_rng_bit_generator_o1
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto outputStateType = outputState.getType().dyn_cast<ShapedType>();
+  if (!outputStateType || !outputStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output_state (result #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_o2
+  auto outputType = output.getType().dyn_cast<ShapedType>();
+  if (!outputType || !outputType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output (result #1) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_c1
+  if (!hlo::isCompatibleForHloTypeInference(initialStateType, outputStateType))
+    return op_.emitError()
+           << "expects initial_state (operand #0) and output_state (result #0) "
+           << "to have compatible shapes";
+
+  // dynamic_rng_bit_generator_c2
+  // TODO(#486): Verify rng_algorithm in RngBitGeneratorOp.
+
+  // dynamic_rng_bit_generator_c3
+  if (!hlo::isCompatibleForHloTypeInference(outputShape, outputType))
+    return op_.emitError() << "expects output (result #1) to have shape  "
+                           << "compatible with output_shape (operand #2)";
+
+  return success();
+}
+
+RngAlgorithm DynamicRngBitGeneratorOpAdaptor::getRngAlgorithm() {
+  return op_->getAttr("rng_algorithm").cast<RngAlgorithmAttr>().getValue();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getInitialState() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputShape() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputState() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutput() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return {};
+  return DynamicRngBitGeneratorOpAdaptor(op);
+}
+
+LogicalResult DynamicTopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_top_k".
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_top_k")
+    return op_.emitError() << "expects @stablehlo.dynamic_top_k";
+
+  auto operand = op_.getInputs()[0];
+  auto k = op_.getInputs()[1];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+
+  // dynamic_top_k_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_i2
+  auto kType = k.getType().dyn_cast<ShapedType>();
+  if (!kType || !kType.hasRank() ||
+      kType.getRank() != 0 || !kType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects k (operand #1) "
+           << "to be a 0-dimensional tensor of integer or index type";
+
+  // dynamic_top_k_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // dynamic_top_k_c1
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] =
+      valuesType.getDimSize(valuesType.getRank() - 1);
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension";
+
+  // dynamic_top_k_c2
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // dynamic_top_k_c3
+  if (!operandType.isDynamicDim(operandLastDim) &&
+      !valuesType.isDynamicDim(operandLastDim) &&
+      operandType.getDimSize(operandLastDim) <
+          valuesType.getDimSize(operandLastDim))
+    return op_.emitError() << "expects the values last dimension to have size "
+                              "at least as large "
+                           << "as operand last dimension";
+
+  // dynamic_top_k_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getK() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_top_k") return {};
+  return DynamicTopKOpAdaptor(op);
+}
+
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/ExperimentalOps.h b/stablehlo/stablehlo/dialect/ExperimentalOps.h
--- stablehlo/stablehlo/dialect/ExperimentalOps.h
+++ stablehlo/stablehlo/dialect/ExperimentalOps.h
@@ -0,0 +1,227 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_DIALECT_EXPERIMENTAL_OPS_H
+#define STABLEHLO_DIALECT_EXPERIMENTAL_OPS_H
+
+// This file supports XLA-specific experiments with the StableHLO opset.
+// These experiments are not yet ready to be upstreamed to openxla/stablehlo
+// and are incubating towards the respective StableHLO RFCs.
+//
+// Custom calls (which are the implementation vehicle of these experiments)
+// don't have compatibility guarantees within the StableHLO process, but
+// the StableHLO team at Google provides out-of-band guarantees for these
+// custom calls, with the same compatibility window as StableHLO upstream.
+
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LogicalResult.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir {
+namespace stablehlo {
+
+// The DynamicReduceWindowOp experiment provides a dynamic version of
+// ReduceWindowOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicReduceWindowOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_reduce_window` custom call.
+// This custom call has the following operands which represent a dynamic version
+// of operands and attributes of ReduceWindowOp:
+//   * [0:N]   => inputs
+//   * [N:2*N] => init_values
+//   * [-5]    => window_dimensions
+//   * [-4]    => window_strides
+//   * [-3]    => base_dilations
+//   * [-2]    => window_dilations
+//   * [-1]    => padding
+// Additionally, to represent the body of DynamicReduceWindowOp, the custom call
+// has a satellite function attached to the custom call via called_computations.
+//
+// Semantics of DynamicReduceWindowOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window
+// with the following exceptions:
+//   1) All tensor constants, i.e. window_dimensions, window_strides,
+//      base_dilations, window_dilations and padding, become tensors of
+//      integer type.
+//   2) As a result, some of the constraints can no longer be validated
+//      statically. However, this operation still expects these constraints
+//      to hold dynamically, and if they don't hold, the behavior is undefined.
+class DynamicReduceWindowOpAdaptor {
+ public:
+  DynamicReduceWindowOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::ReduceWindowOp, except that all the
+  // std::optional<DenseIntElementsAttr> attributes have turned into values.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  ValueRange getInputs();
+  ValueRange getInitValues();
+  TypedValue<ShapedType> getWindowDimensions();
+  TypedValue<ShapedType> getWindowStrides();
+  TypedValue<ShapedType> getBaseDilations();
+  TypedValue<ShapedType> getWindowDilations();
+  TypedValue<ShapedType> getPadding();
+  Region& getBody();
+  ValueRange getResults();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicReduceWindowOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_reduce_window".
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op);
+
+// The DynamicRngBitGeneratorOp experiment provides a dynamic version of
+// RngBitGeneratorOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicRngBitGeneratorOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator` custom call.
+// This custom call has the regular operand of RngBitGeneratorOp plus an
+// additional `output_shape` operand that determines the shape of the output:
+//   * [0] => initial_state
+//   * [1] => output_shape
+//
+// Semantics of DynamicRngBitGeneratorOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator
+// extended with an additional input (I3) and an additional constraint (C3):
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `rng_algorithm` | enum of `DEFAULT`, `THREE_FRY`, and `PHILOX` |
+// | (I2)  | `initial_state` | 1-dimensional tensor of type `ui64`          |
+// | (I3)  | `output_shape`  | 1-dimensional tensor of integer type         |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `output_state` | 1-dimensional tensor of type `ui64`      |
+// | `output`       | tensor of integer or floating-point type |
+//
+// #### Constraints
+//
+// * (C1) `type(initial_state) = type(output_state)`.
+// * (C2) `size(initial_state)` is defined as:
+//   * implementation-defined if `rng_algorithm = DEFAULT`.
+//   * `2` if `rng_algorithm = THREE_FRY`.
+//   * `2` or `3` if `rng_algorithm = PHILOX`.
+// * (C3) `shape(output) = output_shape`.
+class DynamicRngBitGeneratorOpAdaptor {
+ public:
+  DynamicRngBitGeneratorOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::RngBitGeneratorOp, extended with the
+  // additional `output_shape` operand.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  RngAlgorithm getRngAlgorithm();
+  TypedValue<ShapedType> getInitialState();
+  TypedValue<ShapedType> getOutputShape();
+  TypedValue<ShapedType> getOutputState();
+  TypedValue<ShapedType> getOutput();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicRngBitGeneratorOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_rng_bit_generator".
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op);
+
+// The DynamicTopKOp experiment provides a dynamic version of
+// TopKOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicTopKOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_top_k` custom call.
+// This custom call has the regular operand of TopKOp plus an
+// additional `k` operand that determines the shape of the output.
+//
+// Semantics of DynamicTopKOp are inherited from semantics of Chlo.TopKOp.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | 0-dimensional tensor of integer or index type|
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `element_type(values) = element_type(operand)`
+// * (C3) `shape(values)[-1] <= shape(operand)[-1]`
+// * (C4) `shape(indices) = shape(values)`
+class DynamicTopKOpAdaptor {
+ public:
+  DynamicTopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getK();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicTopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_top_k".
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op);
+
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_DIALECT_EXPERIMENTAL_OPS_H
diff --ruN a/stablehlo/stablehlo/reference/Process.cpp b/stablehlo/stablehlo/reference/Process.cpp
--- stablehlo/stablehlo/reference/Process.cpp
+++ stablehlo/stablehlo/reference/Process.cpp
@@ -44,7 +44,7 @@
 
 void Process::outfeed(ArrayRef<Tensor> inputs) { grid_->outfeed(inputs); }
 
-const std::shared_ptr<RendezvousResult> Process::rendezvous(
+std::shared_ptr<RendezvousResult const> Process::rendezvous(
     ProcessGroup processGroup, ChannelId channelId, const Tensor &operand) {
   return grid_->rendezvous(processGroup, channelId, getId(), operand);
 }
diff --ruN a/stablehlo/stablehlo/reference/Process.h b/stablehlo/stablehlo/reference/Process.h
--- stablehlo/stablehlo/reference/Process.h
+++ stablehlo/stablehlo/reference/Process.h
@@ -54,7 +54,7 @@
   void outfeed(ArrayRef<Tensor> inputs);
 
   /// See `ProcessGrid::rendezvous`.
-  const std::shared_ptr<RendezvousResult> rendezvous(ProcessGroup processGroup,
+  std::shared_ptr<RendezvousResult const> rendezvous(ProcessGroup processGroup,
                                                      ChannelId channelId,
                                                      const Tensor &operand);
 
diff --ruN a/stablehlo/stablehlo/reference/ProcessGrid.cpp b/stablehlo/stablehlo/reference/ProcessGrid.cpp
--- stablehlo/stablehlo/reference/ProcessGrid.cpp
+++ stablehlo/stablehlo/reference/ProcessGrid.cpp
@@ -66,13 +66,13 @@
   result_[processId] = tensor;
 }
 
-Tensor RendezvousResult::lookup(ProcessId processId) {
+Tensor RendezvousResult::lookup(ProcessId processId) const {
   auto it = result_.find(processId);
   if (it != result_.end()) return it->second;
   return {};
 }
 
-SmallVector<Tensor> RendezvousResult::getSortedTensors() {
+SmallVector<Tensor> RendezvousResult::getSortedTensors() const {
   return llvm::to_vector(
       llvm::map_range(result_, [](const auto &pair) { return pair.second; }));
 }
@@ -162,7 +162,7 @@
 
 void ProcessGrid::outfeed(ArrayRef<Tensor> inputs) { outfeed_.push(inputs); }
 
-const std::shared_ptr<RendezvousResult> ProcessGrid::rendezvous(
+std::shared_ptr<RendezvousResult const> ProcessGrid::rendezvous(
     ProcessGroup processGroup, ChannelId channelId, ProcessId processId,
     const Tensor &operand) {
   std::pair<ProcessGroup, ChannelId> channelKey(processGroup, channelId);
diff --ruN a/stablehlo/stablehlo/reference/ProcessGrid.h b/stablehlo/stablehlo/reference/ProcessGrid.h
--- stablehlo/stablehlo/reference/ProcessGrid.h
+++ stablehlo/stablehlo/reference/ProcessGrid.h
@@ -75,14 +75,14 @@
   /// Iterates through the (ProcessId, Tensor) map entires and returns a vector
   /// of Tensors sorted by ProcessId--(replicaId, partitionId) pair--in
   /// lexicographical order.
-  SmallVector<Tensor> getSortedTensors();
+  SmallVector<Tensor> getSortedTensors() const;
 
   /// Inserts `tensor` into the map using the key `processId`.
   void insert(ProcessId processId, Tensor tensor);
 
   /// Iterates through the map and returns the value associated with the key
   /// `processId`. If key is not found, return an empty `Tensor`.
-  Tensor lookup(ProcessId processId);
+  Tensor lookup(ProcessId processId) const;
 
  private:
   /// Internal map representation of the result of `ProcessGrid::rendezvous`.
@@ -134,7 +134,7 @@
   /// tensors are accumulated in `RendezvousResult` whose shard pointer is
   /// returned to all callers once the barrier has been reached by all StableHLO
   /// processes.
-  const std::shared_ptr<RendezvousResult> rendezvous(ProcessGroup processGroup,
+  std::shared_ptr<RendezvousResult const> rendezvous(ProcessGroup processGroup,
                                                      ChannelId channelId,
                                                      ProcessId processId,
                                                      const Tensor &operand);
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
@@ -426,6 +426,172 @@
 
 // -----
 
+// CHECK-LABEL: func @dynamic_reduce_window_success_static_result_type
+func.func @dynamic_reduce_window_success_static_result_type(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<2x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = dense<[2, 1]> : tensor<2xi64>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = dense<[3, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_dimensions = dense<[2, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_strides = dense<[4, 1]> : tensor<2xi64>
+  //          CHECK-SAME: } : (tensor<3x2xf32>, tensor<f32>) -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %5 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_dynamic_result_type
+func.func @dynamic_reduce_window_success_dynamic_result_type(%arg0: tensor<?x2xf32>, %arg1: tensor<f32>) -> tensor<?x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = dense<[2, 1]> : tensor<2xi64>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = dense<[3, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_dimensions = dense<[2, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_strides = dense<[4, 1]> : tensor<2xi64>
+  //          CHECK-SAME: } : (tensor<?x2xf32>, tensor<f32>) -> tensor<?x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<?x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<?x2xf32>
+  func.return %5 : tensor<?x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_reduce_window.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %arg2, %0, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_strides
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_strides(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %arg2, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_base_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_base_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %arg2, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %arg2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_padding
+func.func @dynamic_reduce_window_inapplicable_dynamic_padding(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2x2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %arg2) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
 // CHECK-LABEL: func @dynamic_reshape_success
 func.func @dynamic_reshape_success(%arg0: tensor<4xf32>) -> tensor<1x4xf32> {
   // CHECK-NOT: stablehlo.dynamic_reshape
@@ -452,6 +618,185 @@
   %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
   %1 = stablehlo.dynamic_reshape %arg0, %0 : (tensor<4xf32>, tensor<2xi64>) -> tensor<1x?xf32>
   return %1 : tensor<1x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_success
+func.func @dynamic_rng_bit_generator_success(%arg0: tensor<2xui64>) -> tensor<1x4xf32> {
+  // CHECK-NOT: stablehlo.dynamic_rng_bit_generator
+  // CHECK: stablehlo.rng_bit_generator %arg0, algorithm = DEFAULT : (tensor<2xui64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_rng_bit_generator.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape(%arg0: tensor<2xui64>, %arg1: tensor<2xi64>) -> tensor<1x4xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %arg1) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type(%arg0: tensor<2xui64>) -> tensor<?x?xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<?x?xf32>)
+  return %1#1 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_success
+func.func @dynamic_top_k_success(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: chlo.top_k
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_failure_k_mismatch
+func.func @dynamic_top_k_failure_k_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: @stablehlo.dynamic_top_k
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_not_float
+func.func @dynamic_top_k_error_operand_not_float(%arg0: tensor<16xcomplex<f64>>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xcomplex<f64>>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_unranked
+func.func @dynamic_top_k_error_operand_unranked(%arg0: tensor<*xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<*xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_scalar_operand
+func.func @dynamic_top_k_error_scalar_operand(%arg0: tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<f32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_integer
+func.func @dynamic_top_k_error_k_not_integer(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3.> : tensor<f32>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_scalar
+func.func @dynamic_top_k_error_k_not_scalar(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3> : tensor<1xui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<1xui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O1
+// CHECK-LABEL: func @dynamic_top_k_error_values_not_float
+func.func @dynamic_top_k_error_values_not_float(%arg0: tensor<16xf32>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects values (result #0) to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O2
+// CHECK-LABEL: func @dynamic_top_k_error_indices_not_i32
+func.func @dynamic_top_k_error_indices_not_i32(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi64>) {
+  // expected-error@+2{{expects indices (result #1) to be a tensor of si32}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi64>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi64>
+}
+
+// -----
+
+// dynamic_top_k C1
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_rank
+func.func @dynamic_top_k_error_values_bad_rank(%arg0: tensor<16xf32>) -> (tensor<3x4xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values shape to match the operand shape in all but the last dimension}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3x4xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3x4xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C2
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_element_type
+func.func @dynamic_top_k_error_values_bad_element_type(%arg0: tensor<16xf32>) -> (tensor<3xf64>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values element type to be the same as the operand element type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf64>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf64>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C3
+// CHECK-LABEL: func @dynamic_top_k_error_values_last_dim_too_large
+func.func @dynamic_top_k_error_values_last_dim_too_large(%arg0: tensor<16xf32>) -> (tensor<17xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values last dimension to have size at least as large as operand last dimension}}
+  %k = stablehlo.constant dense<17> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<17xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<17xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C4
+// CHECK-LABEL: func @dynamic_top_k_error_indices_shape_mismatch
+func.func @dynamic_top_k_error_indices_shape_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<4xi32>) {
+  // expected-error@+2{{expects the indices shape to match the values shape}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<4xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<4xi32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
@@ -31,6 +31,7 @@
 
 // -----
 
+// CHECK-LABEL: module @has_main
 module @has_main {
   // CHECK: main
   func.func @main(%arg0: tensor<4xf32>) -> tensor<*xi32> {
@@ -38,17 +39,11 @@
     %0 = stablehlo.bitcast_convert %arg0 : (tensor<4xf32>) -> tensor<*xi32>
     func.return %0 : tensor<*xi32>
   }
-
-  // CHECK: helper
-  func.func @helper(%arg0: tensor<4xf32>) -> tensor<*xi32> {
-    // CHECK: stablehlo.bitcast_convert{{.*}} -> tensor<*xi32>
-    %0 = stablehlo.bitcast_convert %arg0 : (tensor<4xf32>) -> tensor<*xi32>
-    func.return %0 : tensor<*xi32>
-  }
-}
-
-// -----
-
+}
+
+// -----
+
+// CHECK-LABEL: func @error_unsupported_operation
 func.func @error_unsupported_operation(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> index {
   // CHECK: stablehlo.add{{.*}} -> tensor<?xf32>
   %0 = stablehlo.add %arg0, %arg1 : (tensor<4xf32>, tensor<4xf32>) -> tensor<?xf32>
@@ -472,11 +467,312 @@
 
 // -----
 
-// CHECK-LABEL: func @refine_bitcast_convert_same_bitwidth
-func.func @refine_bitcast_convert_same_bitwidth(%arg0 : tensor<4xf32>) -> tensor<*xi32> {
+// CHECK-LABEL: func @refine_bitcast_convert_same_bitwidth_unranked_result
+func.func @refine_bitcast_convert_same_bitwidth_unranked_result(%arg0 : tensor<4xf32>) -> tensor<*xi32> {
   // CHECK: stablehlo.bitcast_convert{{.*}} -> tensor<4xi32>
   %0 = stablehlo.bitcast_convert %arg0 : (tensor<4xf32>) -> tensor<*xi32>
   func.return %0 : tensor<*xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_bitcast_convert_same_bitwidth
+func.func @refine_bitcast_convert_same_bitwidth() -> tensor<?x?x0xf32> {
+  %0 = stablehlo.constant dense<[3, 5, 0]> : tensor<3xi32>
+  %21 = stablehlo.dynamic_iota %0, dim = 0 : (tensor<3xi32>) -> tensor<?x?x0xui32>
+  // CHECK: stablehlo.bitcast_convert{{.*}} -> tensor<3x5x0xf32>
+  %48 = stablehlo.bitcast_convert %21 : (tensor<?x?x0xui32>) -> tensor<?x?x0xf32>
+  return %48 : tensor<?x?x0xf32>
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_call
+module @refine_call {
+  func.func @main(%arg1: tensor<4xf32>) -> tensor<?xf32> {
+    %0 = stablehlo.bitcast_convert %arg1 : (tensor<4xf32>) -> tensor<?xf32>
+    %1 = stablehlo.constant dense<4> : tensor<i32>
+    // CHECK: refine_call_callee{{.*}}-> tensor<4xf32>
+    %2 = call @refine_call_callee(%1, %0) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    return %2 : tensor<?xf32>
+  }
+  // CHECK: refine_call_callee(%arg0: tensor<4xf32>) -> tensor<4xf32>
+  func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
+    // CHECK: stablehlo.constant dense<4>
+    %0 = stablehlo.reshape %arg0 : (tensor<i32>) -> tensor<1xi32>
+    %1 = stablehlo.dynamic_iota %0, dim = 0 : (tensor<1xi32>) -> tensor<?xf32>
+    return %1 : tensor<?xf32>
+  }
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_call_dimension_arguments
+module @refine_call_dimension_arguments {
+  func.func public @main(%arg0: tensor<i32>) -> tensor<i32> {
+    // CHECK: [[RESULT:%.*]] = call @callee
+    // CHECK: return [[RESULT]]
+    %0 = stablehlo.constant dense<3> : tensor<i32>
+    %1 = call @callee(%0, %0, %arg0) : (tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<i32>
+    return %1 : tensor<i32>
+  }
+  // %arg0 and %arg1 are dimension arguments
+  // CHECK: @callee([[ARG0:%.*]]: tensor<i32>) -> tensor<i32>
+  func.func private @callee(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {
+    // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<6>
+    // CHECK: [[RESULT1:%.*]] = stablehlo.add [[RESULT0]], [[ARG0]]
+    // CHECK: return [[RESULT1]]
+    %0 = stablehlo.add %arg0, %arg1: tensor<i32>
+    %1 = stablehlo.add %0, %arg2: tensor<i32>
+    return %1 : tensor<i32>
+  }
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_call_prefix_token_and_dimension_arguments
+module @refine_call_prefix_token_and_dimension_arguments {
+  func.func public @main(%arg0: tensor<i32>) -> tensor<i32> {
+    // CHECK: [[RESULT:%.*]] = call @callee
+    // CHECK: return [[RESULT]]
+    %0 = stablehlo.constant dense<3> : tensor<i32>
+    %token = stablehlo.create_token : !stablehlo.token
+    %1 = call @callee(%token, %0, %0, %arg0) : (!stablehlo.token, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<i32>
+    return %1 : tensor<i32>
+  }
+  // %arg0 and %arg1 are dimension arguments
+  // CHECK: @callee([[ARG_TOKEN:%.*]]: !stablehlo.token, [[ARG0:%.*]]: tensor<i32>
+  func.func private @callee(%arg_token: !stablehlo.token, %arg0: tensor<i32>, %arg1: tensor<i32>, %arg2: tensor<i32>) -> tensor<i32> {
+    // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<6>
+    // CHECK: [[RESULT1:%.*]] = stablehlo.add [[RESULT0]], [[ARG0]]
+    // CHECK: return [[RESULT1]]
+    %0 = stablehlo.add %arg0, %arg1: tensor<i32>
+    %1 = stablehlo.add %0, %arg2: tensor<i32>
+    return %1 : tensor<i32>
+  }
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_call_dimension_arguments_followed_by_token
+module @refine_call_dimension_arguments_followed_by_token {
+  func.func public @main(%arg0: tensor<i32>) -> tensor<i32> {
+    // CHECK: [[RESULT:%.*]] = call @callee
+    // CHECK: return [[RESULT]]
+    %0 = stablehlo.constant dense<3> : tensor<i32>
+    %token = stablehlo.create_token : !stablehlo.token
+    %1 = call @callee(%0, %0, %token, %arg0) : (tensor<i32>, tensor<i32>, !stablehlo.token, tensor<i32>) -> tensor<i32>
+    return %1 : tensor<i32>
+  }
+  // %arg0 and %arg1 are dimension arguments
+  // CHECK: @callee([[ARG_TOKEN:%.*]]: !stablehlo.token, [[ARG0:%.*]]: tensor<i32>
+  func.func private @callee(%arg0: tensor<i32>, %arg1: tensor<i32>, %arg_token: !stablehlo.token, %arg2: tensor<i32>) -> tensor<i32> {
+    // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<6>
+    // CHECK: [[RESULT1:%.*]] = stablehlo.add [[RESULT0]], [[ARG0]]
+    // CHECK: return [[RESULT1]]
+    %0 = stablehlo.add %arg0, %arg1: tensor<i32>
+    %1 = stablehlo.add %0, %arg2: tensor<i32>
+    return %1 : tensor<i32>
+  }
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_multiple_call_with_same_context
+module @refine_multiple_call_with_same_context {
+  func.func @main(%arg1: tensor<4xf32>) -> tensor<?xf32> {
+    %0 = stablehlo.bitcast_convert %arg1 : (tensor<4xf32>) -> tensor<?xf32>
+    %arg0_new = "stablehlo.get_dimension_size"(%0) {dimension = 0 : i64} : (tensor<?xf32>) -> tensor<i32>
+    // CHECK: refine_call_callee{{.*}}-> tensor<4xf32>
+    %1 = call @refine_call_callee(%arg0_new, %0) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    // CHECK: refine_call_callee{{.*}}-> tensor<4xf32>
+    %2 = call @refine_call_callee(%arg0_new, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    return %2 : tensor<?xf32>
+  }
+  // CHECK: refine_call_callee{{.*}}-> tensor<4xf32>
+  func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
+    return %arg1 : tensor<?xf32>
+  }
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_multiple_call_constant_function
+module @refine_multiple_call_constant_function {
+  func.func @main(%arg0: tensor<5xf32>) -> tensor<i32> {
+    // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<16>
+    // CHECK: return [[RESULT0]]
+    %0 = stablehlo.constant dense<4> : tensor<i32>
+    %1 = call @refine_call_callee(%0, %arg0) : (tensor<i32>, tensor<5xf32>) -> tensor<i32>
+    %2 = call @refine_call_callee(%0, %arg0) : (tensor<i32>, tensor<5xf32>) -> tensor<i32>
+    %3 = stablehlo.add %1, %2: tensor<i32>
+    return %3 : tensor<i32>
+  }
+  func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<5xf32>) -> tensor<i32> {
+    // CHECK: [[RESULT1:%.*]] = stablehlo.constant dense<8>
+    // CHECK: return [[RESULT1]]
+    %0 = stablehlo.add %arg0, %arg0: tensor<i32>
+    return %0 : tensor<i32>
+  }
+}
+
+// -----
+
+module @refine_call_multiple_with_different_number_dimension_arguments {
+  func.func @main(%arg1: tensor<4xf32>) -> tensor<?xf32> {
+    %0 = stablehlo.bitcast_convert %arg1 : (tensor<4xf32>) -> tensor<?xf32>
+    %arg0_new = "stablehlo.get_dimension_size"(%0) {dimension = 0 : i64} : (tensor<?xf32>) -> tensor<i32>
+    %1 = call @refine_call_callee(%arg0_new, %0) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    // Ensure that the first argument is not a constant at the second call site
+    %arg0_different_f32 = stablehlo.bitcast_convert %arg0_new : (tensor<i32>) -> tensor<f32>
+    %arg0_different_i32 = stablehlo.bitcast_convert %arg0_different_f32 : (tensor<f32>) -> tensor<i32>
+    // expected-error@+1{{incorrect number of operands for callee}}
+    %2 = call @refine_call_callee(%arg0_different_i32, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    return %2 : tensor<?xf32>
+  }
+  // expected-error@+1{{Function refine_call_callee has already been refined with a different refinement context. Previous context had 1 and now we have 2 non-dimension arguments}}
+  func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
+    return %arg1 : tensor<?xf32>
+  }
+}
+
+// -----
+
+module @refine_call_multiple_different_dimension_arguments {
+  func.func @main(%arg1: tensor<4xf32>) -> tensor<?xf32> {
+    %0 = stablehlo.bitcast_convert %arg1 : (tensor<4xf32>) -> tensor<?xf32>
+    %arg0_new = "stablehlo.get_dimension_size"(%0) {dimension = 0 : i64} : (tensor<?xf32>) -> tensor<i32>
+    %1 = call @refine_call_callee(%arg0_new, %0) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    %arg0_different = stablehlo.add %arg0_new, %arg0_new : tensor<i32>
+    // expected-error@+1{{incorrect number of operands for callee}}
+    %2 = call @refine_call_callee(%arg0_different, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    return %2 : tensor<?xf32>
+  }
+  // expected-error@+1{{Function refine_call_callee has already been refined with a different refinement context.}}
+  func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
+    return %arg1 : tensor<?xf32>
+  }
+}
+
+// -----
+
+module @refine_call_multiple_different_non_dimension_arguments {
+  func.func @main(%arg1: tensor<4xf32>) -> tensor<?xf32> {
+    %0 = stablehlo.bitcast_convert %arg1 : (tensor<4xf32>) -> tensor<?xf32>
+    %arg0_new = "stablehlo.get_dimension_size"(%0) {dimension = 0 : i64} : (tensor<?xf32>) -> tensor<i32>
+    %1 = call @refine_call_callee(%arg0_new, %0) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    %2 = stablehlo.constant dense<[1., 2.]> : tensor<2xf32>
+    %3 = stablehlo.concatenate %1, %2, dim = 0 : (tensor<?xf32>, tensor<2xf32>) -> tensor<?xf32>
+    // expected-error@+1{{incorrect number of operands for callee}}
+    %4 = call @refine_call_callee(%arg0_new, %3) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
+    return %4 : tensor<?xf32>
+  }
+  // expected-error@+1{{Function refine_call_callee has already been refined with a different refinement context.}}
+  func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
+    return %arg1 : tensor<?xf32>
+  }
+}
+
+// -----
+
+module @refine_call_recursive {
+  func.func @main() -> tensor<i32> {
+    %0 = stablehlo.constant dense<3> : tensor<i32>
+    %1 = call @refine_call_callee(%0) : (tensor<i32>) -> tensor<i32>
+    return %1 : tensor<i32>
+  }
+  // expected-error@+1{{Function refine_call_callee is being refined recursively}}
+  func.func @refine_call_callee(%arg0: tensor<i32>) -> tensor<i32> {
+    // expected-error@+1{{incorrect number of operands}}
+    %0 = call @refine_call_callee(%arg0) : (tensor<i32>) -> tensor<i32>
+    return %0 : tensor<i32>
+  }
+}
+
+// -----
+
+module @refine_call_main_argument_unranked {
+  // expected-error@+1{{main must be refined with static shape arguments}}
+  func.func public @main(%arg0: tensor<*xi32>) -> tensor<*xi32> {
+    %2 = call @callee(%arg0) : (tensor<*xi32>) -> tensor<*xi32>
+    return %2 : tensor<*xi32>
+  }
+  func.func private @callee(%arg0: tensor<*xi32>) -> tensor<*xi32> {
+    return %arg0 : tensor<*xi32>
+  }
+}
+
+// -----
+
+module @refine_call_main_argument_dynamic_shape {
+  // expected-error@+1{{main must be refined with static shape arguments}}
+  func.func public @main(%arg0: tensor<?xi32>) -> tensor<?xi32> {
+    %2 = call @callee(%arg0) : (tensor<?xi32>) -> tensor<?xi32>
+    return %2 : tensor<?xi32>
+  }
+  func.func private @callee(%arg0: tensor<?xi32>) -> tensor<?xi32> {
+    return %arg0 : tensor<?xi32>
+  }
+}
+
+// -----
+
+module @refine_call_callee_argument_unranked {
+  func.func public @main(%arg0: tensor<1xi64>) -> tensor<*xi32> {
+    %1 = stablehlo.dynamic_iota %arg0, dim = 0 : (tensor<1xi64>) -> tensor<*xi32>
+    %2 = call @callee(%1) : (tensor<*xi32>) -> tensor<*xi32>
+    return %2 : tensor<*xi32>
+  }
+  // expected-error@+1{{callee must be refined with static shape arguments}}
+  func.func private @callee(%arg0: tensor<*xi32>) -> tensor<*xi32> {
+    return %arg0 : tensor<*xi32>
+  }
+}
+
+// -----
+
+module @refine_call_callee_argument_dynamic_shape {
+  func.func public @main(%arg0: tensor<1xi64>) -> tensor<?xi32> {
+    %1 = stablehlo.dynamic_iota %arg0, dim = 0 : (tensor<1xi64>) -> tensor<?xi32>
+    %2 = call @callee(%1) : (tensor<?xi32>) -> tensor<?xi32>
+    return %2 : tensor<?xi32>
+  }
+  // expected-error@+1{{callee must be refined with static shape arguments}}
+  func.func private @callee(%arg0: tensor<?xi32>) -> tensor<?xi32> {
+    return %arg0 : tensor<?xi32>
+  }
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_call_dimension_argument_non_scalar
+// The non-scalar constant is not folded into the callee
+module @refine_call_dimension_argument_non_scalar {
+  func.func public @main() -> tensor<4xi32> {
+    // CHECK: dense<[1, 2, 3, 4]> : tensor<4xi32>
+    %0 = stablehlo.constant dense<[1, 2, 3, 4]> : tensor<4xi32>
+    %1 = call @callee(%0) : (tensor<4xi32>) -> tensor<4xi32>
+    return %1 : tensor<4xi32>
+  }
+  func.func private @callee(%arg0: tensor<4xi32>) -> tensor<4xi32> {
+    // CHECK: return %arg0 : tensor<4xi32>
+    return %arg0 : tensor<4xi32>
+  }
+}
+
+// -----
+
+// CHECK-LABEL: module @refine_call_dimension_argument_not_integer
+module @refine_call_dimension_argument_not_integer {
+  func.func public @main() -> tensor<f32> {
+    %0 = stablehlo.constant dense<3.> : tensor<f32>
+    // CHECK: call @callee({{.*}}) : (tensor<f32>) -> tensor<f32>
+    %2 = call @callee(%0) : (tensor<f32>) -> tensor<f32>
+    return %2 : tensor<f32>
+  }
+  func.func private @callee(%arg0: tensor<f32>) -> tensor<f32> {
+    return %arg0 : tensor<f32>
+  }
 }
 
 // -----
@@ -607,12 +903,55 @@
 
 // -----
 
+// CHECK-LABEL: @main
+func.func @main(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<*xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window{{.*}} -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<*xf32>
+  func.return %5 : tensor<*xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
 // CHECK-LABEL: @refine_dynamic_reshape
 func.func @refine_dynamic_reshape(%arg0: tensor<4xf32>) -> tensor<*xf32> {
   // CHECK: stablehlo.dynamic_reshape{{.*}} -> tensor<1x4xf32>
   %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
   %1 = stablehlo.dynamic_reshape %arg0, %0 : (tensor<4xf32>, tensor<2xi64>) -> tensor<*xf32>
   func.return %1 : tensor<*xf32>
+}
+
+// -----
+
+// CHECK-LABEL: @refine_dynamic_rng_bit_generator
+func.func @refine_dynamic_rng_bit_generator(%arg0: tensor<2xui64>) -> (tensor<?xui64>, tensor<*xf32>) {
+  // CHECK: stablehlo.dynamic_rng_bit_generator{{.*}} -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<?xui64>, tensor<*xf32>)
+  func.return %1#0, %1#1 : tensor<?xui64>, tensor<*xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_dynamic_top_k
+func.func @refine_dynamic_top_k(%arg0: tensor<16xf32>) -> (tensor<?xf32>, tensor<?xi32>) {
+  // CHECK: stablehlo.dynamic_top_k{{.*}} -> (tensor<4xf32>, tensor<4xi32>)
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<?xf32>, tensor<?xi32>)
+  return %1#0, %1#1 : tensor<?xf32>, tensor<?xi32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/transforms/Passes.td b/stablehlo/stablehlo/transforms/Passes.td
--- stablehlo/stablehlo/transforms/Passes.td
+++ stablehlo/stablehlo/transforms/Passes.td
@@ -25,6 +25,7 @@
     For example, if the output_shape operand of DynamicReshapeOp is a constant
     value, then the operation can be transformed to ReshapeOp.
   }];
+  let dependentDialects = ["mlir::chlo::ChloDialect"];
 }
 
 def StablehloLegalizeToVhloPass : Pass<"stablehlo-legalize-to-vhlo", "ModuleOp"> {
diff --ruN a/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
@@ -24,6 +24,8 @@
 #include "mlir/Interfaces/InferTypeOpInterface.h"
 #include "mlir/Support/LogicalResult.h"
 #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/ExperimentalOps.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/transforms/Passes.h"
 
@@ -198,6 +200,54 @@
   }
 };
 
+struct CanonicalizeDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // ReduceWindowOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op, "expected static window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op, "expected static base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected static padding");
+    auto newOp = rewriter.create<ReduceWindowOp>(
+        op->getLoc(), op->getResultTypes(), op.getInputs(), op.getInitValues(),
+        rewriter.getI64TensorAttr(windowDimensions),
+        rewriter.getI64TensorAttr(windowStrides),
+        rewriter.getI64TensorAttr(baseDilations),
+        rewriter.getI64TensorAttr(windowDilations),
+        hlo::getPaddingAttr(&rewriter, padding));
+
+    // Inline the called computation into newOp.
+    // This is somewhat annoying because we also have to rewrite the original
+    // func::ReturnOp into stablehlo::ReturnOp.
+    rewriter.cloneRegionBefore(op.getBody(), newOp.getBody(),
+                               newOp.getBody().end());
+    auto funcReturnOp =
+        cast<func::ReturnOp>(newOp.getBody().front().getTerminator());
+    rewriter.setInsertionPointToEnd(&newOp.getBody().front());
+    rewriter.replaceOpWithNewOp<stablehlo::ReturnOp>(
+        funcReturnOp, funcReturnOp.getOperands());
+    rewriter.replaceOp(op, newOp->getResults());
+    return success();
+  }
+};
+
 struct CanonicalizeDynamicReshapeOpPattern
     : public OpRewritePattern<DynamicReshapeOp> {
   using OpRewritePattern::OpRewritePattern;
@@ -210,6 +260,56 @@
     if (!op.getType().hasStaticShape())
       return rewriter.notifyMatchFailure(op, "expected static result type");
     rewriter.replaceOpWithNewOp<ReshapeOp>(op, op.getType(), op.getOperand());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // This pattern ignores and discards the output_shape operand. We rely on
+    // the verifier to make sure that its value is consistent with result type.
+    if (!succeeded(hlo::matchInts(op.getOutputShape())))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getOutput().getType().cast<ShapedType>().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static output type");
+    rewriter.replaceOpWithNewOp<RngBitGeneratorOp>(
+        op, op->getResultTypes(), op.getRngAlgorithm(), op.getInitialState());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicTopKOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(impl, "expected constant k");
+
+    // We rely on many of the properties checked by verification.
+    auto valuesType = op.getValues().getType().cast<ShapedType>();
+    auto valuesLastDimSize = valuesType.getShape()[valuesType.getRank() - 1];
+    if (hlo::isDynamicDimSize(valuesLastDimSize) ||
+        valuesLastDimSize != k[0])
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected value of k to match the values last dimension size of "
+          "static values type (result #0)");
+
+    rewriter.replaceOpWithNewOp<chlo::TopKOp>(
+        op, op->getResultTypes(), op.getOperand(), k[0]);
     return success();
   }
 };
@@ -320,7 +420,10 @@
     patterns.add<CanonicalizeDynamicGatherOpPattern>(&getContext());
     patterns.add<CanonicalizeDynamicIotaOpPattern>(&getContext());
     patterns.add<CanonicalizeDynamicPadOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicReduceWindowOpPattern>(&getContext());
     patterns.add<CanonicalizeDynamicReshapeOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicTopKOpPattern>(&getContext());
     patterns.add<CanonicalizeRealDynamicSliceOpToDynamicSliceOpPattern>(
         &getContext());
     patterns.add<CanonicalizeRealDynamicSliceOpToSliceOpPattern>(&getContext());
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -11,9 +11,48 @@
 See the License for the specific language governing permissions and
 limitations under the License.
 ==============================================================================*/
-
+/*
+This shape refinement pass was designed to resolve the dynamic shapes in
+a StableHLO module produced by JAX serialization with shape polymorphism.
+Such a module has the following properties:
+
+  * it contains a "main" function with statically-shaped arguments;
+    the result types may be dynamically shaped.
+  * all the dynamic shapes depend only on the input shapes (no shape
+    dependency on the input array contents). We refer to the operations that
+    depend transitively only on the input shapes (e.g., as given by
+    `stablehlo.get_dimension_size`) as `dimension` operations.
+    All dimension values can be resolved to constants through inter-procedural
+    constant folding.
+  * intermediate functions may take a number of token arguments (of type
+    !stablehlo.token) at the start of the argument list, followed by some
+    dimension arguments (integer scalars).
+  * some intermediate functions may return dimension values.
+    E.g., the `floordiv` operation on dimension values may be implemented
+    using intermediate functions. These constant functions need to be
+    constant-folded.
+  * All the dynamic shapes can be resolved through shape inference from the
+    dimension values. The dimension values themselves do not depend on the
+    result of shape inference.
+
+
+For each intermediate function we compute a refinement context, including
+the values of the dimension arguments and the static shapes of the other
+arguments. We compute the refinement context when we encounter a function call,
+and then we refine the callee recursively. We abort in the presence of
+recursive calls.
+We also abort if a function is called with multiple distinct refinement
+contexts.
+
+After refinement, all operations should have static shapes, all calls to
+constant functions are replaced with constants, and all dimension arguments
+for intermediate functions are dropped and are replaced with constants.
+*/
+#include <algorithm>
 #include <cstdint>
 #include <memory>
+#include <optional>
+#include <set>
 #include <string>
 #include <utility>
 
@@ -24,8 +63,10 @@
 #include "llvm/ADT/SmallSet.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/StringRef.h"
+#include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/FormatVariadic.h"
+#include "llvm/Support/ScopedPrinter.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
 #include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinOps.h"
@@ -39,10 +80,13 @@
 #include "mlir/IR/Types.h"
 #include "mlir/IR/Value.h"
 #include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/DebugStringHelper.h"
 #include "mlir/Support/LogicalResult.h"
+#include "mlir/Support/LLVM.h"
 #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
 #include "stablehlo/dialect/Base.h"
 #include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/ExperimentalOps.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/dialect/TypeInference.h"
 #include "stablehlo/transforms/Passes.h"
@@ -50,10 +94,144 @@
 namespace mlir {
 namespace stablehlo {
 
+#define DEBUG_TYPE "stablehlo-refine-shapes"
+
 #define GEN_PASS_DEF_STABLEHLOREFINESHAPESPASS
 #include "stablehlo/transforms/Passes.h.inc"
 
 namespace {
+
+// Per-module state for shape refinement.
+class RefineShapeState {
+ public:
+  // Validates that we are not attempting to refine a function with a different
+  // context than previously, and are not attempting recursive refinement.
+  // Returns failure() if validation fails. On success, returns a boolean
+  // that specifies whether the function has already been refined.
+  FailureOr<bool> validateFunctionRefinement(
+      func::FuncOp func, SmallVector<APSInt> dimensionArguments,
+      SmallVector<Type> nonDimensionArgumentTypes) {
+    StringRef funcName = func.getName();
+    auto found = refinementContexts.find(func);
+    if (found == refinementContexts.end()) {
+      return false;  // not already refined.
+    }
+    auto prevDimensionArguments = std::get<0>(found->second);
+    auto prevNonDimensionArgumentTypes = std::get<1>(found->second);
+    // Since we refine until fixed point, we will refine a call to a function
+    // both for the original function and for the refined one. In the latter
+    // case, we should have empty dimensionArguments but the same
+    // nonDimensionArgumentTypes.
+    if (prevNonDimensionArgumentTypes != nonDimensionArgumentTypes ||
+        (!dimensionArguments.empty() &&
+         prevDimensionArguments != dimensionArguments)) {
+      emitDifferentRefinementContextError(
+          func, /*dimensionArguments=*/dimensionArguments,
+          /*nonDimensionArgumentTypes=*/nonDimensionArgumentTypes,
+          /*prevDimensionArguments=*/prevDimensionArguments,
+          /*prevNonDimensionArgumentShapes=*/prevNonDimensionArgumentTypes);
+      return failure();
+    }
+    for (auto funcOnStack : functionsBeingRefined) {
+      if (funcOnStack == funcName) {
+        func.emitOpError() << "Function " << funcName
+                           << " is being refined recursively\n";
+        return failure();
+      }
+    }
+    return true;  // already refined.
+  }
+
+  // Updates the state to signal the starting of a function refinement.
+  // Callers must call `finishFunctionRefinement` when done.
+  void startFunctionRefinement(func::FuncOp func,
+                               SmallVector<APSInt> dimensionArguments,
+                               SmallVector<Type> nonDimensionArgumentTypes) {
+    StringRef funcName = func.getName();
+    functionsBeingRefined.push_back(funcName);
+    refinementContexts[func] =
+        std::make_tuple(dimensionArguments, nonDimensionArgumentTypes);
+  }
+
+  // Updates the state to signal the starting of a function refinement.
+  LogicalResult finishFunctionRefinement(func::FuncOp func) {
+    if (func.getName() !=
+        functionsBeingRefined[functionsBeingRefined.size() - 1]) {
+      func.emitOpError() << "Expected to find " << func.getName()
+                         << " at the top of the stack";
+      return failure();
+    }
+    functionsBeingRefined.pop_back();
+    return success();
+  }
+
+ private:
+  // Maps refined functions to the refinement context: the values of dimension
+  // arguments and the types of non-dimension arguments. A function is added
+  // here when we start refining it.
+  DenseMap<func::FuncOp, std::tuple<SmallVector<APSInt>, SmallVector<Type>>>
+      refinementContexts;
+
+  // A stack of functions that are in the process of being refined, the current
+  // one is last.
+  SmallVector<llvm::StringRef> functionsBeingRefined;
+
+  void emitDifferentRefinementContextError(
+      func::FuncOp func, SmallVector<APSInt> dimensionArguments,
+      SmallVector<Type> nonDimensionArgumentTypes,
+      SmallVector<APSInt> prevDimensionArguments,
+      SmallVector<Type> prevNonDimensionArgumentShapes) {
+    InFlightDiagnostic msg = func.emitOpError();
+    msg << "Function " << func.getName()
+        << " has already been refined with a different "
+           "refinement context. ";
+    int countShowNonDimensionArguments =
+        std::min(prevNonDimensionArgumentShapes.size(),
+                 nonDimensionArgumentTypes.size());
+    if (prevNonDimensionArgumentShapes.size() !=
+        nonDimensionArgumentTypes.size()) {
+      msg << "Previous context had " << prevNonDimensionArgumentShapes.size()
+          << " and now we have " << nonDimensionArgumentTypes.size()
+          << " non-dimension arguments. ";
+    }
+    msg << "The differences among the first " << countShowNonDimensionArguments
+        << " non-dimension argument types are: ";
+    for (auto i = 0; i < countShowNonDimensionArguments; ++i) {
+      if (prevNonDimensionArgumentShapes[i] != nonDimensionArgumentTypes[i]) {
+        msg << "Non-dimension argument[" << i << "] previously had type "
+            << debugString(prevNonDimensionArgumentShapes[i])
+            << " and now has type " << debugString(nonDimensionArgumentTypes[i])
+            << ". ";
+      }
+    }
+    int countShowDimensionArguments =
+        std::min(prevDimensionArguments.size(), dimensionArguments.size());
+    if (prevDimensionArguments.size() != dimensionArguments.size()) {
+      msg << "Previous context had " << prevDimensionArguments.size()
+          << " and now we have " << dimensionArguments.size()
+          << " dimension arguments. ";
+    }
+    msg << "The differences among the first " << countShowDimensionArguments
+        << " dimension arguments are: ";
+    for (auto i = 0; i < countShowDimensionArguments; ++i) {
+      if (prevDimensionArguments[i] != dimensionArguments[i]) {
+        msg << "Dimension argument[" << i << "] previously was "
+            << prevDimensionArguments[i].getSExtValue() << " and now is "
+            << dimensionArguments[i].getSExtValue() << ". ";
+      }
+    }
+  }
+};
+
+// Refines a function.
+// Returns `true` if the function had already been processed with the same
+// refinement context and `false` if this is the first time we refined the
+// function. Returns failure() if we encounter an error.
+LogicalResult refineFunction(func::FuncOp func, MLIRContext* context,
+                             RefineShapeState* state,
+                             size_t nrPrefixTokenArguments,
+                             SmallVector<APSInt> dimensionArguments,
+                             SmallVector<Type> nonDimensionArgumentTypes);
 
 // DenseElementsAttr can be constructed from ArrayRef<APInt> but not from
 // ArrayRef<APSInt>. This helper bridges the gap.
@@ -424,11 +602,10 @@
       diag << "refineValues failed for " << types << ": expected "
            << values.size() << " types, got " << types.size();
     });
-
-  // Check whether `types` contain any new information with respect to existing
-  // return types. Even if just a single dimension size out of an entire tensor
-  // type got updated, using `inferMostSpecificType` ensures that we don't
-  // miss that.
+  // Check whether `types` contain any new information with respect to
+  // existing return types. Even if just a single dimension size out of an
+  // entire tensor type got updated, using `inferMostSpecificType` ensures
+  // that we don't miss that.
   bool needsRefinement = false;
   SmallVector<Type> refinedTypes;
   for (auto it : llvm::zip(values.getTypes(), types)) {
@@ -468,11 +645,13 @@
 
       // Simply changing operand type of `func.return` won't work because
       // that won't update the FunctionType of the enclosing `func.func`.
-      // Nonetheless, we still want to support these ops because they are widely
-      // used in StableHLO programs (although the plan of record is to replace
-      // `func.return` ops in StableHLO programs with `stablehlo.return`:
-      // https://github.com/openxla/stablehlo/issues/425).
+      // Nonetheless, we still want to support these ops because they are
+      // widely used in StableHLO programs (although the plan of record is to
+      // replace `func.return` ops in StableHLO programs with
+      // `stablehlo.return`: https://github.com/openxla/stablehlo/issues/425).
       if (isa<func::ReturnOp>(user)) continue;
+
+      if (isa<func::CallOp>(user)) continue;
 
       // Unlike in TensorFlow's type inference pass, here we work only with
       // allowlisted ops to focus our support on well-defined semantics of
@@ -489,7 +668,8 @@
     value.setType(refinedType);
 
     // Special case: for `func.return`, guard the refinement with a cast
-    // and leave propagation of the refined return type to a dedicated pattern.
+    // and leave propagation of the refined return type to a dedicated
+    // pattern.
     auto isFuncReturn = [](OpOperand& use) -> bool {
       return isa<func::ReturnOp>(use.getOwner());
     };
@@ -505,8 +685,8 @@
 
 // Refines the return types of the given operation using the given types.
 // This function also signals PatternRewriter that it needs to visit all the
-// users of this op if any updates to its results have happened during execution
-// of the function.
+// users of this op if any updates to its results have happened during
+// execution of the function.
 LogicalResult refineReturnTypes(PatternRewriter& rewriter, Operation* op,
                                 ArrayRef<Type> types) {
   if (failed(refineValues(rewriter, op, op->getResults(), types)))
@@ -528,12 +708,12 @@
 //      traversal, and only then we apply the refinements. If there are other
 //      types, then the corresponding refinements must be completely empty.
 //   2) Encodings are not supported. In principle, TypeExtensions should be
-//      supportable, but this needs careful thinking through. Given that no one
-//      asked for support for bounded dynamism in this pass yet, this is left
-//      for future work.
+//      supportable, but this needs careful thinking through. Given that no
+//      one asked for support for bounded dynamism in this pass yet, this is
+//      left for future work.
 // This function also signals PatternRewriter that it needs to visit all the
-// users of this op if any updates to its results have happened during execution
-// of the function.
+// users of this op if any updates to its results have happened during
+// execution of the function.
 LogicalResult refineReturnTypes(PatternRewriter& rewriter, Operation* op,
                                 ArrayRef<ShapedTypeComponents> refinements) {
   SmallVector<Type> flattenedTypes;
@@ -623,8 +803,8 @@
 
 // Refines the return type of the given operation using the given shape.
 // This function also signals PatternRewriter that it needs to visit all the
-// users of this op if any updates to its results have happened during execution
-// of the function.
+// users of this op if any updates to its results have happened during
+// execution of the function.
 template <typename OpType>
 LogicalResult refineReturnShape(PatternRewriter& rewriter, OpType op,
                                 ArrayRef<int64_t> shape) {
@@ -633,8 +813,8 @@
 
 // Refines the return type of the given operation using the given shape.
 // This function also signals PatternRewriter that it needs to visit all the
-// users of this op if any updates to its results have happened during execution
-// of the function.
+// users of this op if any updates to its results have happened during
+// execution of the function.
 template <typename OpType>
 LogicalResult refineReturnShape(PatternRewriter& rewriter, OpType op,
                                 Value shapeValue) {
@@ -647,6 +827,52 @@
   return refineReturnShape(rewriter, op, shape);
 }
 
+// Dimension arguments are leading scalar constant arguments, optionally
+// preceeded by some stablehlo.token arguments.
+SmallVector<APSInt> getDimensionArguments(func::CallOp callOp,
+                                          size_t* nrPrefixTokenArguments) {
+  *nrPrefixTokenArguments = 0;
+  SmallVector<Value> operands = callOp.getOperands();
+  SmallVector<APSInt> dimensionArguments;
+  for (size_t i = 0; i < operands.size(); ++i) {
+    if (i == *nrPrefixTokenArguments && isa<TokenType>(operands[i].getType())) {
+      (*nrPrefixTokenArguments)++;
+      continue;
+    }
+    RankedTensorType operandType =
+        dyn_cast<RankedTensorType>(operands[i].getType());
+    if (!operandType || operandType.getRank() != 0 ||
+        !operandType.getElementType().template isa<IntegerType>())
+      break;
+    SmallVector<APSInt> operand_int;
+    if (failed(hlo::matchInts(operands[i], operand_int))) {
+      break;
+    }
+    dimensionArguments.push_back(operand_int[0]);
+  }
+  return dimensionArguments;
+}
+
+std::optional<SmallVector<DenseIntElementsAttr>> isConstantFunction(
+    func::FuncOp func) {
+  LLVM_DEBUG(llvm::dbgs() << "check if " << func.getName()
+                          << " is a constant function\n");
+  SmallVector<DenseIntElementsAttr> returnedConstants;
+  func::ReturnOp ret = *func.getOps<func::ReturnOp>().begin();
+  bool isConstant = llvm::all_of(ret->getOperands(), [&](auto returnVal) {
+    DenseIntElementsAttr attr;
+    Operation* return_operand_def = returnVal.getDefiningOp();
+    if (return_operand_def &&
+        matchPattern(return_operand_def, m_Constant(&attr))) {
+      returnedConstants.push_back(attr);
+      return true;
+    }
+    return false;
+  });
+  if (isConstant) return returnedConstants;
+  return std::nullopt;
+}
+
 struct RefineAllGatherOpPattern : public OpRewritePattern<AllGatherOp> {
   using OpRewritePattern::OpRewritePattern;
   LogicalResult matchAndRewrite(AllGatherOp op,
@@ -655,9 +881,9 @@
     if (!operandType.hasRank())
       return rewriter.notifyMatchFailure(op, "expected ranked operand type");
 
-    // This represents the cross_replica_and_partition process grouping strategy
-    // that requires num_partitions to compute shardCount. Since we don't know
-    // num_partitions at this point, we error out.
+    // This represents the cross_replica_and_partition process grouping
+    // strategy that requires num_partitions to compute shardCount. Since we
+    // don't know num_partitions at this point, we error out.
     if (op.getChannelHandle() && !op.getUseGlobalDeviceIds())
       return rewriter.notifyMatchFailure(op, "unsupported strategy");
     DenseIntElementsAttr replicaGroups = op.getReplicaGroups();
@@ -678,12 +904,11 @@
     auto operandType = op.getOperand().getType();
     if (!operandType.hasRank())
       return rewriter.notifyMatchFailure(op, "expected ranked operand type");
-
+    auto resultType = op.getType();
     // If bit widths of the operand and the result are different, then
     // operand and result shapes have different ranks.
     // This complicates the logic quite a bit and is not needed to pass the
     // current tests, so we leave this for future work.
-    auto resultType = op.getType();
     auto getBitWidthFn = [](ShapedType type) {
       auto elementType = type.getElementType();
       if (auto complexType = elementType.dyn_cast<ComplexType>())
@@ -694,8 +919,77 @@
     if (getBitWidthFn(operandType) != getBitWidthFn(resultType))
       return rewriter.notifyMatchFailure(op, "unsupported bit width");
 
-    return refineReturnShape(rewriter, op, operandType.getShape());
-  }
+    auto res = refineReturnShape(rewriter, op, operandType.getShape());
+    if (failed(res)) return failure();
+    if (op.getOperand().getType() == op.getResult().getType()) {
+      LLVM_DEBUG({ llvm::dbgs() << "    ** remove no-op bitcast convert\n"; });
+      rewriter.replaceOp(op, op.getOperand());
+    }
+    return success();
+  }
+};
+
+struct RefineCallOpPattern : public OpRewritePattern<func::CallOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  RefineCallOpPattern(MLIRContext* context, RefineShapeState* state)
+      : OpRewritePattern<func::CallOp>(context), _state(state) {}
+
+  LogicalResult matchAndRewrite(func::CallOp op,
+                                PatternRewriter& rewriter) const override {
+    LLVM_DEBUG({ llvm::dbgs() << "refineCallOp " << debugString(op) << "\n"; });
+
+    // We have a number of prefix token arguments, then the dimension arguments
+    size_t nrPrefixTokenArguments = 0;
+    SmallVector<APSInt> dimensionArguments =
+        getDimensionArguments(op, &nrPrefixTokenArguments);
+    SmallVector<Type> nonDimensionArgumentTypes;
+    SmallVector<Value> nonDimensionArguments;
+    SmallVector<Value> operands = op.getOperands();
+    for (size_t i = 0; i < operands.size(); ++i) {
+      // Skip the dimension arguments.
+      if (i >= nrPrefixTokenArguments &&
+          i < nrPrefixTokenArguments + dimensionArguments.size()) {
+        continue;
+      }
+      nonDimensionArgumentTypes.push_back(operands[i].getType());
+      nonDimensionArguments.push_back(operands[i]);
+    }
+    FlatSymbolRefAttr calleeName = op.getCalleeAttr();
+    const SymbolTable symbolTable(op->getParentOfType<ModuleOp>());
+    func::FuncOp callee = dyn_cast<func::FuncOp>(
+        symbolTable.lookupNearestSymbolFrom(op, calleeName.getAttr()));
+    if (!callee)
+      return rewriter.notifyMatchFailure(
+          op, "cannot find callee in the current scope");
+    if (failed(refineFunction(callee, rewriter.getContext(), _state,
+                              nrPrefixTokenArguments, dimensionArguments,
+                              nonDimensionArgumentTypes)))
+      return failure();
+
+    // Is the callee a constant function in this refinement context?
+    std::optional<SmallVector<DenseIntElementsAttr>> constantAttrs =
+        isConstantFunction(callee);
+    if (constantAttrs.has_value()) {
+      SmallVector<Value> constants;
+      for (auto constAttr : constantAttrs.value()) {
+        constants.push_back(
+            rewriter.create<ConstantOp>(op.getLoc(), constAttr));
+      }
+      rewriter.replaceOp(op, constants);
+      return success();
+    }
+    if (!dimensionArguments.empty()) {
+      // Drop the dimension arguments, but only if necessary, or else we
+      // will end up trying to refine the new CallOp forever.
+      op = rewriter.replaceOpWithNewOp<func::CallOp>(
+          op, op.getResultTypes(), callee.getSymName(), nonDimensionArguments);
+    }
+    return refineReturnTypes(rewriter, op, callee.getResultTypes());
+  }
+
+ private:
+  RefineShapeState* _state;
 };
 
 struct RefineConvertOpPattern : public OpRewritePattern<ConvertOp> {
@@ -844,12 +1138,98 @@
   }
 };
 
+struct RefineDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected constant padding");
+
+    SmallVector<ShapedTypeComponents> inferredReturnTypes;
+    if (failed(hlo::inferReduceWindowOp(
+            /*location=*/{}, op.getInputs(), op.getInitValues(),
+            rewriter.getI64TensorAttr(windowDimensions),
+            rewriter.getI64TensorAttr(windowStrides),
+            rewriter.getI64TensorAttr(baseDilations),
+            rewriter.getI64TensorAttr(windowDilations),
+            hlo::getPaddingAttr(&rewriter, padding), inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferReduceWindowOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
 struct RefineDynamicReshapeOpPattern
     : public OpRewritePattern<DynamicReshapeOp> {
   using OpRewritePattern::OpRewritePattern;
   LogicalResult matchAndRewrite(DynamicReshapeOp op,
                                 PatternRewriter& rewriter) const override {
     return refineReturnShape(rewriter, op, op.getOutputShape());
+  }
+};
+
+struct RefineDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    auto initialStateType = op.getInitialState().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape;
+    if (failed(hlo::matchInts(op.getOutputShape(), outputShape)))
+      return rewriter.notifyMatchFailure(op, "expected constant output_shape");
+
+    // We only need to refine the shape of `output` (the second result).
+    // The shape of `output_state` (the first result) is determined by the
+    // shape of `initial_state`, so we ignore it and provide an empty
+    // refinement.
+    return refineReturnTypes(rewriter, op, {{initialStateType}, {outputShape}});
+  }
+};
+
+struct RefineDynamicTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(op, "expected constant k");
+
+    outputShape[operandType.getRank() - 1] = k[0];
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
   }
 };
 
@@ -865,11 +1245,11 @@
     if (!isa<chlo::ChloDialect, StablehloDialect>(op->getDialect()))
       return rewriter.notifyMatchFailure(op, "unsupported dialect");
 
-    // For the ops that implement InferTypeOpInterface, we reinfer their return
-    // types and see what happens.
-    // Operands of these ops might have been refined elsewhere (e.g. someone
-    // might have updated argument types of a function) or earlier during this
-    // pass, and this might enable refinement opportunities downstream.
+    // For the ops that implement InferTypeOpInterface, we reinfer their
+    // return types and see what happens. Operands of these ops might have
+    // been refined elsewhere (e.g. someone might have updated argument types
+    // of a function) or earlier during this pass, and this might enable
+    // refinement opportunities downstream.
     SmallVector<Type> inferredReturnTypes;
     if (failed(op.inferReturnTypes(getContext(), /*location=*/{},
                                    op->getOperands(), op->getAttrDictionary(),
@@ -925,8 +1305,8 @@
           sliceSizesAttr.size(),
           RankedTensorType::get({}, startIndicesElementType));
 
-      // RealDynamicSliceOp can take tensors of integer or index element types.
-      // DynamicSliceOp::slice_sizes only supports i64 element type.
+      // RealDynamicSliceOp can take tensors of integer or index element
+      // types. DynamicSliceOp::slice_sizes only supports i64 element type.
       // Adapt accordingly in order to be compatible with inferDynamicSliceOp.
       SmallVector<int64_t> sliceSizes;
       for (auto element : sliceSizesAttr.getValues<APInt>()) {
@@ -956,9 +1336,9 @@
     if (!operandType.hasRank())
       return rewriter.notifyMatchFailure(op, "expected ranked operand type");
 
-    // This represents the cross_replica_and_partition process grouping strategy
-    // that requires num_partitions to compute shardCount. Since we don't know
-    // num_partitions at this point, we error out.
+    // This represents the cross_replica_and_partition process grouping
+    // strategy that requires num_partitions to compute shardCount. Since we
+    // don't know num_partitions at this point, we error out.
     if (op.getChannelHandle() && !op.getUseGlobalDeviceIds())
       return rewriter.notifyMatchFailure(op, "unsupported strategy");
     DenseIntElementsAttr replicaGroups = op.getReplicaGroups();
@@ -998,9 +1378,9 @@
                                 PatternRewriter& rewriter) const override {
     // Push the potentially refined operand types into the nested regions.
     // This can lead to refinements of the return types of the body (but not
-    // of the cond since it always returns tensor<i1>), but the key insight here
-    // is that the enclosing while op doesn't care about these refinements
-    // (because its return types are equal to its operand types).
+    // of the cond since it always returns tensor<i1>), but the key insight
+    // here is that the enclosing while op doesn't care about these
+    // refinements (because its return types are equal to its operand types).
     // If we end up with incompatibilities between while's return types and
     // body's return types, the verifier will tell us about that. This means
     // that the original program wasn't well-formed. TODO(burmako): Implement
@@ -1050,8 +1430,8 @@
       if (failed(mostSpecificType) || destType == *mostSpecificType) continue;
 
       // If the source type of the cast is more specific than the target type,
-      // then we conclude that the cast is redundant (i.e. needs to be removed)
-      // and that the return type of the function needs an update.
+      // then we conclude that the cast is redundant (i.e. needs to be
+      // removed) and that the return type of the function needs an update.
       needsUpdate = true;
       updatedResultTypes[i] = sourceType;
 
@@ -1066,9 +1446,6 @@
     for (auto cast : castsToReplace)
       rewriter.replaceOp(cast, cast->getOperands());
 
-    // If the type of the enclosing `func.func` needs an update, we simply
-    // call setType. We can afford this simplicity because our algorithm
-    // currently supports only one function per module.
     auto func = cast<func::FuncOp>(op->getParentOp());
     func.setType(
         rewriter.getFunctionType(func.getArgumentTypes(), updatedResultTypes));
@@ -1100,22 +1477,186 @@
   }
 };
 
+LogicalResult applyRewritePatterns(func::FuncOp func, MLIRContext* context,
+                                   RefineShapeState* state) {
+  // TODO(#1048): Find out why .maxIterations = 1 no longer works.
+  // There have been recent refactors to applyPatternsAndFoldGreedily
+  // upstream, and that might be the reason.
+  GreedyRewriteConfig config;
+  config.useTopDownTraversal = true;
+  config.enableRegionSimplification = true;
+  config.maxIterations = 2;
+  config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+  config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+  RewritePatternSet patterns(context);
+  patterns.add<EvalAddOpPattern>(context);
+  patterns.add<EvalAndOpPattern>(context);
+  patterns.add<EvalBroadcastInDimOpPattern>(context);
+  patterns.add<EvalClampOpPattern>(context);
+  patterns.add<EvalCompareOpPattern>(context);
+  patterns.add<EvalConcatenateOpPattern>(context);
+  patterns.add<EvalConvertOpPattern>(context);
+  patterns.add<EvalDivOpPattern>(context);
+  patterns.add<EvalGetDimensionSizeOpPattern>(context);
+  patterns.add<EvalMaxOpPattern>(context);
+  patterns.add<EvalMinOpPattern>(context);
+  patterns.add<EvalMulOpPattern>(context);
+  patterns.add<EvalRemOpPattern>(context);
+  patterns.add<EvalReshapeOpPattern>(context);
+  patterns.add<EvalSelectOpPattern>(context);
+  patterns.add<EvalSignOpPattern>(context);
+  patterns.add<EvalSliceOpPattern>(context);
+  patterns.add<EvalSubtractOpPattern>(context);
+  patterns.add<RefineAllGatherOpPattern>(context);
+  patterns.add<RefineBitcastConvertOpPattern>(context);
+  patterns.add<RefineCallOpPattern>(context, state);
+  patterns.add<RefineConvertOpPattern>(context);
+  patterns.add<RefineConvolutionOpPattern>(context);
+  patterns.add<RefineCustomCallOpPattern>(context);
+  patterns.add<RefineDotGeneralOpPattern>(context);
+  patterns.add<RefineDynamicBroadcastInDimOpPattern>(context);
+  patterns.add<RefineDynamicConvOpPattern>(context);
+  patterns.add<RefineDynamicIotaOpPattern>(context);
+  patterns.add<RefineDynamicPadOpPattern>(context);
+  patterns.add<RefineDynamicReduceWindowOpPattern>(context);
+  patterns.add<RefineDynamicReshapeOpPattern>(context);
+  patterns.add<RefineDynamicRngBitGeneratorOpPattern>(context);
+  patterns.add<RefineDynamicTopKOpPattern>(context);
+  patterns.add<RefineInferTypeOpInterfacePattern>(context);
+  patterns.add<RefineRealDynamicSliceOpPattern>(context);
+  patterns.add<RefineReduceScatterOpPattern>(context);
+  patterns.add<RefineRngOpPattern>(context);
+  patterns.add<RefineUniformQuantizeOpPattern>(context);
+  patterns.add<RefineWhileOpPattern>(context);
+  patterns.add<UpdateFunctionTypePattern>(context);
+  patterns.add<UpdateRegionTypePattern>(context);
+  if (failed(applyPatternsAndFoldGreedily(func, std::move(patterns), config))) {
+    func.emitOpError() << "applyPatternsAndFoldGreedily failed";
+    return failure();
+  }
+  return success();
+}
+
+LogicalResult refineFunction(func::FuncOp func, MLIRContext* context,
+                             RefineShapeState* state,
+                             size_t nrPrefixTokenArguments,
+                             SmallVector<APSInt> dimensionArguments,
+                             SmallVector<Type> nonDimensionArgumentTypes) {
+  // The nonDimensionArgumentTypes include the prefix token arguments.
+  LLVM_DEBUG({
+    llvm::dbgs() << "refineFunction " << func.getName() << ": initial type "
+                 << debugString(func.getFunctionType()) << "\n";
+    llvm::dbgs() << "   has " << nrPrefixTokenArguments << " prefix tokens\n";
+    for (size_t i = 0; i < dimensionArguments.size(); ++i) {
+      llvm::dbgs() << "   with dimension arg[" << i
+                   << "] = " << dimensionArguments[i] << "\n";
+    }
+  });
+  // Check that the argument types have static shapes.
+  for (size_t i = 0; i < nonDimensionArgumentTypes.size(); ++i) {
+    if (i < nrPrefixTokenArguments) continue;
+    auto argType = nonDimensionArgumentTypes[i];
+    if (isa<TokenType>(argType)) continue;
+    auto argRankedTensorType = dyn_cast<RankedTensorType>(argType);
+    if (!argRankedTensorType || !argRankedTensorType.hasStaticShape()) {
+      func.emitOpError() << func.getName()
+                         << " must be refined with static shape arguments. "
+                         << "Found argument of type " << debugString(argType);
+      return failure();
+    }
+  }
+  auto alreadyRefined = state->validateFunctionRefinement(
+      func, dimensionArguments, nonDimensionArgumentTypes);
+  if (failed(alreadyRefined)) {
+    return failure();
+  }
+  if (*alreadyRefined) {
+    LLVM_DEBUG({
+      llvm::dbgs() << "refineFunction " << func.getName()
+                   << ": skipping, already refined\n";
+    });
+    return success();
+  }
+  state->startFunctionRefinement(func, dimensionArguments,
+                                 nonDimensionArgumentTypes);
+  // Only one block per function is supported at the moment.
+  // At the StableHLO level, functions are expected to only have one block,
+  // so supporting more is out of scope for this pass.
+  if (!func.getRegion().hasOneBlock()) {
+    func.emitOpError() << "must have exactly one block";
+    return failure();
+  }
+
+  // Replace all dimension arguments with constants and remove those arguments.
+  // Wrap non-dimension arguments with bitcast_convert.
+  OpBuilder op_builder(func.getRegion());
+  op_builder.setInsertionPointToStart(&func.getRegion().front());
+  size_t firstNonDimensionArg =
+      nrPrefixTokenArguments + dimensionArguments.size();
+  for (size_t i = 0; i < func.getNumArguments(); ++i) {
+    BlockArgument arg = func.getArgument(i);
+    Type argType = arg.getType();
+    if (i < nrPrefixTokenArguments) {
+      continue;
+    }
+    if (i < firstNonDimensionArg) {
+      ShapedType argShapedType = dyn_cast<ShapedType>(argType);
+      if (!argShapedType) {
+        func.emitOpError() << "dimension arguments must have shaped types";
+        return failure();
+      }
+      // We will drop the dimension arguments, replace them with constants.
+      auto replacement_op = op_builder.create<stablehlo::ConstantOp>(
+          arg.getLoc(), argType,
+          getTensorAttr(argShapedType,
+                        dimensionArguments[i - nrPrefixTokenArguments]));
+      arg.replaceAllUsesWith(replacement_op);
+    } else {
+      int nonDimensionArgumentIndex =
+          nrPrefixTokenArguments + i - firstNonDimensionArg;
+      Type refinedType = nonDimensionArgumentTypes[nonDimensionArgumentIndex];
+      if (refinedType != argType) {
+        // We add BitcastConvertOp as the only uses of the non-dimension
+        // arguments to ensure the module stays valid after we set the argument
+        // type.
+        auto replacement_op = op_builder.create<stablehlo::BitcastConvertOp>(
+            arg.getLoc(), argType, arg);
+        arg.replaceAllUsesExcept(replacement_op->getResult(0), replacement_op);
+        arg.setType(refinedType);
+      }
+    }
+  }
+  BitVector argIndices(func.getNumArguments());
+  argIndices.set(nrPrefixTokenArguments, firstNonDimensionArg);
+  func.eraseArguments(argIndices);
+  func.setType(op_builder.getFunctionType(nonDimensionArgumentTypes,
+                                          func.getResultTypes()));
+  LLVM_DEBUG({
+    llvm::dbgs() << "refineFunction " << func.getName() << ": set type to "
+                 << func.getFunctionType() << "\n";
+  });
+  if (failed(applyRewritePatterns(func, context, state))) return failure();
+  LLVM_DEBUG({
+    llvm::dbgs() << "refineFunction " << func.getName() << ": end with type "
+                 << debugString(func.getFunctionType()) << "\n";
+  });
+  if (failed(state->finishFunctionRefinement(func))) return failure();
+  return success();
+}
+
 struct StablehloRefineShapesPass
     : public impl::StablehloRefineShapesPassBase<StablehloRefineShapesPass> {
   using StablehloRefineShapesPassBase::StablehloRefineShapesPassBase;
 
   void runOnOperation() override {
-    // Only one function per module is supported at the moment to avoid the need
-    // to think about iterative type inference algorithms.
-    // Current use cases are served well by inlining multiple functions into
-    // a single function, so we leave native support for multiple functions to
-    // future work.
     // To enable modules that contain CustomCallOp::called_computations,
     // we allow multiple functions, in which case we only refine the main
     // function called "main", assuming that the called computations will have
     // static shapes. Lifting this assumption and expanding refinement to
     // multiple functions is left for future work.
     ModuleOp module = getOperation();
+    RefineShapeState state;
     auto funcs = llvm::to_vector(module.getOps<func::FuncOp>());
     if (funcs.empty()) return;
     func::FuncOp func;
@@ -1130,70 +1671,14 @@
           << " function to clearly identify which function will be refined";
       return signalPassFailure();
     }
-
-    // Similarly, only one block per function is supported at the moment.
-    // At the StableHLO level, functions are expected to only have one block,
-    // so supporting more is out of scope for this pass.
-    if (!func.getRegion().hasOneBlock()) {
-      func.emitOpError() << "must have exactly one block";
+    SmallVector<APSInt> emptyDimensionArguments;
+    SmallVector<Type> nonDimensionArgumentTypes;
+    for (auto arg : func.getArguments())
+      nonDimensionArgumentTypes.push_back(arg.getType());
+    if (failed(refineFunction(func, &getContext(), &state, 0,
+                              emptyDimensionArguments,
+                              nonDimensionArgumentTypes)))
       return signalPassFailure();
-    }
-
-    // The algorithm behind this pass consists of a single traversal of the
-    // function. This is sufficient because we only support one function per
-    // program at the moment.
-    // TODO(#1048): Find out why .maxIterations = 1 no longer works.
-    // There have been recent refactors to applyPatternsAndFoldGreedily
-    // upstream, and that might be the reason.
-    GreedyRewriteConfig config;
-    config.useTopDownTraversal = true;
-    config.enableRegionSimplification = true;
-    config.maxIterations = 2;
-    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
-    config.strictMode = GreedyRewriteStrictness::AnyOp;
-
-    RewritePatternSet patterns(&getContext());
-    patterns.add<EvalAddOpPattern>(&getContext());
-    patterns.add<EvalAndOpPattern>(&getContext());
-    patterns.add<EvalBroadcastInDimOpPattern>(&getContext());
-    patterns.add<EvalClampOpPattern>(&getContext());
-    patterns.add<EvalCompareOpPattern>(&getContext());
-    patterns.add<EvalConcatenateOpPattern>(&getContext());
-    patterns.add<EvalConvertOpPattern>(&getContext());
-    patterns.add<EvalDivOpPattern>(&getContext());
-    patterns.add<EvalGetDimensionSizeOpPattern>(&getContext());
-    patterns.add<EvalMaxOpPattern>(&getContext());
-    patterns.add<EvalMinOpPattern>(&getContext());
-    patterns.add<EvalMulOpPattern>(&getContext());
-    patterns.add<EvalRemOpPattern>(&getContext());
-    patterns.add<EvalReshapeOpPattern>(&getContext());
-    patterns.add<EvalSelectOpPattern>(&getContext());
-    patterns.add<EvalSignOpPattern>(&getContext());
-    patterns.add<EvalSliceOpPattern>(&getContext());
-    patterns.add<EvalSubtractOpPattern>(&getContext());
-    patterns.add<RefineAllGatherOpPattern>(&getContext());
-    patterns.add<RefineBitcastConvertOpPattern>(&getContext());
-    patterns.add<RefineConvertOpPattern>(&getContext());
-    patterns.add<RefineConvolutionOpPattern>(&getContext());
-    patterns.add<RefineCustomCallOpPattern>(&getContext());
-    patterns.add<RefineDotGeneralOpPattern>(&getContext());
-    patterns.add<RefineDynamicBroadcastInDimOpPattern>(&getContext());
-    patterns.add<RefineDynamicConvOpPattern>(&getContext());
-    patterns.add<RefineDynamicIotaOpPattern>(&getContext());
-    patterns.add<RefineDynamicPadOpPattern>(&getContext());
-    patterns.add<RefineDynamicReshapeOpPattern>(&getContext());
-    patterns.add<RefineInferTypeOpInterfacePattern>(&getContext());
-    patterns.add<RefineRealDynamicSliceOpPattern>(&getContext());
-    patterns.add<RefineReduceScatterOpPattern>(&getContext());
-    patterns.add<RefineRngOpPattern>(&getContext());
-    patterns.add<RefineUniformQuantizeOpPattern>(&getContext());
-    patterns.add<RefineWhileOpPattern>(&getContext());
-    patterns.add<UpdateFunctionTypePattern>(&getContext());
-    patterns.add<UpdateRegionTypePattern>(&getContext());
-    if (failed(
-            applyPatternsAndFoldGreedily(func, std::move(patterns), config))) {
-      return signalPassFailure();
-    }
   }
 };
 

