diff --git a/lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp b/lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp
--- a/lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp
@@ -787,7 +787,7 @@ static void rewriteSlice(SetVector<Value
       }
       continue;
     }
-    builder.setInsertionPoint(op);
+    builder.setInsertionPointAfter(op);
     if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {
       auto yieldOperands = llvm::to_vector(yieldOp.getOperands());
       for (Value operand : yieldOp.getOperands()) {
diff --git a/test/TritonGPU/combine.mlir b/test/TritonGPU/combine.mlir
--- a/test/TritonGPU/combine.mlir
+++ b/test/TritonGPU/combine.mlir
@@ -53,7 +53,7 @@ tt.func @remat(%arg0: i32) -> tensor<102
   // CHECK: %3 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, [[$target_layout]]>
   // CHECK: %4 = arith.muli %0, %2 : tensor<1024xi32, [[$target_layout]]>
   // CHECK: %5 = arith.muli %1, %3 : tensor<1024xi32, [[$target_layout]]>
-  // CHECK: %6 = arith.addi %4, %5 : tensor<1024xi32, [[$target_layout]]>
+  // CHECK: %6 = arith.addi %5, %4 : tensor<1024xi32, [[$target_layout]]>
   // CHECK: tt.return %6 : tensor<1024xi32, [[$target_layout]]>
 }
 
