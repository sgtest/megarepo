diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp
@@ -22,16 +22,16 @@ Value convertLayout(
   // Calculate tile size as number of mask elements (4xi4).
   NvidiaMmaEncodingAttr mmaLayout =
       sparseEncoding.getParent().cast<NvidiaMmaEncodingAttr>();
+  SmallVector<unsigned> warpsPerCTA = mmaLayout.getWarpsPerCTA();
   SmallVector<unsigned> shapePerCTATile = {
-      kTileSize * mmaLayout.getWarpsPerCTA()[0],
-      kTileSize / kMetadataElementsPerPackedValue};
+      kTileSize * warpsPerCTA[0], kTileSize / kMetadataElementsPerPackedValue};
   Value strideM = smemObj.strides[0];
   Value strideK = smemObj.strides[1];
 
   // Calculate offset in the tile for the current thread.
   Value threadsPerWarp = i32_val(kThreadsPerWarp);
   Value warpId = udiv(thread, threadsPerWarp);
-  Value warpGroupId = urem(warpId, i32_val(shapePerCTATile[0] / kTileSize));
+  Value warpGroupId = udiv(warpId, i32_val(warpsPerCTA[1]));
   Value laneId = urem(thread, threadsPerWarp);
   Value laneGroupId = udiv(laneId, i32_val(kThreadsInGroup));
   Value columnId = urem(laneId, i32_val(shapePerCTATile[1]));
diff --git a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
--- a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
@@ -139,6 +139,7 @@ class BlockedToMMA : public mlir::Rewrit
                 mlir::TypeID::get<arith::ArithDialect>());
   }
 
+public:
   // Finds the first different bitwidth in the chain of shape-preserving
   // unary ops that x depends on.
   // There are two primary scenarios:
@@ -172,7 +173,6 @@ class BlockedToMMA : public mlir::Rewrit
     return origBitWidth;
   }
 
-public:
   BlockedToMMA(mlir::MLIRContext *context, int computeCapability)
       : mlir::RewritePattern(tt::DotOp::getOperationName(), 2, context),
         computeCapability(computeCapability) {}
@@ -388,18 +388,22 @@ class SparseBlockedToMMA : public mlir::
                                                         newRetType, oldAcc);
 
     if (versionMajor == 2) {
+      int minBitwidth = std::min(BlockedToMMA::computeOrigBitWidth(a),
+                                 BlockedToMMA::computeOrigBitWidth(b));
+      int kWidth = 32 / minBitwidth;
+
       // convert A operand
       auto oldAType = a.getType().cast<RankedTensorType>();
-      auto newAEncoding = ttg::DotOperandEncodingAttr::get(
-          ctx, 0, mmaEnc, oldAType.getElementType());
+      auto newAEncoding =
+          ttg::DotOperandEncodingAttr::get(ctx, 0, mmaEnc, kWidth);
       auto newAType = RankedTensorType::get(
           oldAType.getShape(), oldAType.getElementType(), newAEncoding);
       a = rewriter.create<ttg::ConvertLayoutOp>(a.getLoc(), newAType, a);
 
       // convert B operand
       auto oldBType = b.getType().cast<RankedTensorType>();
-      auto newBEncoding = ttg::DotOperandEncodingAttr::get(
-          ctx, 1, mmaEnc, oldBType.getElementType());
+      auto newBEncoding =
+          ttg::DotOperandEncodingAttr::get(ctx, 1, mmaEnc, kWidth);
       auto newBType = RankedTensorType::get(
           oldBType.getShape(), oldBType.getElementType(), newBEncoding);
       b = rewriter.create<ttg::ConvertLayoutOp>(b.getLoc(), newBType, b);
