diff --git a/include/triton/Dialect/NVGPU/IR/NVGPUOps.td b/include/triton/Dialect/NVGPU/IR/NVGPUOps.td
--- a/include/triton/Dialect/NVGPU/IR/NVGPUOps.td
+++ b/include/triton/Dialect/NVGPU/IR/NVGPUOps.td
@@ -87,6 +87,15 @@ def NVGPU_WGMMAOp : NVGPU_Op<"wgmma", []
   let assemblyFormat = "$opA `,` $opB (`,` $opC^)? attr-dict `:` functional-type(operands, $res)";
 }
 
+def NVGPU_SparseWGMMAOp : NVGPU_Op<"wgmma_sp", []> {
+  let arguments = (ins WGMMA_OperandType:$opA, I32:$metaA, WGMMA_OperandType:$opB, LLVM_AnyStruct:$opC,
+                   I32Attr:$m, I32Attr:$n, I32Attr:$k,
+                   WGMMA_EltTypeAttr:$eltTypeC, WGMMA_EltTypeAttr:$eltTypeA, WGMMA_EltTypeAttr:$eltTypeB,
+                   WGMMA_LayoutAttr:$layoutA, WGMMA_LayoutAttr:$layoutB);
+  let results = (outs LLVM_AnyStruct:$res);
+  let assemblyFormat = "$opA `meta` $metaA `,` $opB `,` $opC attr-dict `:` functional-type(operands, $res)";
+}
+
 def NVGPU_LoadDSmemOp : NVGPU_Op<"load_dsmem", [MemoryEffects<[MemRead]>]> {
   let arguments = (ins LLVM_AnyPointer:$addr, I32:$ctaId, I32Attr:$bitwidth, I32Attr:$vec);
   let builders = [
diff --git a/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp b/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp
--- a/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp
+++ b/third_party/nvidia/lib/NVGPUToLLVM/NVGPUToLLVMPass.cpp
@@ -688,6 +688,84 @@ public:
   }
 };
 
+class SparseWGMMAOpPattern
+    : public NVGPUOpPatternBase<ttn::SparseWGMMAOp, SparseWGMMAOpPattern> {
+public:
+  using Base = NVGPUOpPatternBase<ttn::SparseWGMMAOp, SparseWGMMAOpPattern>;
+  using Base::Base;
+
+  std::vector<std::string> getOutputConstraints(ttn::SparseWGMMAOp op) const {
+    auto outputStructType = op.getType().cast<LLVM::LLVMStructType>();
+    uint32_t numOutputRegs = outputStructType.getBody().size();
+    std::string output =
+        outputStructType.getBody().front().isF32() ? "=f" : "=r";
+    return std::vector<std::string>(numOutputRegs, output);
+  }
+
+  OperandsAndConstraints getOperandsAndConstraints(
+      ttn::SparseWGMMAOp op) const {
+    return {{op.getOpC(), "0"}, {op.getOpA(), "l"}, {op.getOpB(), "l"},
+            {op.getMetaA(), "r"}};
+  }
+
+  std::string getPtxAsm(ttn::SparseWGMMAOp op) const {
+    using namespace ttn;
+    auto opA = op.getOpA();
+    auto opB = op.getOpB();
+    auto m = op.getM();
+    auto n = op.getN();
+    auto k = op.getK();
+    auto eltTypeC = op.getEltTypeC();
+    auto eltTypeA = op.getEltTypeA();
+    auto eltTypeB = op.getEltTypeB();
+    auto layoutA = op.getLayoutA();
+    auto layoutB = op.getLayoutB();
+
+    // Only f16/bf16 variant is supported.
+    bool supported =
+        eltTypeC == WGMMAEltType::f32 &&
+        ((eltTypeA == WGMMAEltType::f16 && eltTypeB == WGMMAEltType::f16) ||
+         (eltTypeA == WGMMAEltType::bf16 && eltTypeB == WGMMAEltType::bf16)) &&
+        (m == 64 && 8 <= n && n <= 256 && n % 8 == 0 && k == 32);
+    assert(supported && "Sparse WGMMA type or shape is not supported");
+
+    // Operands
+    uint32_t asmOpIdx = 0;
+    std::string args = "";
+
+    // Output and operand C
+    uint32_t numCRegs =
+        op.getType().cast<LLVM::LLVMStructType>().getBody().size();
+    args += "{";
+    for (uint32_t i = 0; i < numCRegs; ++i) {
+      args += "$" + std::to_string(asmOpIdx++) + (i == numCRegs - 1 ? "" : ",");
+    }
+    args += "}, ";
+    asmOpIdx += numCRegs;
+
+    // Operands A and B (must be `desc`)
+    args += "$" + std::to_string(asmOpIdx++) + ", ";
+    args += "$" + std::to_string(asmOpIdx++) + ", ";
+
+    // Metadata for A
+    args += "$" + std::to_string(asmOpIdx++) + ", 0, ";
+
+    // `scale-d`, `imm-scale-a`, and `imm-scale-b` are 1 by default
+    args += "1, 1, 1";
+
+    // `trans-a` and `trans-b`
+    args += ", " + std::to_string(layoutA == WGMMALayout::col);
+    args += ", " + std::to_string(layoutB == WGMMALayout::row);
+
+    auto ptxAsm = "wgmma.mma_async.sp.sync.aligned"
+                  ".m" + std::to_string(m) + "n" + std::to_string(n) + "k" +
+                  std::to_string(k) + "." + stringifyEnum(eltTypeC).str() +
+                  "." + stringifyEnum(eltTypeA).str() + "." +
+                  stringifyEnum(eltTypeB).str() + " " + args + ";";
+    return ptxAsm;
+  }
+};
+
 class ConvertNVGPUToLLVM : public ConvertNVGPUToLLVMBase<ConvertNVGPUToLLVM> {
 
 public:
@@ -711,7 +789,8 @@ public:
 
     patterns.add<FenceAsyncSharedOpPattern, StoreMatrixOpPattern,
                  ClusterArriveOpPattern, LoadDSmemOpPattern, WGMMAOpPattern,
-                 WGMMAWaitGroupOpPattern, StoreDSmemOpPattern>(context);
+                 WGMMAWaitGroupOpPattern, StoreDSmemOpPattern,
+                 SparseWGMMAOpPattern>(context);
 
     if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())
       signalPassFailure();
