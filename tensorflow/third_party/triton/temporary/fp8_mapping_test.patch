diff --git a/test/Conversion/tritongpu_to_llvm_hopper.mlir b/test/Conversion/tritongpu_to_llvm_hopper.mlir
--- a/test/Conversion/tritongpu_to_llvm_hopper.mlir
+++ b/test/Conversion/tritongpu_to_llvm_hopper.mlir
@@ -228,3 +228,26 @@ module attributes {"triton_gpu.target" =
     tt.return
   }
 }
+
+// -----
+
+// Check that Triton uses incorrect type to map to NVIDIA .e4m3 type (b/345700241). When this test fails, change the mapping in ir_emitter_triton.cc
+#mma = #triton_gpu.nvidia_mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 1], instrShape = [16, 8]}>
+// CHECK-LABEL: e4m3_mapping
+module attributes {"triton_gpu.compute-capability" = 90 : i32,
+                   "triton_gpu.num-ctas" = 1 : i32,
+                   "triton_gpu.num-warps" = 1 : i32,
+                   "triton_gpu.threads-per-warp" = 32 : i32} {
+  tt.func @e4m3_mapping(
+      %arg0: tensor<16x256xf8E4M3FNUZ, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>,
+      %arg1: tensor<256x16xf8E4M3FNUZ, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>
+    ) {
+    %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma>
+    // CHECK: mma.{{.*}}.e4m3.e4m3.f32
+    %res = tt.dot %arg0, %arg1, %cst {allowTF32 = true, maxNumImpreciseAcc = 0 : i32}
+        : tensor<16x256xf8E4M3FNUZ, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> *
+          tensor<256x16xf8E4M3FNUZ, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>
+          -> tensor<16x16xf32, #mma>
+    tt.return
+  }
+}
