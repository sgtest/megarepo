Cherry-picking https://github.com/openai/triton/commit/62706e8c518c8c56e56460a43732d8e375217860
until the next integration lands it. Can be removed as it is already merged.

diff --git a/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp b/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp
--- a/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp
+++ b/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp
@@ -805,6 +805,7 @@ void mlir::triton::populateElementwiseOp
   POPULATE_UNARY_OP(arith::FPToUIOp, LLVM::FPToUIOp)
   POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)
   POPULATE_UNARY_OP(math::FloorOp, math::FloorOp)
+  POPULATE_UNARY_OP(math::CeilOp, math::CeilOp)
   POPULATE_UNARY_OP(math::LogOp, math::LogOp)
   POPULATE_UNARY_OP(math::Log2Op, math::Log2Op)
   POPULATE_UNARY_OP(math::CosOp, math::CosOp)
diff --git a/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp b/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp
--- a/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp
+++ b/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp
@@ -125,12 +125,13 @@ void populateMathPatternsAndLegality(Tri
   MLIRContext *context = patterns.getContext();
   // Rewrite rule
   patterns.add<GenericOpPattern<math::ExpOp>, GenericOpPattern<math::Exp2Op>,
-               GenericOpPattern<math::FloorOp>, GenericOpPattern<math::CosOp>,
-               GenericOpPattern<math::SinOp>, GenericOpPattern<math::LogOp>,
-               GenericOpPattern<math::Log2Op>, GenericOpPattern<math::ErfOp>,
-               GenericOpPattern<math::AbsFOp>, GenericOpPattern<math::AbsIOp>,
-               GenericOpPattern<math::SqrtOp>, GenericOpPattern<math::RsqrtOp>,
-               GenericOpPattern<math::FmaOp>>(typeConverter, context);
+               GenericOpPattern<math::FloorOp>, GenericOpPattern<math::CeilOp>,
+               GenericOpPattern<math::CosOp>, GenericOpPattern<math::SinOp>,
+               GenericOpPattern<math::LogOp>, GenericOpPattern<math::Log2Op>,
+               GenericOpPattern<math::ErfOp>, GenericOpPattern<math::AbsFOp>,
+               GenericOpPattern<math::AbsIOp>, GenericOpPattern<math::SqrtOp>,
+               GenericOpPattern<math::RsqrtOp>, GenericOpPattern<math::FmaOp>>(
+      typeConverter, context);
 }
 
 //
diff --git a/lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp b/lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp
--- a/lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp
+++ b/lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp
@@ -651,10 +651,9 @@ bool CTAPlanner::isElementwiseOp(Operati
                 math::CeilOp, math::CopySignOp, math::CosOp, math::SinOp,
                 math::CountLeadingZerosOp, math::CountTrailingZerosOp,
                 math::CtPopOp, math::ErfOp, math::ExpOp, math::Exp2Op,
-                math::FloorOp, math::ExpM1Op, math::FloorOp, math::FmaOp,
-                math::LogOp, math::Log10Op, math::Log1pOp, math::Log2Op,
-                math::PowFOp, math::RsqrtOp, math::SqrtOp, math::RsqrtOp,
-                math::TanhOp>(op))
+                math::FloorOp, math::ExpM1Op, math::FmaOp, math::LogOp,
+                math::Log10Op, math::Log1pOp, math::Log2Op, math::PowFOp,
+                math::RsqrtOp, math::SqrtOp, math::RsqrtOp, math::TanhOp>(op))
     return true;
   if (llvm::isa<triton::IntToPtrOp, triton::PtrToIntOp, triton::BitcastOp,
                 triton::FpToFpOp, triton::AddPtrOp, triton::PreciseSqrtOp,
diff --git a/python/src/ir.cc b/python/src/ir.cc
--- a/python/src/ir.cc
+++ b/python/src/ir.cc
@@ -1379,6 +1379,10 @@ void init_triton_ir(py::module &&m) {
            [](TritonOpBuilder &self, Value &val) -> Value {
              return self.create<math::FloorOp>(val);
            })
+      .def("create_ceil",
+           [](TritonOpBuilder &self, Value &val) -> Value {
+             return self.create<math::CeilOp>(val);
+           })
       .def("create_exp",
            [](TritonOpBuilder &self, Value &val) -> Value {
              return self.create<math::ExpOp>(val);
diff --git a/python/test/unit/language/test_core.py b/python/test/unit/language/test_core.py
--- a/python/test/unit/language/test_core.py
+++ b/python/test/unit/language/test_core.py
@@ -915,10 +915,11 @@ def test_unary_op(dtype_x, expr, num_cta
 
 
 @pytest.mark.interpreter
-@pytest.mark.parametrize("dtype_x, expr, x", [(dtype_x, expr, x)
-                                              for dtype_x in ["float32", "float64"]
-                                              for expr in ['exp', 'log', 'cos', 'sin', 'exp2', 'log2', 'sqrt', 'floor']
-                                              for x in ['x', '3.0']])
+@pytest.mark.parametrize("dtype_x, expr, x",
+                         [(dtype_x, expr, x)
+                          for dtype_x in ["float32", "float64"]
+                          for expr in ['exp', 'log', 'cos', 'sin', 'exp2', 'log2', 'sqrt', 'floor', 'ceil']
+                          for x in ['x', '3.0']])
 def test_math_op(dtype_x, expr, x, device):
     _test_unary(dtype_x, f'tl.{expr}({x})', f'np.{expr}({x}) ', device=device)
 
diff --git a/python/triton/language/__init__.py b/python/triton/language/__init__.py
--- a/python/triton/language/__init__.py
+++ b/python/triton/language/__init__.py
@@ -102,7 +102,8 @@ from .core import (
     void,
     where,
 )
-from .math import (umulhi, exp, exp2, fma, log, log2, cos, rsqrt, sin, sqrt, sqrt_rn, abs, fdiv, div_rn, erf, floor)
+from .math import (umulhi, exp, exp2, fma, log, log2, cos, rsqrt, sin, sqrt, sqrt_rn, abs, fdiv, div_rn, erf, floor,
+                   ceil)
 from .random import (
     pair_uniform_to_normal,
     philox,
@@ -142,6 +143,7 @@ from .random import (
     "builtin",
     "cat",
     "cdiv",
+    "ceil",
     "clamp",
     "const",
     "const_pointer_type",
diff --git a/python/triton/language/math.py b/python/triton/language/math.py
--- a/python/triton/language/math.py
+++ b/python/triton/language/math.py
@@ -230,6 +230,15 @@ def floor(x, _builder=None):
 
 
 @core.builtin
+@_check_dtype(dtypes=["fp32", "fp64"])
+@_add_math_1arg_docstr("ceil")
+@core._tensor_member_fn
+def ceil(x, _builder=None):
+    x = core._to_tensor(x, _builder)
+    return core.tensor(_builder.create_ceil(x.handle), x.type)
+
+
+@core.builtin
 @_add_math_3arg_docstr("fused multiply-add")
 def fma(x, y, z, _builder=None):
     x = core._to_tensor(x, _builder)
diff --git a/python/triton/runtime/interpreter.py b/python/triton/runtime/interpreter.py
--- a/python/triton/runtime/interpreter.py
+++ b/python/triton/runtime/interpreter.py
@@ -391,6 +391,7 @@ class InterpreterBuilder:
     create_fabs = lambda self, arg: self.unary_op(arg, np.abs)
     create_iabs = lambda self, arg: self.unary_op(arg, np.abs)
     create_floor = lambda self, arg: self.unary_op(arg, np.floor)
+    create_ceil = lambda self, arg: self.unary_op(arg, np.ceil)
     create_log = lambda self, arg: self.unary_op(arg, np.log)
     create_log2 = lambda self, arg: self.unary_op(arg, np.log2)
     create_precise_sqrt = lambda self, arg: self.unary_op(arg, np.sqrt)
