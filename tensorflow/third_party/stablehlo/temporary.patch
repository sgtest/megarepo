diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel
--- stablehlo/BUILD.bazel
+++ stablehlo/BUILD.bazel
@@ -279,6 +279,24 @@
 )
 
 cc_library(
+    name = "experimental_ops",
+    srcs = [
+        "stablehlo/dialect/ExperimentalOps.cpp",
+    ],
+    hdrs = [
+        "stablehlo/dialect/ExperimentalOps.h",
+    ],
+    strip_include_prefix = ".",
+    deps = [
+        ":stablehlo_ops",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_library(
     name = "reference_axes",
     srcs = [
         "stablehlo/reference/Axes.cpp",
@@ -677,6 +695,7 @@
     deps = [
         ":base",
         ":chlo_ops",
+        ":experimental_ops",
         ":stablehlo_ops",
         ":stablehlo_ops_inc_gen",
         ":stablehlo_pass_inc_gen",
diff --ruN a/stablehlo/CMakeLists.txt b/stablehlo/CMakeLists.txt
--- stablehlo/CMakeLists.txt
+++ stablehlo/CMakeLists.txt
@@ -13,135 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-cmake_minimum_required(VERSION 3.15.0)
 
-if(POLICY CMP0068)
-  cmake_policy(SET CMP0068 NEW)
-  set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)
-endif()
-
-if(POLICY CMP0075)
-  cmake_policy(SET CMP0075 NEW)
-endif()
-
-if(POLICY CMP0077)
-  cmake_policy(SET CMP0077 NEW)
-endif()
-
-# CMP0116: Ninja generators transform `DEPFILE`s from `add_custom_command()`
-# New in CMake 3.20. https://cmake.org/cmake/help/latest/policy/CMP0116.html
-if(POLICY CMP0116)
-  cmake_policy(SET CMP0116 OLD)
-endif()
+# This build of StableHLO is meant to be embedded in MLIR-HLO.
+# As a result, its root CMakeLists.txt is different from the original
+# CMakeLists.txt from https://github.com/openxla/stablehlo.
+# All other files of this build of StableHLO except for this one are the same
+# as the original files.
+# To get access to a standalone build of StableHLO, check out the
+# openxla/stablehlo repository.
 
 #-------------------------------------------------------------------------------
 # Options and settings
 #-------------------------------------------------------------------------------
-option(STABLEHLO_BUILD_EMBEDDED "Build StableHLO as part of another project" OFF)
-option(STABLEHLO_ENABLE_BINDINGS_PYTHON "Enables StableHLO Python bindings" OFF)
-option(STABLEHLO_ENABLE_STRICT_BUILD "Build StableHLO with strict warnings and warnings as errors" OFF)
 
-#-------------------------------------------------------------------------------
-# Project setup and globals
-#-------------------------------------------------------------------------------
-set(STABLEHLO_EXTERNAL_PROJECT_BUILD OFF)
-
-if(NOT (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR) AND NOT MLIR_BINARY_DIR)
-  # Building as part of LLVM via the external project mechanism.
-  set(STABLEHLO_EXTERNAL_PROJECT_BUILD ON)
-else()
-  # Building standalone.
-  project(stablehlo LANGUAGES CXX C)
-  set(CMAKE_C_STANDARD 11)
-  set(CMAKE_CXX_STANDARD 17)
-endif()
-
-# Build with ccache if the package is present
-set(LLVM_CCACHE_BUILD OFF CACHE BOOL "Set to ON for a ccache enabled build")
-if(LLVM_CCACHE_BUILD)
-  find_program(CCACHE_PROGRAM ccache)
-  if(CCACHE_PROGRAM)
-      set(LLVM_CCACHE_MAXSIZE "" CACHE STRING "Size of ccache")
-      set(LLVM_CCACHE_DIR "" CACHE STRING "Directory to keep ccached data")
-      set(LLVM_CCACHE_PARAMS "CCACHE_CPP2=yes CCACHE_HASHDIR=yes"
-          CACHE STRING "Parameters to pass through to ccache")
-
-      set(CCACHE_PROGRAM "${LLVM_CCACHE_PARAMS} ${CCACHE_PROGRAM}")
-      if (LLVM_CCACHE_MAXSIZE)
-        set(CCACHE_PROGRAM "CCACHE_MAXSIZE=${LLVM_CCACHE_MAXSIZE} ${CCACHE_PROGRAM}")
-      endif()
-      if (LLVM_CCACHE_DIR)
-        set(CCACHE_PROGRAM "CCACHE_DIR=${LLVM_CCACHE_DIR} ${CCACHE_PROGRAM}")
-      endif()
-      set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM})
-  else()
-    message(FATAL_ERROR "Unable to find the program ccache. Set LLVM_CCACHE_BUILD to OFF")
-  endif()
-endif()
-
-#-------------------------------------------------------------------------------
-# MLIR/LLVM Configuration
-#-------------------------------------------------------------------------------
-if (STABLEHLO_ENABLE_STRICT_BUILD)
-  set(LLVM_ENABLE_WARNINGS ON)
-  set(LLVM_ENABLE_WERROR ON)
-  set(LLVM_ENABLE_PEDANTIC ON)
-endif()
-
-# Find MLIR to install if we are building standalone. If building as part of
-# another project, let it handle the MLIR dependency. The dependent project
-# might use a bundled version of MLIR instead of installing, for instance.
-if(STABLEHLO_EXTERNAL_PROJECT_BUILD)
-  message(STATUS "Building StableHLO as an external LLVM project")
-  set(MLIR_MAIN_SRC_DIR ${LLVM_MAIN_SRC_DIR}/../mlir ) # --src-root
-  set(MLIR_INCLUDE_DIR ${MLIR_MAIN_SRC_DIR}/include ) # --includedir
-  set(MLIR_GENERATED_INCLUDE_DIR ${LLVM_BINARY_DIR}/tools/mlir/include)
-  include_directories(SYSTEM ${MLIR_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_GENERATED_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_TABLEGEN_OUTPUT_DIR})
-
-  set(BACKEND_PACKAGE_STRING "${PACKAGE_STRING}")
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_MAIN_SRC_DIR}/cmake/modules")
-elseif(NOT STABLEHLO_BUILD_EMBEDDED)
-  message(STATUS "Building StableHLO with an installed MLIR")
-  find_package(MLIR REQUIRED CONFIG)
-  message(STATUS "Using MLIRConfig.cmake in: ${MLIR_DIR}")
-  message(STATUS "Using LLVMConfig.cmake in: ${LLVM_DIR}")
-  set(LLVM_RUNTIME_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/bin)
-  set(LLVM_LIBRARY_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/lib)
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_CMAKE_DIR}")
-  list(APPEND CMAKE_MODULE_PATH "${LLVM_CMAKE_DIR}")
-else()
-  message(STATUS "Building StableHLO embedded in another project")
-endif()
-
-if(LLVM_ENABLE_ZLIB)
-  find_package(ZLIB)
-endif()
-
-include(TableGen)
-include(AddLLVM)
-include(AddMLIR)
-include(HandleLLVMOptions)
-include_directories(${LLVM_INCLUDE_DIRS})
-include_directories(${MLIR_INCLUDE_DIRS})
-include_directories(${CMAKE_CURRENT_SOURCE_DIR})
-include_directories(${CMAKE_CURRENT_BINARY_DIR})
-link_directories(${LLVM_BUILD_LIBRARY_DIR})
-add_definitions(${LLVM_DEFINITIONS})
-
-#-------------------------------------------------------------------------------
-# Python configuration
-#-------------------------------------------------------------------------------
-
-if(STABLEHLO_ENABLE_BINDINGS_PYTHON)
-  if(NOT STABLEHLO_EXTERNAL_PROJECT_BUILD)
-    message(WARNING "StableHLO Python bindings are not supported in standalone mode")
-  endif()
-
-  include(MLIRDetectPythonEnv)
-  mlir_configure_python_dev_packages()
-endif()
+set(STABLEHLO_ENABLE_BINDINGS_PYTHON ${MHLO_ENABLE_BINDINGS_PYTHON})
 
 #-------------------------------------------------------------------------------
 # Directory setup
diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp
--- stablehlo/stablehlo/dialect/Base.cpp
+++ stablehlo/stablehlo/dialect/Base.cpp
@@ -156,6 +156,7 @@
   DenseIntElementsAttr attr;
   if (!matchPattern(value, m_Constant(&attr))) return failure();
 
+  // Signless types are treated as signed, per StableHLO convention.
   // Unless the type is i1 (which models boolean type from the StableHLO spec),
   // in which case it's considered to be unsigned.
   auto elementType = attr.getType().getElementType();
@@ -599,5 +600,18 @@
   return UnrankedTensorType::get(components.getElementType());
 }
 
+DenseIntElementsAttr getPaddingAttr(MLIRContext* context,
+                                    ArrayRef<int64_t> values) {
+  return DenseIntElementsAttr::get(
+      RankedTensorType::get({static_cast<int64_t>(values.size()) / 2, 2},
+                            IntegerType::get(context, 64)),
+      values);
+}
+
+DenseIntElementsAttr getPaddingAttr(Builder* builder,
+                                    ArrayRef<int64_t> values) {
+  return getPaddingAttr(builder->getContext(), values);
+}
+
 }  // namespace hlo
 }  // namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h
--- stablehlo/stablehlo/dialect/Base.h
+++ stablehlo/stablehlo/dialect/Base.h
@@ -194,6 +194,10 @@
 
 ShapedType createShapedType(ShapedTypeComponents components);
 
+DenseIntElementsAttr getPaddingAttr(MLIRContext *context,
+                                    ArrayRef<int64_t> value);
+DenseIntElementsAttr getPaddingAttr(Builder *builder, ArrayRef<int64_t> value);
+
 // This interface is implemented by both StableHLO and MHLO dialects
 // and is used as the foundation for sharing verification, type inference and
 // prettyprinting logic between them.
@@ -249,6 +253,10 @@
 template <typename ConcreteType>
 class BroadcastingElementwise
     : public mlir::OpTrait::TraitBase<ConcreteType, BroadcastingElementwise> {};
+
+template <typename ConcreteType>
+class IsCommutative
+    : public mlir::OpTrait::TraitBase<ConcreteType, IsCommutative> {};
 
 template <typename ConcreteType>
 class PairwiseSameOperandAndResultType
diff --ruN a/stablehlo/stablehlo/dialect/Base.td b/stablehlo/stablehlo/dialect/Base.td
--- stablehlo/stablehlo/dialect/Base.td
+++ stablehlo/stablehlo/dialect/Base.td
@@ -188,6 +188,11 @@
 // An operation that is essentially element-wise but may implement broadcasting
 // semantics.
 def HLO_BroadcastingElementwise : HLO_NativeOpTrait<"BroadcastingElementwise">;
+
+// This class adds property that the operation is commutative.
+// Upstream IsCommutative has default folders, and StableHLO aims to have no
+// default folders or canonicalizations.
+def HLO_Commutative : HLO_NativeOpTrait<"IsCommutative">;
 
 // Op has pairwise operand and result type matching: the number of operands
 // must be equal to the number of results and the type of ith operand must
diff --ruN a/stablehlo/stablehlo/dialect/CMakeLists.txt b/stablehlo/stablehlo/dialect/CMakeLists.txt
--- stablehlo/stablehlo/dialect/CMakeLists.txt
+++ stablehlo/stablehlo/dialect/CMakeLists.txt
@@ -77,6 +77,20 @@
 target_include_directories(ChloOps INTERFACE
   $<BUILD_INTERFACE:${STABLEHLO_SOURCE_DIR}>
   $<BUILD_INTERFACE:${STABLEHLO_BINARY_DIR}>
+)
+
+add_mlir_dialect_library(ExperimentalOps
+  PARTIAL_SOURCES_INTENDED
+  ExperimentalOps.cpp
+
+  DEPENDS
+  StablehloOpsIncGen
+
+  LINK_LIBS PUBLIC
+  MLIRFuncDialect
+  MLIRIR
+  MLIRSupport
+  StablehloOps
 )
 
 add_mlir_dialect_library(StablehloRegister
diff --ruN a/stablehlo/stablehlo/dialect/ExperimentalOps.cpp b/stablehlo/stablehlo/dialect/ExperimentalOps.cpp
--- stablehlo/stablehlo/dialect/ExperimentalOps.cpp
+++ stablehlo/stablehlo/dialect/ExperimentalOps.cpp
@@ -0,0 +1,504 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/dialect/ExperimentalOps.h"
+
+#include <optional>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/Types.h"
+
+namespace mlir {
+namespace stablehlo {
+
+LogicalResult DynamicReduceWindowOpAdaptor::verify() {
+  // Before checking the constraints inherited from ReduceWindowOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2 * op_->getNumResults() + 5)
+    return op_.emitError("expects size(operands) = 2 * size(results) + 5");
+  if (op_->getNumResults() == 0)
+    return op_.emitError("expects size(results) > 0");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_reduce_window".
+    // called_computations carries the body.
+    if (attr.getName() != "api_version" &&
+        attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "called_computations")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_reduce_window")
+    return op_.emitError() << "expects @stablehlo.dynamic_reduce_window";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto numInputs = getInputs().size();
+  auto inputs = op_.getInputs().slice(0, numInputs);
+  auto initValues = op_.getInputs().slice(numInputs, numInputs);
+  auto windowDimensions = op_.getInputs()[op_.getInputs().size() - 5];
+  auto windowStrides = op_.getInputs()[op_.getInputs().size() - 4];
+  auto baseDilations = op_.getInputs()[op_.getInputs().size() - 3];
+  auto windowDilations = op_.getInputs()[op_.getInputs().size() - 2];
+  auto padding = op_.getInputs()[op_.getInputs().size() - 1];
+  auto results = op_.getResults();
+
+  // reduce_window_c1
+  // This constraint hold automatically thanks to the checks that we have
+  // performed above.
+
+  // reduce_window_i1
+  SmallVector<ShapedType> inputTypes;
+  for (auto [index, input] : llvm::enumerate(inputs)) {
+    auto inputType = input.getType().dyn_cast<ShapedType>();
+    inputTypes.push_back(inputType);
+    if (!inputType)
+      return op_.emitError()
+             << "expects inputs (e.g. operand #" << index << ") to be tensors";
+  }
+
+  // reduce_window_i2
+  SmallVector<ShapedType> initValueTypes;
+  for (auto [index, initValue] : llvm::enumerate(initValues)) {
+    auto initValueType = initValue.getType().dyn_cast<ShapedType>();
+    initValueTypes.push_back(initValueType);
+    if (!initValueType || !initValueType.hasRank() ||
+        initValueType.getRank() != 0)
+      return op_.emitError() << "expects init_values (e.g. operand #"
+                             << numInputs + index << ") "
+                             << "to be 0-dimensional tensors";
+  }
+
+  // reduce_window_i3...reduce_window_i7
+  auto checkRank = [&](StringRef name, int64_t index, Value dynamicAttr,
+                       int64_t expectedRank) -> LogicalResult {
+    auto type = dynamicAttr.getType().dyn_cast<ShapedType>();
+    if (!type || !type.hasRank() || type.getRank() != expectedRank ||
+        !type.getElementType().isIntOrIndex()) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to be a " << expectedRank << "-dimensional tensor "
+             << "of integer or index type";
+    }
+    return success();
+  };
+  if (failed(checkRank("window_dimensions", -5, windowDimensions, 1)) ||
+      failed(checkRank("window_strides", -4, windowStrides, 1)) ||
+      failed(checkRank("base_dilations", -3, baseDilations, 1)) ||
+      failed(checkRank("window_dilations", -2, windowDilations, 1)) ||
+      failed(checkRank("padding", -1, padding, 2)))
+    return failure();
+
+  // reduce_window_i7
+  auto paddingType = getPadding().getType().dyn_cast<ShapedType>();
+  if (!paddingType || !paddingType.hasRank() || paddingType.getRank() != 2 ||
+      paddingType.getDimSize(1) != 2 ||
+      !paddingType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects padding_type (operand #" << op_.getNumOperands() - 1
+           << ") to be a 2-dimensional tensor of integer or index type";
+
+  // reduce_window_c2
+  std::optional<ArrayRef<int64_t>> inputShape;
+  for (auto inputType : inputTypes) {
+    if (!inputType.hasRank()) continue;
+    if (!inputShape) inputShape = inputType.getShape();
+    if (failed(verifyCompatibleShape(inputType.getShape(), *inputShape)))
+      return op_.emitError() << "expects all inputs (operands 0.." << numInputs
+                             << ") to have compatible shapes";
+  }
+
+  // reduce_window_c3
+  for (auto [inputType, initValueType] :
+       llvm::zip(inputTypes, initValueTypes)) {
+    if (inputType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects inputs (operands 0.." << numInputs
+                             << ") and init_values (operands " << numInputs
+                             << ".." << numInputs * 2 << ") to have pairwise "
+                             << "the same element types";
+  }
+
+  // reduce_window_c4...reduce_window_c12
+  // In this range, we only verify the constraints with even numbers.
+  // Verifying the constraints with odd numbers would require knowing the
+  // actual values of window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+  auto checkShape = [&](StringRef name, int64_t index, Value dynamicAttr,
+                        ArrayRef<int64_t> expectedShape) -> LogicalResult {
+    auto type = dynamicAttr.getType().cast<ShapedType>();
+    if (type.getShape() != expectedShape) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to have shape [" << expectedShape << "]";
+    }
+    return success();
+  };
+  if (inputShape) {
+    auto inputRank = static_cast<int64_t>(inputShape->size());
+    if (failed(checkShape("window_dimensions", -5, windowDimensions,
+                          {inputRank})) ||
+        failed(checkShape("window_strides", -4, windowStrides, {inputRank})) ||
+        failed(checkShape("base_dilations", -3, baseDilations, {inputRank})) ||
+        failed(
+            checkShape("window_dilations", -2, windowDilations, {inputRank})) ||
+        failed(checkShape("padding", -1, padding, {inputRank, 2})))
+      return failure();
+  }
+
+  // reduce_window_c13
+  if (op_.getCalledComputations().size() != 1)
+    return op_.emitError() << "expects called_computations to have 1 element";
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  if (!bodyFunc)
+    return op_.emitError() << "expects called_computations to refer to "
+                           << "a function that exists within a parent module";
+
+  // reduce_window_c13
+  SmallVector<Type> expectedBodyInputs;
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  SmallVector<Type> expectedBodyOutputs;
+  llvm::append_range(expectedBodyOutputs, initValueTypes);
+  auto expectedBodyType = FunctionType::get(
+      op_.getContext(), expectedBodyInputs, expectedBodyOutputs);
+  if (bodyFunc.getFunctionType() != expectedBodyType)
+    return op_.emitError() << "expects body to have type " << expectedBodyType;
+
+  // reduce_window_c14
+  SmallVector<ShapedType> resultTypes;
+  std::optional<ArrayRef<int64_t>> resultShape;
+  for (auto result : results) {
+    auto resultType = result.getType().dyn_cast<ShapedType>();
+    resultTypes.push_back(resultType);
+    if (!resultType) return op_.emitError() << "expects results to be tensors";
+
+    if (!resultType.hasRank()) continue;
+    if (!resultShape) resultShape = resultType.getShape();
+    if (failed(verifyCompatibleShape(resultType.getShape(), *resultShape)))
+      return op_.emitError() << "expects all results to have compatible shapes";
+  }
+
+  // reduce_window_c15
+  // Verifying this constraint would require knowing the actual values of
+  // window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+
+  // reduce_window_c16
+  for (auto [resultType, initValueType] :
+       llvm::zip(resultTypes, initValueTypes)) {
+    if (resultType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects results and init_values (operands "
+                             << numInputs << ".." << numInputs * 2 << ") "
+                             << "to have pairwise the same element types";
+  }
+
+  return success();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInputs() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(0, numInputs);
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInitValues() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(numInputs, numInputs);
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDimensions() {
+  return op_.getInputs()[op_.getInputs().size() - 5]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowStrides() {
+  return op_.getInputs()[op_.getInputs().size() - 4]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getBaseDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 3]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 2]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getPadding() {
+  return op_.getInputs()[op_.getInputs().size() - 1]
+      .cast<TypedValue<ShapedType>>();
+}
+
+Region& DynamicReduceWindowOpAdaptor::getBody() {
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  return bodyFunc.getBody();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getResults() {
+  return op_.getResults();
+}
+
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_reduce_window") return {};
+  return DynamicReduceWindowOpAdaptor(op);
+}
+
+LogicalResult DynamicRngBitGeneratorOpAdaptor::verify() {
+  // Before checking the constraints inherited from RngBitGeneratorOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_rng_bit_generator".
+    // rng_algorithm comes from the operation.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "rng_algorithm")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return op_.emitError() << "expects @stablehlo.dynamic_rng_bit_generator";
+  if (!op_->hasAttr("rng_algorithm"))
+    return op_.emitError() << "expects an rng_algorithm";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto rngAlgorithmAttr = op_->getAttr("rng_algorithm");
+  auto initialState = op_.getInputs()[0];
+  auto outputShape = op_.getInputs()[1];
+  auto outputState = op_.getResults()[0];
+  auto output = op_.getResults()[1];
+
+  // dynamic_rng_bit_generator_i1
+  if (!rngAlgorithmAttr.isa<RngAlgorithmAttr>())
+    return op_.emitError()
+           << "expects a #stablehlo<rng_algorithm ...> rng_algorithm";
+
+  // dynamic_rng_bit_generator_i2
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto initialStateType = initialState.getType().dyn_cast<ShapedType>();
+  if (!initialStateType || !initialStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects initial_state (operand #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_i3
+  auto outputShapeType = outputShape.getType().dyn_cast<ShapedType>();
+  if (!outputShapeType || !outputShapeType.hasRank() ||
+      outputShapeType.getRank() != 1 ||
+      !outputShapeType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects output_shape (operand #1) "
+           << "to be a 1-dimensional tensor of integer or index type";
+
+  // dynamic_rng_bit_generator_o1
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto outputStateType = outputState.getType().dyn_cast<ShapedType>();
+  if (!outputStateType || !outputStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output_state (result #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_o2
+  auto outputType = output.getType().dyn_cast<ShapedType>();
+  if (!outputType || !outputType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output (result #1) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_c1
+  if (!hlo::isCompatibleForHloTypeInference(initialStateType, outputStateType))
+    return op_.emitError()
+           << "expects initial_state (operand #0) and output_state (result #0) "
+           << "to have compatible shapes";
+
+  // dynamic_rng_bit_generator_c2
+  // TODO(#486): Verify rng_algorithm in RngBitGeneratorOp.
+
+  // dynamic_rng_bit_generator_c3
+  if (!hlo::isCompatibleForHloTypeInference(outputShape, outputType))
+    return op_.emitError() << "expects output (result #1) to have shape  "
+                           << "compatible with output_shape (operand #2)";
+
+  return success();
+}
+
+RngAlgorithm DynamicRngBitGeneratorOpAdaptor::getRngAlgorithm() {
+  return op_->getAttr("rng_algorithm").cast<RngAlgorithmAttr>().getValue();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getInitialState() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputShape() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputState() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutput() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return {};
+  return DynamicRngBitGeneratorOpAdaptor(op);
+}
+
+LogicalResult DynamicTopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_top_k".
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_top_k")
+    return op_.emitError() << "expects @stablehlo.dynamic_top_k";
+
+  auto operand = op_.getInputs()[0];
+  auto k = op_.getInputs()[1];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+
+  // dynamic_top_k_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_i2
+  auto kType = k.getType().dyn_cast<ShapedType>();
+  if (!kType || !kType.hasRank() ||
+      kType.getRank() != 0 || !kType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects k (operand #1) "
+           << "to be a 0-dimensional tensor of integer or index type";
+
+  // dynamic_top_k_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // dynamic_top_k_c1
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] =
+      valuesType.getDimSize(valuesType.getRank() - 1);
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension";
+
+  // dynamic_top_k_c2
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // dynamic_top_k_c3
+  if (!operandType.isDynamicDim(operandLastDim) &&
+      !valuesType.isDynamicDim(operandLastDim) &&
+      operandType.getDimSize(operandLastDim) <
+          valuesType.getDimSize(operandLastDim))
+    return op_.emitError() << "expects the values last dimension to have size "
+                              "at least as large "
+                           << "as operand last dimension";
+
+  // dynamic_top_k_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getK() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_top_k") return {};
+  return DynamicTopKOpAdaptor(op);
+}
+
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/ExperimentalOps.h b/stablehlo/stablehlo/dialect/ExperimentalOps.h
--- stablehlo/stablehlo/dialect/ExperimentalOps.h
+++ stablehlo/stablehlo/dialect/ExperimentalOps.h
@@ -0,0 +1,227 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_DIALECT_EXPERIMENTAL_OPS_H
+#define STABLEHLO_DIALECT_EXPERIMENTAL_OPS_H
+
+// This file supports XLA-specific experiments with the StableHLO opset.
+// These experiments are not yet ready to be upstreamed to openxla/stablehlo
+// and are incubating towards the respective StableHLO RFCs.
+//
+// Custom calls (which are the implementation vehicle of these experiments)
+// don't have compatibility guarantees within the StableHLO process, but
+// the StableHLO team at Google provides out-of-band guarantees for these
+// custom calls, with the same compatibility window as StableHLO upstream.
+
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LogicalResult.h"
+#include "stablehlo/dialect/StablehloOps.h"
+
+namespace mlir {
+namespace stablehlo {
+
+// The DynamicReduceWindowOp experiment provides a dynamic version of
+// ReduceWindowOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicReduceWindowOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_reduce_window` custom call.
+// This custom call has the following operands which represent a dynamic version
+// of operands and attributes of ReduceWindowOp:
+//   * [0:N]   => inputs
+//   * [N:2*N] => init_values
+//   * [-5]    => window_dimensions
+//   * [-4]    => window_strides
+//   * [-3]    => base_dilations
+//   * [-2]    => window_dilations
+//   * [-1]    => padding
+// Additionally, to represent the body of DynamicReduceWindowOp, the custom call
+// has a satellite function attached to the custom call via called_computations.
+//
+// Semantics of DynamicReduceWindowOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window
+// with the following exceptions:
+//   1) All tensor constants, i.e. window_dimensions, window_strides,
+//      base_dilations, window_dilations and padding, become tensors of
+//      integer type.
+//   2) As a result, some of the constraints can no longer be validated
+//      statically. However, this operation still expects these constraints
+//      to hold dynamically, and if they don't hold, the behavior is undefined.
+class DynamicReduceWindowOpAdaptor {
+ public:
+  DynamicReduceWindowOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::ReduceWindowOp, except that all the
+  // std::optional<DenseIntElementsAttr> attributes have turned into values.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  ValueRange getInputs();
+  ValueRange getInitValues();
+  TypedValue<ShapedType> getWindowDimensions();
+  TypedValue<ShapedType> getWindowStrides();
+  TypedValue<ShapedType> getBaseDilations();
+  TypedValue<ShapedType> getWindowDilations();
+  TypedValue<ShapedType> getPadding();
+  Region& getBody();
+  ValueRange getResults();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicReduceWindowOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_reduce_window".
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op);
+
+// The DynamicRngBitGeneratorOp experiment provides a dynamic version of
+// RngBitGeneratorOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicRngBitGeneratorOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator` custom call.
+// This custom call has the regular operand of RngBitGeneratorOp plus an
+// additional `output_shape` operand that determines the shape of the output:
+//   * [0] => initial_state
+//   * [1] => output_shape
+//
+// Semantics of DynamicRngBitGeneratorOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator
+// extended with an additional input (I3) and an additional constraint (C3):
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `rng_algorithm` | enum of `DEFAULT`, `THREE_FRY`, and `PHILOX` |
+// | (I2)  | `initial_state` | 1-dimensional tensor of type `ui64`          |
+// | (I3)  | `output_shape`  | 1-dimensional tensor of integer type         |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `output_state` | 1-dimensional tensor of type `ui64`      |
+// | `output`       | tensor of integer or floating-point type |
+//
+// #### Constraints
+//
+// * (C1) `type(initial_state) = type(output_state)`.
+// * (C2) `size(initial_state)` is defined as:
+//   * implementation-defined if `rng_algorithm = DEFAULT`.
+//   * `2` if `rng_algorithm = THREE_FRY`.
+//   * `2` or `3` if `rng_algorithm = PHILOX`.
+// * (C3) `shape(output) = output_shape`.
+class DynamicRngBitGeneratorOpAdaptor {
+ public:
+  DynamicRngBitGeneratorOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::RngBitGeneratorOp, extended with the
+  // additional `output_shape` operand.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  RngAlgorithm getRngAlgorithm();
+  TypedValue<ShapedType> getInitialState();
+  TypedValue<ShapedType> getOutputShape();
+  TypedValue<ShapedType> getOutputState();
+  TypedValue<ShapedType> getOutput();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicRngBitGeneratorOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_rng_bit_generator".
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op);
+
+// The DynamicTopKOp experiment provides a dynamic version of
+// TopKOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicTopKOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_top_k` custom call.
+// This custom call has the regular operand of TopKOp plus an
+// additional `k` operand that determines the shape of the output.
+//
+// Semantics of DynamicTopKOp are inherited from semantics of Chlo.TopKOp.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | 0-dimensional tensor of integer or index type|
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `element_type(values) = element_type(operand)`
+// * (C3) `shape(values)[-1] <= shape(operand)[-1]`
+// * (C4) `shape(indices) = shape(values)`
+class DynamicTopKOpAdaptor {
+ public:
+  DynamicTopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getK();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicTopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_top_k".
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op);
+
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_DIALECT_EXPERIMENTAL_OPS_H
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -1467,7 +1467,7 @@
   if (innerOp.getNumOperands() != 2 ||
       !innerOp.hasTrait<mlir::OpTrait::OneResult>() ||
       !hasSameOperandAndResultTypes(innerOp) ||
-      !innerOp.hasTrait<mlir::OpTrait::IsCommutative>() ||
+      !innerOp.hasTrait<mlir::hlo::OpTrait::IsCommutative>() ||
       !innerOp.hasTrait<mlir::OpTrait::ZeroRegions>())
     return false;
 
@@ -1664,7 +1664,7 @@
   if (!innerOpDialect || !innerOpDialect->getNamespace().equals("stablehlo") ||
       !innerOpNameInfo->hasTrait<mlir::OpTrait::NOperands<2>::Impl>() ||
       !innerOpNameInfo->hasTrait<mlir::OpTrait::OneResult>() ||
-      !innerOpNameInfo->hasTrait<mlir::OpTrait::IsCommutative>() ||
+      !innerOpNameInfo->hasTrait<mlir::hlo::OpTrait::IsCommutative>() ||
       !innerOpNameInfo->hasTrait<mlir::OpTrait::ZeroRegions>()) {
     parser.emitError(loc,
                      "expected the inner-op to be a commutative binary-op from "
@@ -2786,7 +2786,8 @@
 void printConvolutionDimensions(AsmPrinter& p,
                                 ConvDimensionNumbersAttr dimNums) {
   // TODO(b/202040055): we should check the attribute invariant and print the
-  // "raw" form if they are violated, otherwise we'll crash here.
+  // "raw" form if they are violated, for now report_fatal_error is used to
+  // prevent invalid access.
   auto printDim =
       [&p](ArrayRef<int64_t> spatialDims,
            ArrayRef<std::pair<int64_t, NonSpatialDim>> non_spatialDims) {
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -687,7 +687,7 @@
 }
 
 def StableHLO_AddOp : StableHLO_BinaryElementwiseOp<"add",
-      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
+      [HLO_Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
   let summary = "Add operation";
   let description = [{
     Performs element-wise addition of two tensors `lhs` and `rhs` and produces a
@@ -769,7 +769,7 @@
 }
 
 def StableHLO_MaxOp : StableHLO_BinaryElementwiseOp<"maximum",
-      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
+      [HLO_Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
   let summary = "Max operation";
   let description = [{
     Performs element-wise max operation on tensors `lhs` and `rhs` and produces
@@ -786,7 +786,7 @@
 }
 
 def StableHLO_MinOp : StableHLO_BinaryElementwiseOp<"minimum",
-      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
+      [HLO_Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
   let summary = "Min operation";
   let description = [{
     Performs element-wise min operation on tensors `lhs` and `rhs` and produces a
@@ -803,7 +803,7 @@
 }
 
 def StableHLO_MulOp : StableHLO_BinaryElementwiseOp<"multiply",
-      [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
+      [HLO_Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
   let summary = "Mul operation";
   let description = [{
     Performs element-wise product of two tensors `lhs` and `rhs` and produces a
@@ -933,7 +933,7 @@
 // See https://www.tensorflow.org/xla/operation_semantics#element-wise_binary_arithmetic_operations
 class StableHLO_BinaryBiwiseOrLogicalElementwiseOp<string mnemonic> :
         StableHLO_BinaryElementwiseOp<mnemonic,
-          [Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
+          [HLO_Commutative, Pure, HLO_CompatibleOperandsAndResultType]> {
   let arguments = (ins
     HLO_PredOrIntTensor:$lhs,
     HLO_PredOrIntTensor:$rhs
diff --ruN a/stablehlo/stablehlo/dialect/TypeInference.cpp b/stablehlo/stablehlo/dialect/TypeInference.cpp
--- stablehlo/stablehlo/dialect/TypeInference.cpp
+++ stablehlo/stablehlo/dialect/TypeInference.cpp
@@ -3400,7 +3400,7 @@
 
   for (int i = 0; i != bcastDimensionsSize; ++i) {
     auto dimIndex = bcastDimensions.getValues<int64_t>()[i];
-    if (dimIndex >= resultRank)
+    if (dimIndex < 0 || dimIndex >= resultRank)
       return emitOptionalError(location,
                                "broadcast_dimensions contains invalid value ",
                                dimIndex, " for result with rank ", resultRank);
diff --ruN a/stablehlo/stablehlo/testdata/acosh_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/acosh_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/acosh_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/acosh_shape_bfloat16_20_20.mlir
@@ -16,9 +16,9 @@
     %10 = stablehlo.constant dense<6.914060e-01> : tensor<20x20xbf16>
     %11 = stablehlo.add %8, %10 : tensor<20x20xbf16>
     %12 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xbf16>
-    %13 = stablehlo.add %12, %0 : tensor<20x20xbf16>
+    %13 = stablehlo.add %0, %12 : tensor<20x20xbf16>
     %14 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xbf16>
-    %15 = stablehlo.add %14, %0 : tensor<20x20xbf16>
+    %15 = stablehlo.add %0, %14 : tensor<20x20xbf16>
     %16 = stablehlo.multiply %13, %15 : tensor<20x20xbf16>
     %17 = stablehlo.sqrt %16 : tensor<20x20xbf16>
     %18 = stablehlo.add %0, %17 : tensor<20x20xbf16>
diff --ruN a/stablehlo/stablehlo/testdata/acosh_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/acosh_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/acosh_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/acosh_shape_float16_20_20.mlir
@@ -16,9 +16,9 @@
     %10 = stablehlo.constant dense<6.933590e-01> : tensor<20x20xf16>
     %11 = stablehlo.add %8, %10 : tensor<20x20xf16>
     %12 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf16>
-    %13 = stablehlo.add %12, %0 : tensor<20x20xf16>
+    %13 = stablehlo.add %0, %12 : tensor<20x20xf16>
     %14 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf16>
-    %15 = stablehlo.add %14, %0 : tensor<20x20xf16>
+    %15 = stablehlo.add %0, %14 : tensor<20x20xf16>
     %16 = stablehlo.multiply %13, %15 : tensor<20x20xf16>
     %17 = stablehlo.sqrt %16 : tensor<20x20xf16>
     %18 = stablehlo.add %0, %17 : tensor<20x20xf16>
diff --ruN a/stablehlo/stablehlo/testdata/acosh_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/acosh_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/acosh_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/acosh_shape_float32_20_20.mlir
@@ -16,9 +16,9 @@
     %10 = stablehlo.constant dense<0.693147182> : tensor<20x20xf32>
     %11 = stablehlo.add %8, %10 : tensor<20x20xf32>
     %12 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
-    %13 = stablehlo.add %12, %0 : tensor<20x20xf32>
+    %13 = stablehlo.add %0, %12 : tensor<20x20xf32>
     %14 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %15 = stablehlo.add %14, %0 : tensor<20x20xf32>
+    %15 = stablehlo.add %0, %14 : tensor<20x20xf32>
     %16 = stablehlo.multiply %13, %15 : tensor<20x20xf32>
     %17 = stablehlo.sqrt %16 : tensor<20x20xf32>
     %18 = stablehlo.add %0, %17 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/asin_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/asin_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/asin_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/asin_shape_bfloat16_20_20.mlir
@@ -11,9 +11,9 @@
     %5 = stablehlo.multiply %0, %0 : tensor<20x20xbf16>
     %6 = stablehlo.subtract %4, %5 : tensor<20x20xbf16>
     %7 = stablehlo.sqrt %6 : tensor<20x20xbf16>
-    %8 = stablehlo.add %3, %7 : tensor<20x20xbf16>
+    %8 = stablehlo.add %7, %3 : tensor<20x20xbf16>
     %9 = stablehlo.atan2 %0, %8 : tensor<20x20xbf16>
-    %10 = stablehlo.multiply %2, %9 : tensor<20x20xbf16>
+    %10 = stablehlo.multiply %9, %2 : tensor<20x20xbf16>
     %11 = stablehlo.custom_call @check.eq(%10, %1) : (tensor<20x20xbf16>, tensor<20x20xbf16>) -> tensor<i1>
     return %11 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/asin_shape_complex64_20_20.mlir b/stablehlo/stablehlo/testdata/asin_shape_complex64_20_20.mlir
--- stablehlo/stablehlo/testdata/asin_shape_complex64_20_20.mlir
+++ stablehlo/stablehlo/testdata/asin_shape_complex64_20_20.mlir
@@ -11,9 +11,9 @@
     %5 = stablehlo.multiply %0, %0 : tensor<20x20xcomplex<f32>>
     %6 = stablehlo.subtract %4, %5 : tensor<20x20xcomplex<f32>>
     %7 = stablehlo.sqrt %6 : tensor<20x20xcomplex<f32>>
-    %8 = stablehlo.add %3, %7 : tensor<20x20xcomplex<f32>>
+    %8 = stablehlo.add %7, %3 : tensor<20x20xcomplex<f32>>
     %9 = stablehlo.atan2 %0, %8 : tensor<20x20xcomplex<f32>>
-    %10 = stablehlo.multiply %2, %9 : tensor<20x20xcomplex<f32>>
+    %10 = stablehlo.multiply %9, %2 : tensor<20x20xcomplex<f32>>
     %11 = stablehlo.custom_call @check.eq(%10, %1) : (tensor<20x20xcomplex<f32>>, tensor<20x20xcomplex<f32>>) -> tensor<i1>
     return %11 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/asin_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/asin_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/asin_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/asin_shape_float16_20_20.mlir
@@ -11,9 +11,9 @@
     %5 = stablehlo.multiply %0, %0 : tensor<20x20xf16>
     %6 = stablehlo.subtract %4, %5 : tensor<20x20xf16>
     %7 = stablehlo.sqrt %6 : tensor<20x20xf16>
-    %8 = stablehlo.add %3, %7 : tensor<20x20xf16>
+    %8 = stablehlo.add %7, %3 : tensor<20x20xf16>
     %9 = stablehlo.atan2 %0, %8 : tensor<20x20xf16>
-    %10 = stablehlo.multiply %2, %9 : tensor<20x20xf16>
+    %10 = stablehlo.multiply %9, %2 : tensor<20x20xf16>
     %11 = stablehlo.custom_call @check.eq(%10, %1) : (tensor<20x20xf16>, tensor<20x20xf16>) -> tensor<i1>
     return %11 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/asin_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/asin_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/asin_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/asin_shape_float32_20_20.mlir
@@ -11,9 +11,9 @@
     %5 = stablehlo.multiply %0, %0 : tensor<20x20xf32>
     %6 = stablehlo.subtract %4, %5 : tensor<20x20xf32>
     %7 = stablehlo.sqrt %6 : tensor<20x20xf32>
-    %8 = stablehlo.add %3, %7 : tensor<20x20xf32>
+    %8 = stablehlo.add %7, %3 : tensor<20x20xf32>
     %9 = stablehlo.atan2 %0, %8 : tensor<20x20xf32>
-    %10 = stablehlo.multiply %2, %9 : tensor<20x20xf32>
+    %10 = stablehlo.multiply %9, %2 : tensor<20x20xf32>
     %11 = stablehlo.custom_call @check.eq(%10, %1) : (tensor<20x20xf32>, tensor<20x20xf32>) -> tensor<i1>
     return %11 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/asinh_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/asinh_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/asinh_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/asinh_shape_bfloat16_20_20.mlir
@@ -28,7 +28,7 @@
     %22 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xbf16>
     %23 = stablehlo.add %21, %22 : tensor<20x20xbf16>
     %24 = stablehlo.sqrt %23 : tensor<20x20xbf16>
-    %25 = stablehlo.add %18, %24 : tensor<20x20xbf16>
+    %25 = stablehlo.add %24, %18 : tensor<20x20xbf16>
     %26 = stablehlo.divide %17, %25 : tensor<20x20xbf16>
     %27 = stablehlo.multiply %16, %26 : tensor<20x20xbf16>
     %28 = stablehlo.add %15, %27 : tensor<20x20xbf16>
diff --ruN a/stablehlo/stablehlo/testdata/asinh_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/asinh_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/asinh_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/asinh_shape_float16_20_20.mlir
@@ -28,7 +28,7 @@
     %22 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf16>
     %23 = stablehlo.add %21, %22 : tensor<20x20xf16>
     %24 = stablehlo.sqrt %23 : tensor<20x20xf16>
-    %25 = stablehlo.add %18, %24 : tensor<20x20xf16>
+    %25 = stablehlo.add %24, %18 : tensor<20x20xf16>
     %26 = stablehlo.divide %17, %25 : tensor<20x20xf16>
     %27 = stablehlo.multiply %16, %26 : tensor<20x20xf16>
     %28 = stablehlo.add %15, %27 : tensor<20x20xf16>
diff --ruN a/stablehlo/stablehlo/testdata/asinh_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/asinh_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/asinh_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/asinh_shape_float32_20_20.mlir
@@ -28,7 +28,7 @@
     %22 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %23 = stablehlo.add %21, %22 : tensor<20x20xf32>
     %24 = stablehlo.sqrt %23 : tensor<20x20xf32>
-    %25 = stablehlo.add %18, %24 : tensor<20x20xf32>
+    %25 = stablehlo.add %24, %18 : tensor<20x20xf32>
     %26 = stablehlo.divide %17, %25 : tensor<20x20xf32>
     %27 = stablehlo.multiply %16, %26 : tensor<20x20xf32>
     %28 = stablehlo.add %15, %27 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/bessel_i0e_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/bessel_i0e_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/bessel_i0e_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/bessel_i0e_shape_bfloat16_20_20.mlir
@@ -33,7 +33,7 @@
     %8 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
     %9 = stablehlo.constant dense<5.000000e-01> : tensor<f32>
     %10 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
-    %11 = stablehlo.multiply %10, %3 : tensor<20x20xf32>
+    %11 = stablehlo.multiply %3, %10 : tensor<20x20xf32>
     %12 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
     %13 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %14 = stablehlo.subtract %11, %13 : tensor<20x20xf32>
@@ -133,7 +133,7 @@
     %108 = stablehlo.constant dense<0.676795303> : tensor<20x20xf32>
     %109 = stablehlo.add %106, %108 : tensor<20x20xf32>
     %110 = stablehlo.subtract %109, %99 : tensor<20x20xf32>
-    %111 = stablehlo.multiply %8, %110 : tensor<20x20xf32>
+    %111 = stablehlo.multiply %110, %8 : tensor<20x20xf32>
     %112 = stablehlo.constant dense<5.000000e-01> : tensor<f32>
     %113 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
     %114 = stablehlo.constant dense<3.200000e+01> : tensor<f32>
@@ -182,7 +182,7 @@
     %157 = stablehlo.constant dense<0.804490387> : tensor<20x20xf32>
     %158 = stablehlo.add %155, %157 : tensor<20x20xf32>
     %159 = stablehlo.subtract %158, %148 : tensor<20x20xf32>
-    %160 = stablehlo.multiply %113, %159 : tensor<20x20xf32>
+    %160 = stablehlo.multiply %159, %113 : tensor<20x20xf32>
     %161 = stablehlo.sqrt %3 : tensor<20x20xf32>
     %162 = stablehlo.divide %160, %161 : tensor<20x20xf32>
     %163 = stablehlo.select %6, %111, %162 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/bessel_i0e_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/bessel_i0e_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/bessel_i0e_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/bessel_i0e_shape_float16_20_20.mlir
@@ -33,7 +33,7 @@
     %8 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
     %9 = stablehlo.constant dense<5.000000e-01> : tensor<f32>
     %10 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
-    %11 = stablehlo.multiply %10, %3 : tensor<20x20xf32>
+    %11 = stablehlo.multiply %3, %10 : tensor<20x20xf32>
     %12 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
     %13 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %14 = stablehlo.subtract %11, %13 : tensor<20x20xf32>
@@ -133,7 +133,7 @@
     %108 = stablehlo.constant dense<0.676795303> : tensor<20x20xf32>
     %109 = stablehlo.add %106, %108 : tensor<20x20xf32>
     %110 = stablehlo.subtract %109, %99 : tensor<20x20xf32>
-    %111 = stablehlo.multiply %8, %110 : tensor<20x20xf32>
+    %111 = stablehlo.multiply %110, %8 : tensor<20x20xf32>
     %112 = stablehlo.constant dense<5.000000e-01> : tensor<f32>
     %113 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
     %114 = stablehlo.constant dense<3.200000e+01> : tensor<f32>
@@ -182,7 +182,7 @@
     %157 = stablehlo.constant dense<0.804490387> : tensor<20x20xf32>
     %158 = stablehlo.add %155, %157 : tensor<20x20xf32>
     %159 = stablehlo.subtract %158, %148 : tensor<20x20xf32>
-    %160 = stablehlo.multiply %113, %159 : tensor<20x20xf32>
+    %160 = stablehlo.multiply %159, %113 : tensor<20x20xf32>
     %161 = stablehlo.sqrt %3 : tensor<20x20xf32>
     %162 = stablehlo.divide %160, %161 : tensor<20x20xf32>
     %163 = stablehlo.select %6, %111, %162 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/bessel_i0e_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/bessel_i0e_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/bessel_i0e_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/bessel_i0e_shape_float32_20_20.mlir
@@ -32,7 +32,7 @@
     %7 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
     %8 = stablehlo.constant dense<5.000000e-01> : tensor<f32>
     %9 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
-    %10 = stablehlo.multiply %9, %2 : tensor<20x20xf32>
+    %10 = stablehlo.multiply %2, %9 : tensor<20x20xf32>
     %11 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
     %12 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %13 = stablehlo.subtract %10, %12 : tensor<20x20xf32>
@@ -132,7 +132,7 @@
     %107 = stablehlo.constant dense<0.676795303> : tensor<20x20xf32>
     %108 = stablehlo.add %105, %107 : tensor<20x20xf32>
     %109 = stablehlo.subtract %108, %98 : tensor<20x20xf32>
-    %110 = stablehlo.multiply %7, %109 : tensor<20x20xf32>
+    %110 = stablehlo.multiply %109, %7 : tensor<20x20xf32>
     %111 = stablehlo.constant dense<5.000000e-01> : tensor<f32>
     %112 = stablehlo.constant dense<5.000000e-01> : tensor<20x20xf32>
     %113 = stablehlo.constant dense<3.200000e+01> : tensor<f32>
@@ -181,7 +181,7 @@
     %156 = stablehlo.constant dense<0.804490387> : tensor<20x20xf32>
     %157 = stablehlo.add %154, %156 : tensor<20x20xf32>
     %158 = stablehlo.subtract %157, %147 : tensor<20x20xf32>
-    %159 = stablehlo.multiply %112, %158 : tensor<20x20xf32>
+    %159 = stablehlo.multiply %158, %112 : tensor<20x20xf32>
     %160 = stablehlo.sqrt %2 : tensor<20x20xf32>
     %161 = stablehlo.divide %159, %160 : tensor<20x20xf32>
     %162 = stablehlo.select %5, %110, %161 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/bessel_i1e_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/bessel_i1e_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/bessel_i1e_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/bessel_i1e_shape_bfloat16_20_20.mlir
@@ -11,7 +11,7 @@
     %5 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %6 = stablehlo.constant dense<3.200000e+01> : tensor<20x20xf32>
     %7 = stablehlo.constant dense<8.000000e+00> : tensor<20x20xf32>
-    %8 = stablehlo.multiply %4, %3 : tensor<20x20xf32>
+    %8 = stablehlo.multiply %3, %4 : tensor<20x20xf32>
     %9 = stablehlo.subtract %8, %5 : tensor<20x20xf32>
     %10 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
     %11 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/bessel_i1e_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/bessel_i1e_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/bessel_i1e_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/bessel_i1e_shape_float16_20_20.mlir
@@ -11,7 +11,7 @@
     %5 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %6 = stablehlo.constant dense<3.200000e+01> : tensor<20x20xf32>
     %7 = stablehlo.constant dense<8.000000e+00> : tensor<20x20xf32>
-    %8 = stablehlo.multiply %4, %3 : tensor<20x20xf32>
+    %8 = stablehlo.multiply %3, %4 : tensor<20x20xf32>
     %9 = stablehlo.subtract %8, %5 : tensor<20x20xf32>
     %10 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
     %11 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/bessel_i1e_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/bessel_i1e_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/bessel_i1e_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/bessel_i1e_shape_float32_20_20.mlir
@@ -10,7 +10,7 @@
     %4 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %5 = stablehlo.constant dense<3.200000e+01> : tensor<20x20xf32>
     %6 = stablehlo.constant dense<8.000000e+00> : tensor<20x20xf32>
-    %7 = stablehlo.multiply %3, %2 : tensor<20x20xf32>
+    %7 = stablehlo.multiply %2, %3 : tensor<20x20xf32>
     %8 = stablehlo.subtract %7, %4 : tensor<20x20xf32>
     %9 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
     %10 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
@@ -16,7 +16,7 @@
     %11 = stablehlo.constant dense<1> : tensor<i64>
     %12 = stablehlo.subtract %3, %11 : tensor<i64>
     %13 = stablehlo.select %10, %12, %3 : tensor<i1>, tensor<i64>
-    %14 = stablehlo.multiply %2, %13 : tensor<i64>
+    %14 = stablehlo.multiply %13, %2 : tensor<i64>
     %15 = stablehlo.subtract %1, %14 : tensor<i64>
     %16 = stablehlo.constant dense<2> : tensor<i64>
     %17 = stablehlo.add %15, %16 : tensor<i64>
@@ -32,7 +32,7 @@
     %27 = stablehlo.constant dense<1> : tensor<i64>
     %28 = stablehlo.subtract %19, %27 : tensor<i64>
     %29 = stablehlo.select %26, %28, %19 : tensor<i1>, tensor<i64>
-    %30 = stablehlo.multiply %18, %29 : tensor<i64>
+    %30 = stablehlo.multiply %29, %18 : tensor<i64>
     %31 = stablehlo.subtract %17, %30 : tensor<i64>
     %32 = stablehlo.constant dense<-1> : tensor<i64>
     %33 = stablehlo.multiply %arg0, %32 : tensor<i64>
@@ -48,7 +48,7 @@
     %43 = stablehlo.constant dense<1> : tensor<i64>
     %44 = stablehlo.subtract %35, %43 : tensor<i64>
     %45 = stablehlo.select %42, %44, %35 : tensor<i1>, tensor<i64>
-    %46 = stablehlo.multiply %34, %45 : tensor<i64>
+    %46 = stablehlo.multiply %45, %34 : tensor<i64>
     %47 = stablehlo.subtract %33, %46 : tensor<i64>
     %48 = stablehlo.constant dense<-1> : tensor<i64>
     %49 = stablehlo.multiply %arg0, %48 : tensor<i64>
@@ -64,7 +64,7 @@
     %59 = stablehlo.constant dense<1> : tensor<i64>
     %60 = stablehlo.subtract %51, %59 : tensor<i64>
     %61 = stablehlo.select %58, %60, %51 : tensor<i1>, tensor<i64>
-    %62 = stablehlo.multiply %50, %61 : tensor<i64>
+    %62 = stablehlo.multiply %61, %50 : tensor<i64>
     %63 = stablehlo.subtract %49, %62 : tensor<i64>
     %64 = stablehlo.constant dense<2> : tensor<i64>
     %65 = stablehlo.add %63, %64 : tensor<i64>
@@ -80,7 +80,7 @@
     %75 = stablehlo.constant dense<1> : tensor<i64>
     %76 = stablehlo.subtract %67, %75 : tensor<i64>
     %77 = stablehlo.select %74, %76, %67 : tensor<i1>, tensor<i64>
-    %78 = stablehlo.multiply %66, %77 : tensor<i64>
+    %78 = stablehlo.multiply %77, %66 : tensor<i64>
     %79 = stablehlo.subtract %65, %78 : tensor<i64>
     %80 = stablehlo.constant dense<-1> : tensor<i64>
     %81 = stablehlo.multiply %77, %80 : tensor<i64>
diff --ruN a/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
@@ -16,7 +16,7 @@
     %11 = stablehlo.constant dense<1> : tensor<i64>
     %12 = stablehlo.subtract %3, %11 : tensor<i64>
     %13 = stablehlo.select %10, %12, %3 : tensor<i1>, tensor<i64>
-    %14 = stablehlo.multiply %2, %13 : tensor<i64>
+    %14 = stablehlo.multiply %13, %2 : tensor<i64>
     %15 = stablehlo.subtract %1, %14 : tensor<i64>
     %16 = stablehlo.constant dense<2> : tensor<i64>
     %17 = stablehlo.add %15, %16 : tensor<i64>
@@ -32,7 +32,7 @@
     %27 = stablehlo.constant dense<1> : tensor<i64>
     %28 = stablehlo.subtract %19, %27 : tensor<i64>
     %29 = stablehlo.select %26, %28, %19 : tensor<i1>, tensor<i64>
-    %30 = stablehlo.multiply %18, %29 : tensor<i64>
+    %30 = stablehlo.multiply %29, %18 : tensor<i64>
     %31 = stablehlo.subtract %17, %30 : tensor<i64>
     %32 = stablehlo.constant dense<-1> : tensor<i64>
     %33 = stablehlo.multiply %arg0, %32 : tensor<i64>
@@ -48,7 +48,7 @@
     %43 = stablehlo.constant dense<1> : tensor<i64>
     %44 = stablehlo.subtract %35, %43 : tensor<i64>
     %45 = stablehlo.select %42, %44, %35 : tensor<i1>, tensor<i64>
-    %46 = stablehlo.multiply %34, %45 : tensor<i64>
+    %46 = stablehlo.multiply %45, %34 : tensor<i64>
     %47 = stablehlo.subtract %33, %46 : tensor<i64>
     %48 = stablehlo.constant dense<-1> : tensor<i64>
     %49 = stablehlo.multiply %arg0, %48 : tensor<i64>
@@ -64,7 +64,7 @@
     %59 = stablehlo.constant dense<1> : tensor<i64>
     %60 = stablehlo.subtract %51, %59 : tensor<i64>
     %61 = stablehlo.select %58, %60, %51 : tensor<i1>, tensor<i64>
-    %62 = stablehlo.multiply %50, %61 : tensor<i64>
+    %62 = stablehlo.multiply %61, %50 : tensor<i64>
     %63 = stablehlo.subtract %49, %62 : tensor<i64>
     %64 = stablehlo.constant dense<2> : tensor<i64>
     %65 = stablehlo.add %63, %64 : tensor<i64>
@@ -80,7 +80,7 @@
     %75 = stablehlo.constant dense<1> : tensor<i64>
     %76 = stablehlo.subtract %67, %75 : tensor<i64>
     %77 = stablehlo.select %74, %76, %67 : tensor<i1>, tensor<i64>
-    %78 = stablehlo.multiply %66, %77 : tensor<i64>
+    %78 = stablehlo.multiply %77, %66 : tensor<i64>
     %79 = stablehlo.subtract %65, %78 : tensor<i64>
     %80 = stablehlo.constant dense<-1> : tensor<i64>
     %81 = stablehlo.multiply %77, %80 : tensor<i64>
diff --ruN a/stablehlo/stablehlo/testdata/digamma_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/digamma_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/digamma_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/digamma_shape_bfloat16_20_20.mlir
@@ -21,7 +21,7 @@
     %15 = stablehlo.divide %11, %14 : tensor<20x20xf32>
     %16 = stablehlo.subtract %9, %15 : tensor<20x20xf32>
     %17 = stablehlo.divide %11, %13 : tensor<20x20xf32>
-    %18 = stablehlo.add %10, %17 : tensor<20x20xf32>
+    %18 = stablehlo.add %17, %10 : tensor<20x20xf32>
     %19 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %20 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %21 = stablehlo.add %8, %20 : tensor<20x20xf32>
@@ -79,11 +79,11 @@
     %73 = stablehlo.divide %67, %69 : tensor<20x20xf32>
     %74 = stablehlo.add %66, %73 : tensor<20x20xf32>
     %75 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %76 = stablehlo.add %75, %8 : tensor<20x20xf32>
+    %76 = stablehlo.add %8, %75 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %78 = stablehlo.divide %8, %75 : tensor<20x20xf32>
     %79 = stablehlo.log_plus_one %78 : tensor<20x20xf32>
-    %80 = stablehlo.add %77, %79 : tensor<20x20xf32>
+    %80 = stablehlo.add %79, %77 : tensor<20x20xf32>
     %81 = stablehlo.divide %72, %74 : tensor<20x20xf32>
     %82 = stablehlo.constant dense<7.000000e+00> : tensor<20x20xf32>
     %83 = stablehlo.divide %82, %76 : tensor<20x20xf32>
@@ -95,10 +95,10 @@
     %89 = stablehlo.abs %88 : tensor<20x20xf32>
     %90 = stablehlo.add %2, %89 : tensor<20x20xf32>
     %91 = stablehlo.constant dense<3.14159274> : tensor<20x20xf32>
-    %92 = stablehlo.multiply %91, %90 : tensor<20x20xf32>
+    %92 = stablehlo.multiply %90, %91 : tensor<20x20xf32>
     %93 = stablehlo.cosine %92 : tensor<20x20xf32>
     %94 = stablehlo.sine %92 : tensor<20x20xf32>
-    %95 = stablehlo.multiply %91, %93 : tensor<20x20xf32>
+    %95 = stablehlo.multiply %93, %91 : tensor<20x20xf32>
     %96 = stablehlo.divide %95, %94 : tensor<20x20xf32>
     %97 = stablehlo.subtract %85, %96 : tensor<20x20xf32>
     %98 = stablehlo.select %4, %97, %85 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/digamma_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/digamma_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/digamma_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/digamma_shape_float16_20_20.mlir
@@ -21,7 +21,7 @@
     %15 = stablehlo.divide %11, %14 : tensor<20x20xf32>
     %16 = stablehlo.subtract %9, %15 : tensor<20x20xf32>
     %17 = stablehlo.divide %11, %13 : tensor<20x20xf32>
-    %18 = stablehlo.add %10, %17 : tensor<20x20xf32>
+    %18 = stablehlo.add %17, %10 : tensor<20x20xf32>
     %19 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %20 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %21 = stablehlo.add %8, %20 : tensor<20x20xf32>
@@ -79,11 +79,11 @@
     %73 = stablehlo.divide %67, %69 : tensor<20x20xf32>
     %74 = stablehlo.add %66, %73 : tensor<20x20xf32>
     %75 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %76 = stablehlo.add %75, %8 : tensor<20x20xf32>
+    %76 = stablehlo.add %8, %75 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %78 = stablehlo.divide %8, %75 : tensor<20x20xf32>
     %79 = stablehlo.log_plus_one %78 : tensor<20x20xf32>
-    %80 = stablehlo.add %77, %79 : tensor<20x20xf32>
+    %80 = stablehlo.add %79, %77 : tensor<20x20xf32>
     %81 = stablehlo.divide %72, %74 : tensor<20x20xf32>
     %82 = stablehlo.constant dense<7.000000e+00> : tensor<20x20xf32>
     %83 = stablehlo.divide %82, %76 : tensor<20x20xf32>
@@ -95,10 +95,10 @@
     %89 = stablehlo.abs %88 : tensor<20x20xf32>
     %90 = stablehlo.add %2, %89 : tensor<20x20xf32>
     %91 = stablehlo.constant dense<3.14159274> : tensor<20x20xf32>
-    %92 = stablehlo.multiply %91, %90 : tensor<20x20xf32>
+    %92 = stablehlo.multiply %90, %91 : tensor<20x20xf32>
     %93 = stablehlo.cosine %92 : tensor<20x20xf32>
     %94 = stablehlo.sine %92 : tensor<20x20xf32>
-    %95 = stablehlo.multiply %91, %93 : tensor<20x20xf32>
+    %95 = stablehlo.multiply %93, %91 : tensor<20x20xf32>
     %96 = stablehlo.divide %95, %94 : tensor<20x20xf32>
     %97 = stablehlo.subtract %85, %96 : tensor<20x20xf32>
     %98 = stablehlo.select %4, %97, %85 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/digamma_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/digamma_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/digamma_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/digamma_shape_float32_20_20.mlir
@@ -20,7 +20,7 @@
     %14 = stablehlo.divide %10, %13 : tensor<20x20xf32>
     %15 = stablehlo.subtract %8, %14 : tensor<20x20xf32>
     %16 = stablehlo.divide %10, %12 : tensor<20x20xf32>
-    %17 = stablehlo.add %9, %16 : tensor<20x20xf32>
+    %17 = stablehlo.add %16, %9 : tensor<20x20xf32>
     %18 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %19 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %20 = stablehlo.add %7, %19 : tensor<20x20xf32>
@@ -78,11 +78,11 @@
     %72 = stablehlo.divide %66, %68 : tensor<20x20xf32>
     %73 = stablehlo.add %65, %72 : tensor<20x20xf32>
     %74 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %75 = stablehlo.add %74, %7 : tensor<20x20xf32>
+    %75 = stablehlo.add %7, %74 : tensor<20x20xf32>
     %76 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %77 = stablehlo.divide %7, %74 : tensor<20x20xf32>
     %78 = stablehlo.log_plus_one %77 : tensor<20x20xf32>
-    %79 = stablehlo.add %76, %78 : tensor<20x20xf32>
+    %79 = stablehlo.add %78, %76 : tensor<20x20xf32>
     %80 = stablehlo.divide %71, %73 : tensor<20x20xf32>
     %81 = stablehlo.constant dense<7.000000e+00> : tensor<20x20xf32>
     %82 = stablehlo.divide %81, %75 : tensor<20x20xf32>
@@ -94,10 +94,10 @@
     %88 = stablehlo.abs %87 : tensor<20x20xf32>
     %89 = stablehlo.add %0, %88 : tensor<20x20xf32>
     %90 = stablehlo.constant dense<3.14159274> : tensor<20x20xf32>
-    %91 = stablehlo.multiply %90, %89 : tensor<20x20xf32>
+    %91 = stablehlo.multiply %89, %90 : tensor<20x20xf32>
     %92 = stablehlo.cosine %91 : tensor<20x20xf32>
     %93 = stablehlo.sine %91 : tensor<20x20xf32>
-    %94 = stablehlo.multiply %90, %92 : tensor<20x20xf32>
+    %94 = stablehlo.multiply %92, %90 : tensor<20x20xf32>
     %95 = stablehlo.divide %94, %93 : tensor<20x20xf32>
     %96 = stablehlo.subtract %84, %95 : tensor<20x20xf32>
     %97 = stablehlo.select %3, %96, %84 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/erf_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/erf_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/erf_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/erf_shape_bfloat16_20_20.mlir
@@ -11,7 +11,7 @@
     %5 = stablehlo.clamp %3, %2, %4 : tensor<20x20xf32>
     %6 = stablehlo.multiply %5, %5 : tensor<20x20xf32>
     %7 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %8 = stablehlo.multiply %7, %6 : tensor<20x20xf32>
+    %8 = stablehlo.multiply %6, %7 : tensor<20x20xf32>
     %9 = stablehlo.constant dense<-2.72614237E-10> : tensor<20x20xf32>
     %10 = stablehlo.add %8, %9 : tensor<20x20xf32>
     %11 = stablehlo.multiply %10, %6 : tensor<20x20xf32>
@@ -33,7 +33,7 @@
     %27 = stablehlo.constant dense<-0.0160960332> : tensor<20x20xf32>
     %28 = stablehlo.add %26, %27 : tensor<20x20xf32>
     %29 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %30 = stablehlo.multiply %29, %6 : tensor<20x20xf32>
+    %30 = stablehlo.multiply %6, %29 : tensor<20x20xf32>
     %31 = stablehlo.constant dense<-1.45660715E-5> : tensor<20x20xf32>
     %32 = stablehlo.add %30, %31 : tensor<20x20xf32>
     %33 = stablehlo.multiply %32, %6 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/erf_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/erf_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/erf_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/erf_shape_float16_20_20.mlir
@@ -11,7 +11,7 @@
     %5 = stablehlo.clamp %3, %2, %4 : tensor<20x20xf32>
     %6 = stablehlo.multiply %5, %5 : tensor<20x20xf32>
     %7 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %8 = stablehlo.multiply %7, %6 : tensor<20x20xf32>
+    %8 = stablehlo.multiply %6, %7 : tensor<20x20xf32>
     %9 = stablehlo.constant dense<-2.72614237E-10> : tensor<20x20xf32>
     %10 = stablehlo.add %8, %9 : tensor<20x20xf32>
     %11 = stablehlo.multiply %10, %6 : tensor<20x20xf32>
@@ -33,7 +33,7 @@
     %27 = stablehlo.constant dense<-0.0160960332> : tensor<20x20xf32>
     %28 = stablehlo.add %26, %27 : tensor<20x20xf32>
     %29 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %30 = stablehlo.multiply %29, %6 : tensor<20x20xf32>
+    %30 = stablehlo.multiply %6, %29 : tensor<20x20xf32>
     %31 = stablehlo.constant dense<-1.45660715E-5> : tensor<20x20xf32>
     %32 = stablehlo.add %30, %31 : tensor<20x20xf32>
     %33 = stablehlo.multiply %32, %6 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/erf_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/erf_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/erf_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/erf_shape_float32_20_20.mlir
@@ -10,7 +10,7 @@
     %4 = stablehlo.clamp %2, %0, %3 : tensor<20x20xf32>
     %5 = stablehlo.multiply %4, %4 : tensor<20x20xf32>
     %6 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %7 = stablehlo.multiply %6, %5 : tensor<20x20xf32>
+    %7 = stablehlo.multiply %5, %6 : tensor<20x20xf32>
     %8 = stablehlo.constant dense<-2.72614237E-10> : tensor<20x20xf32>
     %9 = stablehlo.add %7, %8 : tensor<20x20xf32>
     %10 = stablehlo.multiply %9, %5 : tensor<20x20xf32>
@@ -32,7 +32,7 @@
     %26 = stablehlo.constant dense<-0.0160960332> : tensor<20x20xf32>
     %27 = stablehlo.add %25, %26 : tensor<20x20xf32>
     %28 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %29 = stablehlo.multiply %28, %5 : tensor<20x20xf32>
+    %29 = stablehlo.multiply %5, %28 : tensor<20x20xf32>
     %30 = stablehlo.constant dense<-1.45660715E-5> : tensor<20x20xf32>
     %31 = stablehlo.add %29, %30 : tensor<20x20xf32>
     %32 = stablehlo.multiply %31, %5 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/erfc_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/erfc_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/erfc_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/erfc_shape_bfloat16_20_20.mlir
@@ -17,7 +17,7 @@
     %11 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %12 = stablehlo.compare  LT, %5, %11 : (tensor<20x20xf32>, tensor<20x20xf32>) -> tensor<20x20xi1>
     %13 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %14 = stablehlo.multiply %13, %7 : tensor<20x20xf32>
+    %14 = stablehlo.multiply %7, %13 : tensor<20x20xf32>
     %15 = stablehlo.constant dense<2.326820e-02> : tensor<20x20xf32>
     %16 = stablehlo.add %14, %15 : tensor<20x20xf32>
     %17 = stablehlo.multiply %16, %7 : tensor<20x20xf32>
@@ -45,7 +45,7 @@
     %39 = stablehlo.constant dense<0.563825965> : tensor<20x20xf32>
     %40 = stablehlo.add %38, %39 : tensor<20x20xf32>
     %41 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %42 = stablehlo.multiply %41, %7 : tensor<20x20xf32>
+    %42 = stablehlo.multiply %7, %41 : tensor<20x20xf32>
     %43 = stablehlo.constant dense<-10.477664> : tensor<20x20xf32>
     %44 = stablehlo.add %42, %43 : tensor<20x20xf32>
     %45 = stablehlo.multiply %44, %7 : tensor<20x20xf32>
@@ -81,7 +81,7 @@
     %75 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %76 = stablehlo.multiply %2, %2 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %78 = stablehlo.multiply %77, %76 : tensor<20x20xf32>
+    %78 = stablehlo.multiply %76, %77 : tensor<20x20xf32>
     %79 = stablehlo.constant dense<7.85386146E-5> : tensor<20x20xf32>
     %80 = stablehlo.add %78, %79 : tensor<20x20xf32>
     %81 = stablehlo.multiply %80, %76 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/erfc_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/erfc_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/erfc_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/erfc_shape_float16_20_20.mlir
@@ -17,7 +17,7 @@
     %11 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %12 = stablehlo.compare  LT, %5, %11 : (tensor<20x20xf32>, tensor<20x20xf32>) -> tensor<20x20xi1>
     %13 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %14 = stablehlo.multiply %13, %7 : tensor<20x20xf32>
+    %14 = stablehlo.multiply %7, %13 : tensor<20x20xf32>
     %15 = stablehlo.constant dense<2.326820e-02> : tensor<20x20xf32>
     %16 = stablehlo.add %14, %15 : tensor<20x20xf32>
     %17 = stablehlo.multiply %16, %7 : tensor<20x20xf32>
@@ -45,7 +45,7 @@
     %39 = stablehlo.constant dense<0.563825965> : tensor<20x20xf32>
     %40 = stablehlo.add %38, %39 : tensor<20x20xf32>
     %41 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %42 = stablehlo.multiply %41, %7 : tensor<20x20xf32>
+    %42 = stablehlo.multiply %7, %41 : tensor<20x20xf32>
     %43 = stablehlo.constant dense<-10.477664> : tensor<20x20xf32>
     %44 = stablehlo.add %42, %43 : tensor<20x20xf32>
     %45 = stablehlo.multiply %44, %7 : tensor<20x20xf32>
@@ -81,7 +81,7 @@
     %75 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %76 = stablehlo.multiply %2, %2 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %78 = stablehlo.multiply %77, %76 : tensor<20x20xf32>
+    %78 = stablehlo.multiply %76, %77 : tensor<20x20xf32>
     %79 = stablehlo.constant dense<7.85386146E-5> : tensor<20x20xf32>
     %80 = stablehlo.add %78, %79 : tensor<20x20xf32>
     %81 = stablehlo.multiply %80, %76 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/erfc_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/erfc_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/erfc_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/erfc_shape_float32_20_20.mlir
@@ -16,7 +16,7 @@
     %10 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %11 = stablehlo.compare  LT, %4, %10 : (tensor<20x20xf32>, tensor<20x20xf32>) -> tensor<20x20xi1>
     %12 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %13 = stablehlo.multiply %12, %6 : tensor<20x20xf32>
+    %13 = stablehlo.multiply %6, %12 : tensor<20x20xf32>
     %14 = stablehlo.constant dense<2.326820e-02> : tensor<20x20xf32>
     %15 = stablehlo.add %13, %14 : tensor<20x20xf32>
     %16 = stablehlo.multiply %15, %6 : tensor<20x20xf32>
@@ -44,7 +44,7 @@
     %38 = stablehlo.constant dense<0.563825965> : tensor<20x20xf32>
     %39 = stablehlo.add %37, %38 : tensor<20x20xf32>
     %40 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %41 = stablehlo.multiply %40, %6 : tensor<20x20xf32>
+    %41 = stablehlo.multiply %6, %40 : tensor<20x20xf32>
     %42 = stablehlo.constant dense<-10.477664> : tensor<20x20xf32>
     %43 = stablehlo.add %41, %42 : tensor<20x20xf32>
     %44 = stablehlo.multiply %43, %6 : tensor<20x20xf32>
@@ -80,7 +80,7 @@
     %74 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %75 = stablehlo.multiply %0, %0 : tensor<20x20xf32>
     %76 = stablehlo.constant dense<0.000000e+00> : tensor<20x20xf32>
-    %77 = stablehlo.multiply %76, %75 : tensor<20x20xf32>
+    %77 = stablehlo.multiply %75, %76 : tensor<20x20xf32>
     %78 = stablehlo.constant dense<7.85386146E-5> : tensor<20x20xf32>
     %79 = stablehlo.add %77, %78 : tensor<20x20xf32>
     %80 = stablehlo.multiply %79, %75 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
@@ -34,7 +34,7 @@
     %12 = stablehlo.compare  LT, %6, %11,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %13 = stablehlo.constant dense<1> : tensor<i32>
     %14 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %15 = stablehlo.add %6, %14 : tensor<1xi32>
+    %15 = stablehlo.add %14, %6 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -45,7 +45,7 @@
     %23 = stablehlo.compare  LT, %7, %22,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = stablehlo.add %7, %25 : tensor<1xi32>
+    %26 = stablehlo.add %25, %7 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
@@ -39,7 +39,7 @@
     %17 = stablehlo.compare  LT, %6, %16,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %18 = stablehlo.constant dense<3> : tensor<i32>
     %19 = stablehlo.broadcast_in_dim %18, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %20 = stablehlo.add %6, %19 : tensor<1xi32>
+    %20 = stablehlo.add %19, %6 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -55,7 +55,7 @@
     %33 = stablehlo.compare  LT, %7, %32,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %34 = stablehlo.constant dense<3> : tensor<i32>
     %35 = stablehlo.broadcast_in_dim %34, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %36 = stablehlo.add %7, %35 : tensor<1xi32>
+    %36 = stablehlo.add %35, %7 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
@@ -39,7 +39,7 @@
     %17 = stablehlo.compare  LT, %6, %16,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %18 = stablehlo.constant dense<3> : tensor<i32>
     %19 = stablehlo.broadcast_in_dim %18, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %20 = stablehlo.add %6, %19 : tensor<1xi32>
+    %20 = stablehlo.add %19, %6 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -55,7 +55,7 @@
     %33 = stablehlo.compare  LT, %7, %32,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %34 = stablehlo.constant dense<3> : tensor<i32>
     %35 = stablehlo.broadcast_in_dim %34, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %36 = stablehlo.add %7, %35 : tensor<1xi32>
+    %36 = stablehlo.add %35, %7 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
@@ -39,7 +39,7 @@
     %17 = stablehlo.compare  LT, %6, %16,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %18 = stablehlo.constant dense<3> : tensor<i32>
     %19 = stablehlo.broadcast_in_dim %18, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %20 = stablehlo.add %6, %19 : tensor<1xi32>
+    %20 = stablehlo.add %19, %6 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -55,7 +55,7 @@
     %33 = stablehlo.compare  LT, %7, %32,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %34 = stablehlo.constant dense<3> : tensor<i32>
     %35 = stablehlo.broadcast_in_dim %34, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %36 = stablehlo.add %7, %35 : tensor<1xi32>
+    %36 = stablehlo.add %35, %7 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
@@ -42,7 +42,7 @@
     %20 = stablehlo.compare  LT, %8, %19,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %21 = stablehlo.constant dense<3> : tensor<i32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %23 = stablehlo.add %8, %22 : tensor<1xi32>
+    %23 = stablehlo.add %22, %8 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -58,7 +58,7 @@
     %36 = stablehlo.compare  LT, %9, %35,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %37 = stablehlo.constant dense<3> : tensor<i32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %39 = stablehlo.add %9, %38 : tensor<1xi32>
+    %39 = stablehlo.add %38, %9 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
@@ -42,7 +42,7 @@
     %20 = stablehlo.compare  LT, %8, %19,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %21 = stablehlo.constant dense<3> : tensor<i32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %23 = stablehlo.add %8, %22 : tensor<1xi32>
+    %23 = stablehlo.add %22, %8 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -58,7 +58,7 @@
     %36 = stablehlo.compare  LT, %9, %35,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %37 = stablehlo.constant dense<3> : tensor<i32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %39 = stablehlo.add %9, %38 : tensor<1xi32>
+    %39 = stablehlo.add %38, %9 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
@@ -42,7 +42,7 @@
     %20 = stablehlo.compare  LT, %8, %19,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %21 = stablehlo.constant dense<3> : tensor<i32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %23 = stablehlo.add %8, %22 : tensor<1xi32>
+    %23 = stablehlo.add %22, %8 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -58,7 +58,7 @@
     %36 = stablehlo.compare  LT, %9, %35,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %37 = stablehlo.constant dense<3> : tensor<i32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %39 = stablehlo.add %9, %38 : tensor<1xi32>
+    %39 = stablehlo.add %38, %9 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
@@ -41,7 +41,7 @@
     %19 = stablehlo.compare  LT, %8, %18,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %20 = stablehlo.constant dense<3> : tensor<i32>
     %21 = stablehlo.broadcast_in_dim %20, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %22 = stablehlo.add %8, %21 : tensor<1xi32>
+    %22 = stablehlo.add %21, %8 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
@@ -57,7 +57,7 @@
     %35 = stablehlo.compare  LT, %9, %34,  SIGNED : (tensor<1xi32>, tensor<1xi32>) -> tensor<1xi1>
     %36 = stablehlo.constant dense<3> : tensor<i32>
     %37 = stablehlo.broadcast_in_dim %36, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %38 = stablehlo.add %9, %37 : tensor<1xi32>
+    %38 = stablehlo.add %37, %9 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
     %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir b/stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir
@@ -202,7 +202,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -272,7 +272,7 @@
     %34 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %35 = stablehlo.subtract %34, %29 : tensor<20x20xf32>
     %36 = stablehlo.select %32, %35, %29 : tensor<20x20xi1>, tensor<20x20xf32>
-    %37 = stablehlo.multiply %26, %36 : tensor<20x20xf32>
+    %37 = stablehlo.multiply %36, %26 : tensor<20x20xf32>
     %38 = stablehlo.sine %37 : tensor<20x20xf32>
     %39 = stablehlo.log %38 : tensor<20x20xf32>
     %40 = stablehlo.is_finite %39 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -290,17 +290,17 @@
     %52 = stablehlo.add %50, %51 : tensor<20x20xf32>
     %53 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %55 = stablehlo.add %54, %50 : tensor<20x20xf32>
+    %55 = stablehlo.add %50, %54 : tensor<20x20xf32>
     %56 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %57 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %58 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %59 = stablehlo.divide %50, %58 : tensor<20x20xf32>
     %60 = stablehlo.log_plus_one %59 : tensor<20x20xf32>
-    %61 = stablehlo.add %57, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %57 : tensor<20x20xf32>
     %62 = stablehlo.divide %55, %61 : tensor<20x20xf32>
     %63 = stablehlo.subtract %52, %62 : tensor<20x20xf32>
     %64 = stablehlo.multiply %63, %61 : tensor<20x20xf32>
-    %65 = stablehlo.add %45, %64 : tensor<20x20xf32>
+    %65 = stablehlo.add %64, %45 : tensor<20x20xf32>
     %66 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %67 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %68 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -311,7 +311,7 @@
     %73 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %74 = stablehlo.add %72, %73 : tensor<20x20xf32>
     %75 = stablehlo.divide %69, %74 : tensor<20x20xf32>
-    %76 = stablehlo.add %67, %75 : tensor<20x20xf32>
+    %76 = stablehlo.add %75, %67 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %78 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %79 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -585,7 +585,7 @@
       %240 = stablehlo.multiply %iterArg_4, %239 : tensor<20x20xf32>
       %241 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %242 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %243 = stablehlo.multiply %242, %iterArg_1 : tensor<20x20xf32>
+      %243 = stablehlo.multiply %iterArg_1, %242 : tensor<20x20xf32>
       %244 = stablehlo.multiply %243, %iterArg_3 : tensor<20x20xf32>
       %245 = stablehlo.multiply %227, %227 : tensor<20x20xf32>
       %246 = stablehlo.divide %244, %245 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir b/stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir
--- stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir
+++ stablehlo/stablehlo/testdata/igamma_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir
@@ -202,7 +202,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -272,7 +272,7 @@
     %34 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %35 = stablehlo.subtract %34, %29 : tensor<20x20xf32>
     %36 = stablehlo.select %32, %35, %29 : tensor<20x20xi1>, tensor<20x20xf32>
-    %37 = stablehlo.multiply %26, %36 : tensor<20x20xf32>
+    %37 = stablehlo.multiply %36, %26 : tensor<20x20xf32>
     %38 = stablehlo.sine %37 : tensor<20x20xf32>
     %39 = stablehlo.log %38 : tensor<20x20xf32>
     %40 = stablehlo.is_finite %39 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -290,17 +290,17 @@
     %52 = stablehlo.add %50, %51 : tensor<20x20xf32>
     %53 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %55 = stablehlo.add %54, %50 : tensor<20x20xf32>
+    %55 = stablehlo.add %50, %54 : tensor<20x20xf32>
     %56 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %57 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %58 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %59 = stablehlo.divide %50, %58 : tensor<20x20xf32>
     %60 = stablehlo.log_plus_one %59 : tensor<20x20xf32>
-    %61 = stablehlo.add %57, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %57 : tensor<20x20xf32>
     %62 = stablehlo.divide %55, %61 : tensor<20x20xf32>
     %63 = stablehlo.subtract %52, %62 : tensor<20x20xf32>
     %64 = stablehlo.multiply %63, %61 : tensor<20x20xf32>
-    %65 = stablehlo.add %45, %64 : tensor<20x20xf32>
+    %65 = stablehlo.add %64, %45 : tensor<20x20xf32>
     %66 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %67 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %68 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -311,7 +311,7 @@
     %73 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %74 = stablehlo.add %72, %73 : tensor<20x20xf32>
     %75 = stablehlo.divide %69, %74 : tensor<20x20xf32>
-    %76 = stablehlo.add %67, %75 : tensor<20x20xf32>
+    %76 = stablehlo.add %75, %67 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %78 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %79 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -585,7 +585,7 @@
       %240 = stablehlo.multiply %iterArg_4, %239 : tensor<20x20xf32>
       %241 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %242 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %243 = stablehlo.multiply %242, %iterArg_1 : tensor<20x20xf32>
+      %243 = stablehlo.multiply %iterArg_1, %242 : tensor<20x20xf32>
       %244 = stablehlo.multiply %243, %iterArg_3 : tensor<20x20xf32>
       %245 = stablehlo.multiply %227, %227 : tensor<20x20xf32>
       %246 = stablehlo.divide %244, %245 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igamma_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/igamma_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/igamma_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/igamma_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir
@@ -202,7 +202,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -272,7 +272,7 @@
     %34 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %35 = stablehlo.subtract %34, %29 : tensor<20x20xf32>
     %36 = stablehlo.select %32, %35, %29 : tensor<20x20xi1>, tensor<20x20xf32>
-    %37 = stablehlo.multiply %26, %36 : tensor<20x20xf32>
+    %37 = stablehlo.multiply %36, %26 : tensor<20x20xf32>
     %38 = stablehlo.sine %37 : tensor<20x20xf32>
     %39 = stablehlo.log %38 : tensor<20x20xf32>
     %40 = stablehlo.is_finite %39 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -290,17 +290,17 @@
     %52 = stablehlo.add %50, %51 : tensor<20x20xf32>
     %53 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %55 = stablehlo.add %54, %50 : tensor<20x20xf32>
+    %55 = stablehlo.add %50, %54 : tensor<20x20xf32>
     %56 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %57 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %58 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %59 = stablehlo.divide %50, %58 : tensor<20x20xf32>
     %60 = stablehlo.log_plus_one %59 : tensor<20x20xf32>
-    %61 = stablehlo.add %57, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %57 : tensor<20x20xf32>
     %62 = stablehlo.divide %55, %61 : tensor<20x20xf32>
     %63 = stablehlo.subtract %52, %62 : tensor<20x20xf32>
     %64 = stablehlo.multiply %63, %61 : tensor<20x20xf32>
-    %65 = stablehlo.add %45, %64 : tensor<20x20xf32>
+    %65 = stablehlo.add %64, %45 : tensor<20x20xf32>
     %66 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %67 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %68 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -311,7 +311,7 @@
     %73 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %74 = stablehlo.add %72, %73 : tensor<20x20xf32>
     %75 = stablehlo.divide %69, %74 : tensor<20x20xf32>
-    %76 = stablehlo.add %67, %75 : tensor<20x20xf32>
+    %76 = stablehlo.add %75, %67 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %78 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %79 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -585,7 +585,7 @@
       %241 = stablehlo.multiply %iterArg_4, %240 : tensor<20x20xf32>
       %242 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %243 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %244 = stablehlo.multiply %243, %iterArg_1 : tensor<20x20xf32>
+      %244 = stablehlo.multiply %iterArg_1, %243 : tensor<20x20xf32>
       %245 = stablehlo.multiply %244, %iterArg_3 : tensor<20x20xf32>
       %246 = stablehlo.multiply %228, %228 : tensor<20x20xf32>
       %247 = stablehlo.divide %245, %246 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir b/stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir
@@ -202,7 +202,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -272,7 +272,7 @@
     %34 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %35 = stablehlo.subtract %34, %29 : tensor<20x20xf32>
     %36 = stablehlo.select %32, %35, %29 : tensor<20x20xi1>, tensor<20x20xf32>
-    %37 = stablehlo.multiply %26, %36 : tensor<20x20xf32>
+    %37 = stablehlo.multiply %36, %26 : tensor<20x20xf32>
     %38 = stablehlo.sine %37 : tensor<20x20xf32>
     %39 = stablehlo.log %38 : tensor<20x20xf32>
     %40 = stablehlo.is_finite %39 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -290,17 +290,17 @@
     %52 = stablehlo.add %50, %51 : tensor<20x20xf32>
     %53 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %55 = stablehlo.add %54, %50 : tensor<20x20xf32>
+    %55 = stablehlo.add %50, %54 : tensor<20x20xf32>
     %56 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %57 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %58 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %59 = stablehlo.divide %50, %58 : tensor<20x20xf32>
     %60 = stablehlo.log_plus_one %59 : tensor<20x20xf32>
-    %61 = stablehlo.add %57, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %57 : tensor<20x20xf32>
     %62 = stablehlo.divide %55, %61 : tensor<20x20xf32>
     %63 = stablehlo.subtract %52, %62 : tensor<20x20xf32>
     %64 = stablehlo.multiply %63, %61 : tensor<20x20xf32>
-    %65 = stablehlo.add %45, %64 : tensor<20x20xf32>
+    %65 = stablehlo.add %64, %45 : tensor<20x20xf32>
     %66 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %67 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %68 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -311,7 +311,7 @@
     %73 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %74 = stablehlo.add %72, %73 : tensor<20x20xf32>
     %75 = stablehlo.divide %69, %74 : tensor<20x20xf32>
-    %76 = stablehlo.add %67, %75 : tensor<20x20xf32>
+    %76 = stablehlo.add %75, %67 : tensor<20x20xf32>
     %77 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %78 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %79 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -585,7 +585,7 @@
       %241 = stablehlo.multiply %iterArg_4, %240 : tensor<20x20xf32>
       %242 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %243 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %244 = stablehlo.multiply %243, %iterArg_1 : tensor<20x20xf32>
+      %244 = stablehlo.multiply %iterArg_1, %243 : tensor<20x20xf32>
       %245 = stablehlo.multiply %244, %iterArg_3 : tensor<20x20xf32>
       %246 = stablehlo.multiply %228, %228 : tensor<20x20xf32>
       %247 = stablehlo.divide %245, %246 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir b/stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/igamma_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir
@@ -202,7 +202,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -270,7 +270,7 @@
     %32 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %33 = stablehlo.subtract %32, %27 : tensor<20x20xf32>
     %34 = stablehlo.select %30, %33, %27 : tensor<20x20xi1>, tensor<20x20xf32>
-    %35 = stablehlo.multiply %24, %34 : tensor<20x20xf32>
+    %35 = stablehlo.multiply %34, %24 : tensor<20x20xf32>
     %36 = stablehlo.sine %35 : tensor<20x20xf32>
     %37 = stablehlo.log %36 : tensor<20x20xf32>
     %38 = stablehlo.is_finite %37 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -288,17 +288,17 @@
     %50 = stablehlo.add %48, %49 : tensor<20x20xf32>
     %51 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %52 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %53 = stablehlo.add %52, %48 : tensor<20x20xf32>
+    %53 = stablehlo.add %48, %52 : tensor<20x20xf32>
     %54 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %55 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %56 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %57 = stablehlo.divide %48, %56 : tensor<20x20xf32>
     %58 = stablehlo.log_plus_one %57 : tensor<20x20xf32>
-    %59 = stablehlo.add %55, %58 : tensor<20x20xf32>
+    %59 = stablehlo.add %58, %55 : tensor<20x20xf32>
     %60 = stablehlo.divide %53, %59 : tensor<20x20xf32>
     %61 = stablehlo.subtract %50, %60 : tensor<20x20xf32>
     %62 = stablehlo.multiply %61, %59 : tensor<20x20xf32>
-    %63 = stablehlo.add %43, %62 : tensor<20x20xf32>
+    %63 = stablehlo.add %62, %43 : tensor<20x20xf32>
     %64 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %65 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %66 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -309,7 +309,7 @@
     %71 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %72 = stablehlo.add %70, %71 : tensor<20x20xf32>
     %73 = stablehlo.divide %67, %72 : tensor<20x20xf32>
-    %74 = stablehlo.add %65, %73 : tensor<20x20xf32>
+    %74 = stablehlo.add %73, %65 : tensor<20x20xf32>
     %75 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %76 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %77 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -583,7 +583,7 @@
       %238 = stablehlo.multiply %iterArg_4, %237 : tensor<20x20xf32>
       %239 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %240 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %241 = stablehlo.multiply %240, %iterArg_1 : tensor<20x20xf32>
+      %241 = stablehlo.multiply %iterArg_1, %240 : tensor<20x20xf32>
       %242 = stablehlo.multiply %241, %iterArg_3 : tensor<20x20xf32>
       %243 = stablehlo.multiply %225, %225 : tensor<20x20xf32>
       %244 = stablehlo.divide %242, %243 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir b/stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_1_20__rhs_float32_20_20.mlir
@@ -47,7 +47,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -268,7 +268,7 @@
     %30 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %31 = stablehlo.subtract %30, %25 : tensor<20x20xf32>
     %32 = stablehlo.select %28, %31, %25 : tensor<20x20xi1>, tensor<20x20xf32>
-    %33 = stablehlo.multiply %22, %32 : tensor<20x20xf32>
+    %33 = stablehlo.multiply %32, %22 : tensor<20x20xf32>
     %34 = stablehlo.sine %33 : tensor<20x20xf32>
     %35 = stablehlo.log %34 : tensor<20x20xf32>
     %36 = stablehlo.is_finite %35 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -286,17 +286,17 @@
     %48 = stablehlo.add %46, %47 : tensor<20x20xf32>
     %49 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %50 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %51 = stablehlo.add %50, %46 : tensor<20x20xf32>
+    %51 = stablehlo.add %46, %50 : tensor<20x20xf32>
     %52 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %53 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %55 = stablehlo.divide %46, %54 : tensor<20x20xf32>
     %56 = stablehlo.log_plus_one %55 : tensor<20x20xf32>
-    %57 = stablehlo.add %53, %56 : tensor<20x20xf32>
+    %57 = stablehlo.add %56, %53 : tensor<20x20xf32>
     %58 = stablehlo.divide %51, %57 : tensor<20x20xf32>
     %59 = stablehlo.subtract %48, %58 : tensor<20x20xf32>
     %60 = stablehlo.multiply %59, %57 : tensor<20x20xf32>
-    %61 = stablehlo.add %41, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %41 : tensor<20x20xf32>
     %62 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %63 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %64 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -307,7 +307,7 @@
     %69 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %70 = stablehlo.add %68, %69 : tensor<20x20xf32>
     %71 = stablehlo.divide %65, %70 : tensor<20x20xf32>
-    %72 = stablehlo.add %63, %71 : tensor<20x20xf32>
+    %72 = stablehlo.add %71, %63 : tensor<20x20xf32>
     %73 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %74 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %75 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -428,7 +428,7 @@
       %228 = stablehlo.multiply %iterArg_4, %227 : tensor<20x20xf32>
       %229 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %230 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %231 = stablehlo.multiply %230, %iterArg_1 : tensor<20x20xf32>
+      %231 = stablehlo.multiply %iterArg_1, %230 : tensor<20x20xf32>
       %232 = stablehlo.multiply %231, %iterArg_3 : tensor<20x20xf32>
       %233 = stablehlo.multiply %215, %215 : tensor<20x20xf32>
       %234 = stablehlo.divide %232, %233 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir b/stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir
--- stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir
+++ stablehlo/stablehlo/testdata/igammac_broadcasting_lhs_float32_20_20__rhs_float32_1_20.mlir
@@ -47,7 +47,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -268,7 +268,7 @@
     %30 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %31 = stablehlo.subtract %30, %25 : tensor<20x20xf32>
     %32 = stablehlo.select %28, %31, %25 : tensor<20x20xi1>, tensor<20x20xf32>
-    %33 = stablehlo.multiply %22, %32 : tensor<20x20xf32>
+    %33 = stablehlo.multiply %32, %22 : tensor<20x20xf32>
     %34 = stablehlo.sine %33 : tensor<20x20xf32>
     %35 = stablehlo.log %34 : tensor<20x20xf32>
     %36 = stablehlo.is_finite %35 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -286,17 +286,17 @@
     %48 = stablehlo.add %46, %47 : tensor<20x20xf32>
     %49 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %50 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %51 = stablehlo.add %50, %46 : tensor<20x20xf32>
+    %51 = stablehlo.add %46, %50 : tensor<20x20xf32>
     %52 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %53 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %55 = stablehlo.divide %46, %54 : tensor<20x20xf32>
     %56 = stablehlo.log_plus_one %55 : tensor<20x20xf32>
-    %57 = stablehlo.add %53, %56 : tensor<20x20xf32>
+    %57 = stablehlo.add %56, %53 : tensor<20x20xf32>
     %58 = stablehlo.divide %51, %57 : tensor<20x20xf32>
     %59 = stablehlo.subtract %48, %58 : tensor<20x20xf32>
     %60 = stablehlo.multiply %59, %57 : tensor<20x20xf32>
-    %61 = stablehlo.add %41, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %41 : tensor<20x20xf32>
     %62 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %63 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %64 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -307,7 +307,7 @@
     %69 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %70 = stablehlo.add %68, %69 : tensor<20x20xf32>
     %71 = stablehlo.divide %65, %70 : tensor<20x20xf32>
-    %72 = stablehlo.add %63, %71 : tensor<20x20xf32>
+    %72 = stablehlo.add %71, %63 : tensor<20x20xf32>
     %73 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %74 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %75 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -428,7 +428,7 @@
       %228 = stablehlo.multiply %iterArg_4, %227 : tensor<20x20xf32>
       %229 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %230 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %231 = stablehlo.multiply %230, %iterArg_1 : tensor<20x20xf32>
+      %231 = stablehlo.multiply %iterArg_1, %230 : tensor<20x20xf32>
       %232 = stablehlo.multiply %231, %iterArg_3 : tensor<20x20xf32>
       %233 = stablehlo.multiply %215, %215 : tensor<20x20xf32>
       %234 = stablehlo.divide %232, %233 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igammac_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/igammac_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/igammac_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/igammac_dtypes_lhs_bfloat16_20_20__rhs_bfloat16_20_20.mlir
@@ -47,7 +47,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -268,7 +268,7 @@
     %30 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %31 = stablehlo.subtract %30, %25 : tensor<20x20xf32>
     %32 = stablehlo.select %28, %31, %25 : tensor<20x20xi1>, tensor<20x20xf32>
-    %33 = stablehlo.multiply %22, %32 : tensor<20x20xf32>
+    %33 = stablehlo.multiply %32, %22 : tensor<20x20xf32>
     %34 = stablehlo.sine %33 : tensor<20x20xf32>
     %35 = stablehlo.log %34 : tensor<20x20xf32>
     %36 = stablehlo.is_finite %35 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -286,17 +286,17 @@
     %48 = stablehlo.add %46, %47 : tensor<20x20xf32>
     %49 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %50 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %51 = stablehlo.add %50, %46 : tensor<20x20xf32>
+    %51 = stablehlo.add %46, %50 : tensor<20x20xf32>
     %52 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %53 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %55 = stablehlo.divide %46, %54 : tensor<20x20xf32>
     %56 = stablehlo.log_plus_one %55 : tensor<20x20xf32>
-    %57 = stablehlo.add %53, %56 : tensor<20x20xf32>
+    %57 = stablehlo.add %56, %53 : tensor<20x20xf32>
     %58 = stablehlo.divide %51, %57 : tensor<20x20xf32>
     %59 = stablehlo.subtract %48, %58 : tensor<20x20xf32>
     %60 = stablehlo.multiply %59, %57 : tensor<20x20xf32>
-    %61 = stablehlo.add %41, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %41 : tensor<20x20xf32>
     %62 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %63 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %64 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -307,7 +307,7 @@
     %69 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %70 = stablehlo.add %68, %69 : tensor<20x20xf32>
     %71 = stablehlo.divide %65, %70 : tensor<20x20xf32>
-    %72 = stablehlo.add %63, %71 : tensor<20x20xf32>
+    %72 = stablehlo.add %71, %63 : tensor<20x20xf32>
     %73 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %74 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %75 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -428,7 +428,7 @@
       %229 = stablehlo.multiply %iterArg_4, %228 : tensor<20x20xf32>
       %230 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %231 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %232 = stablehlo.multiply %231, %iterArg_1 : tensor<20x20xf32>
+      %232 = stablehlo.multiply %iterArg_1, %231 : tensor<20x20xf32>
       %233 = stablehlo.multiply %232, %iterArg_3 : tensor<20x20xf32>
       %234 = stablehlo.multiply %216, %216 : tensor<20x20xf32>
       %235 = stablehlo.divide %233, %234 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir b/stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float16_20_20__rhs_float16_20_20.mlir
@@ -47,7 +47,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -268,7 +268,7 @@
     %30 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %31 = stablehlo.subtract %30, %25 : tensor<20x20xf32>
     %32 = stablehlo.select %28, %31, %25 : tensor<20x20xi1>, tensor<20x20xf32>
-    %33 = stablehlo.multiply %22, %32 : tensor<20x20xf32>
+    %33 = stablehlo.multiply %32, %22 : tensor<20x20xf32>
     %34 = stablehlo.sine %33 : tensor<20x20xf32>
     %35 = stablehlo.log %34 : tensor<20x20xf32>
     %36 = stablehlo.is_finite %35 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -286,17 +286,17 @@
     %48 = stablehlo.add %46, %47 : tensor<20x20xf32>
     %49 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %50 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %51 = stablehlo.add %50, %46 : tensor<20x20xf32>
+    %51 = stablehlo.add %46, %50 : tensor<20x20xf32>
     %52 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %53 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %54 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %55 = stablehlo.divide %46, %54 : tensor<20x20xf32>
     %56 = stablehlo.log_plus_one %55 : tensor<20x20xf32>
-    %57 = stablehlo.add %53, %56 : tensor<20x20xf32>
+    %57 = stablehlo.add %56, %53 : tensor<20x20xf32>
     %58 = stablehlo.divide %51, %57 : tensor<20x20xf32>
     %59 = stablehlo.subtract %48, %58 : tensor<20x20xf32>
     %60 = stablehlo.multiply %59, %57 : tensor<20x20xf32>
-    %61 = stablehlo.add %41, %60 : tensor<20x20xf32>
+    %61 = stablehlo.add %60, %41 : tensor<20x20xf32>
     %62 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %63 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %64 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -307,7 +307,7 @@
     %69 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %70 = stablehlo.add %68, %69 : tensor<20x20xf32>
     %71 = stablehlo.divide %65, %70 : tensor<20x20xf32>
-    %72 = stablehlo.add %63, %71 : tensor<20x20xf32>
+    %72 = stablehlo.add %71, %63 : tensor<20x20xf32>
     %73 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %74 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %75 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -428,7 +428,7 @@
       %229 = stablehlo.multiply %iterArg_4, %228 : tensor<20x20xf32>
       %230 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %231 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %232 = stablehlo.multiply %231, %iterArg_1 : tensor<20x20xf32>
+      %232 = stablehlo.multiply %iterArg_1, %231 : tensor<20x20xf32>
       %233 = stablehlo.multiply %232, %iterArg_3 : tensor<20x20xf32>
       %234 = stablehlo.multiply %216, %216 : tensor<20x20xf32>
       %235 = stablehlo.divide %233, %234 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir b/stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/igammac_dtypes_lhs_float32_20_20__rhs_float32_20_20.mlir
@@ -47,7 +47,7 @@
     %21 = stablehlo.multiply %19, %20 : tensor<20x20xf32>
     %22 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
     %23 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-    %24 = stablehlo.multiply %23, %1 : tensor<20x20xf32>
+    %24 = stablehlo.multiply %1, %23 : tensor<20x20xf32>
     %25 = stablehlo.multiply %24, %2 : tensor<20x20xf32>
     %26 = stablehlo.multiply %6, %6 : tensor<20x20xf32>
     %27 = stablehlo.divide %25, %26 : tensor<20x20xf32>
@@ -266,7 +266,7 @@
     %28 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %29 = stablehlo.subtract %28, %23 : tensor<20x20xf32>
     %30 = stablehlo.select %26, %29, %23 : tensor<20x20xi1>, tensor<20x20xf32>
-    %31 = stablehlo.multiply %20, %30 : tensor<20x20xf32>
+    %31 = stablehlo.multiply %30, %20 : tensor<20x20xf32>
     %32 = stablehlo.sine %31 : tensor<20x20xf32>
     %33 = stablehlo.log %32 : tensor<20x20xf32>
     %34 = stablehlo.is_finite %33 : (tensor<20x20xf32>) -> tensor<20x20xi1>
@@ -284,17 +284,17 @@
     %46 = stablehlo.add %44, %45 : tensor<20x20xf32>
     %47 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %48 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %49 = stablehlo.add %48, %44 : tensor<20x20xf32>
+    %49 = stablehlo.add %44, %48 : tensor<20x20xf32>
     %50 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %51 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %52 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
     %53 = stablehlo.divide %44, %52 : tensor<20x20xf32>
     %54 = stablehlo.log_plus_one %53 : tensor<20x20xf32>
-    %55 = stablehlo.add %51, %54 : tensor<20x20xf32>
+    %55 = stablehlo.add %54, %51 : tensor<20x20xf32>
     %56 = stablehlo.divide %49, %55 : tensor<20x20xf32>
     %57 = stablehlo.subtract %46, %56 : tensor<20x20xf32>
     %58 = stablehlo.multiply %57, %55 : tensor<20x20xf32>
-    %59 = stablehlo.add %39, %58 : tensor<20x20xf32>
+    %59 = stablehlo.add %58, %39 : tensor<20x20xf32>
     %60 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %61 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %62 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -305,7 +305,7 @@
     %67 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %68 = stablehlo.add %66, %67 : tensor<20x20xf32>
     %69 = stablehlo.divide %63, %68 : tensor<20x20xf32>
-    %70 = stablehlo.add %61, %69 : tensor<20x20xf32>
+    %70 = stablehlo.add %69, %61 : tensor<20x20xf32>
     %71 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %72 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %73 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -426,7 +426,7 @@
       %226 = stablehlo.multiply %iterArg_4, %225 : tensor<20x20xf32>
       %227 = stablehlo.constant dense<-1.000000e+00> : tensor<f32>
       %228 = stablehlo.constant dense<-1.000000e+00> : tensor<20x20xf32>
-      %229 = stablehlo.multiply %228, %iterArg_1 : tensor<20x20xf32>
+      %229 = stablehlo.multiply %iterArg_1, %228 : tensor<20x20xf32>
       %230 = stablehlo.multiply %229, %iterArg_3 : tensor<20x20xf32>
       %231 = stablehlo.multiply %213, %213 : tensor<20x20xf32>
       %232 = stablehlo.divide %230, %231 : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/index_in_dim_0_dynamic.mlir b/stablehlo/stablehlo/testdata/index_in_dim_0_dynamic.mlir
--- stablehlo/stablehlo/testdata/index_in_dim_0_dynamic.mlir
+++ stablehlo/stablehlo/testdata/index_in_dim_0_dynamic.mlir
@@ -3,7 +3,7 @@
 module @jit_fun_flat_jax {
   func.func public @main(%arg0: tensor<i64>, %arg1: tensor<?x4xf32> {mhlo.sharding = ""}) -> tensor<4xf32> {
     %0 = stablehlo.constant dense<-1> : tensor<i64>
-    %1 = stablehlo.add %0, %arg0 : tensor<i64>
+    %1 = stablehlo.add %arg0, %0 : tensor<i64>
     %2 = stablehlo.convert %1 : (tensor<i64>) -> tensor<i32>
     %3 = stablehlo.reshape %2 : (tensor<i32>) -> tensor<1xi32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/index_in_dim_idx_neg_dynamic.mlir b/stablehlo/stablehlo/testdata/index_in_dim_idx_neg_dynamic.mlir
--- stablehlo/stablehlo/testdata/index_in_dim_idx_neg_dynamic.mlir
+++ stablehlo/stablehlo/testdata/index_in_dim_idx_neg_dynamic.mlir
@@ -3,7 +3,7 @@
 module @jit_fun_flat_jax {
   func.func public @main(%arg0: tensor<i64>, %arg1: tensor<?x4xf32> {mhlo.sharding = ""}) -> tensor<4xf32> {
     %0 = stablehlo.constant dense<-1> : tensor<i64>
-    %1 = stablehlo.add %0, %arg0 : tensor<i64>
+    %1 = stablehlo.add %arg0, %0 : tensor<i64>
     %2 = stablehlo.convert %1 : (tensor<i64>) -> tensor<i32>
     %3 = stablehlo.reshape %2 : (tensor<i32>) -> tensor<1xi32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/lgamma_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/lgamma_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/lgamma_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/lgamma_shape_bfloat16_20_20.mlir
@@ -17,7 +17,7 @@
     %11 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %12 = stablehlo.add %8, %11 : tensor<20x20xf32>
     %13 = stablehlo.divide %10, %12 : tensor<20x20xf32>
-    %14 = stablehlo.add %9, %13 : tensor<20x20xf32>
+    %14 = stablehlo.add %13, %9 : tensor<20x20xf32>
     %15 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %16 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %17 = stablehlo.add %8, %16 : tensor<20x20xf32>
@@ -54,18 +54,18 @@
     %48 = stablehlo.divide %45, %47 : tensor<20x20xf32>
     %49 = stablehlo.add %44, %48 : tensor<20x20xf32>
     %50 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %51 = stablehlo.add %50, %8 : tensor<20x20xf32>
+    %51 = stablehlo.add %8, %50 : tensor<20x20xf32>
     %52 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %53 = stablehlo.divide %8, %50 : tensor<20x20xf32>
     %54 = stablehlo.log_plus_one %53 : tensor<20x20xf32>
-    %55 = stablehlo.add %52, %54 : tensor<20x20xf32>
+    %55 = stablehlo.add %54, %52 : tensor<20x20xf32>
     %56 = stablehlo.divide %51, %55 : tensor<20x20xf32>
     %57 = stablehlo.add %8, %3 : tensor<20x20xf32>
     %58 = stablehlo.subtract %57, %56 : tensor<20x20xf32>
     %59 = stablehlo.multiply %58, %55 : tensor<20x20xf32>
     %60 = stablehlo.log %49 : tensor<20x20xf32>
     %61 = stablehlo.constant dense<0.918938517> : tensor<20x20xf32>
-    %62 = stablehlo.add %61, %59 : tensor<20x20xf32>
+    %62 = stablehlo.add %59, %61 : tensor<20x20xf32>
     %63 = stablehlo.add %62, %60 : tensor<20x20xf32>
     %64 = stablehlo.abs %2 : tensor<20x20xf32>
     %65 = stablehlo.floor %64 : tensor<20x20xf32>
@@ -74,7 +74,7 @@
     %68 = stablehlo.subtract %6, %66 : tensor<20x20xf32>
     %69 = stablehlo.select %67, %68, %66 : tensor<20x20xi1>, tensor<20x20xf32>
     %70 = stablehlo.constant dense<3.14159274> : tensor<20x20xf32>
-    %71 = stablehlo.multiply %70, %69 : tensor<20x20xf32>
+    %71 = stablehlo.multiply %69, %70 : tensor<20x20xf32>
     %72 = stablehlo.sine %71 : tensor<20x20xf32>
     %73 = stablehlo.log %72 : tensor<20x20xf32>
     %74 = stablehlo.constant dense<1.14472985> : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/lgamma_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/lgamma_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/lgamma_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/lgamma_shape_float16_20_20.mlir
@@ -17,7 +17,7 @@
     %11 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %12 = stablehlo.add %8, %11 : tensor<20x20xf32>
     %13 = stablehlo.divide %10, %12 : tensor<20x20xf32>
-    %14 = stablehlo.add %9, %13 : tensor<20x20xf32>
+    %14 = stablehlo.add %13, %9 : tensor<20x20xf32>
     %15 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %16 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %17 = stablehlo.add %8, %16 : tensor<20x20xf32>
@@ -54,18 +54,18 @@
     %48 = stablehlo.divide %45, %47 : tensor<20x20xf32>
     %49 = stablehlo.add %44, %48 : tensor<20x20xf32>
     %50 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %51 = stablehlo.add %50, %8 : tensor<20x20xf32>
+    %51 = stablehlo.add %8, %50 : tensor<20x20xf32>
     %52 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %53 = stablehlo.divide %8, %50 : tensor<20x20xf32>
     %54 = stablehlo.log_plus_one %53 : tensor<20x20xf32>
-    %55 = stablehlo.add %52, %54 : tensor<20x20xf32>
+    %55 = stablehlo.add %54, %52 : tensor<20x20xf32>
     %56 = stablehlo.divide %51, %55 : tensor<20x20xf32>
     %57 = stablehlo.add %8, %3 : tensor<20x20xf32>
     %58 = stablehlo.subtract %57, %56 : tensor<20x20xf32>
     %59 = stablehlo.multiply %58, %55 : tensor<20x20xf32>
     %60 = stablehlo.log %49 : tensor<20x20xf32>
     %61 = stablehlo.constant dense<0.918938517> : tensor<20x20xf32>
-    %62 = stablehlo.add %61, %59 : tensor<20x20xf32>
+    %62 = stablehlo.add %59, %61 : tensor<20x20xf32>
     %63 = stablehlo.add %62, %60 : tensor<20x20xf32>
     %64 = stablehlo.abs %2 : tensor<20x20xf32>
     %65 = stablehlo.floor %64 : tensor<20x20xf32>
@@ -74,7 +74,7 @@
     %68 = stablehlo.subtract %6, %66 : tensor<20x20xf32>
     %69 = stablehlo.select %67, %68, %66 : tensor<20x20xi1>, tensor<20x20xf32>
     %70 = stablehlo.constant dense<3.14159274> : tensor<20x20xf32>
-    %71 = stablehlo.multiply %70, %69 : tensor<20x20xf32>
+    %71 = stablehlo.multiply %69, %70 : tensor<20x20xf32>
     %72 = stablehlo.sine %71 : tensor<20x20xf32>
     %73 = stablehlo.log %72 : tensor<20x20xf32>
     %74 = stablehlo.constant dense<1.14472985> : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/lgamma_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/lgamma_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/lgamma_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/lgamma_shape_float32_20_20.mlir
@@ -16,7 +16,7 @@
     %10 = stablehlo.constant dense<1.000000e+00> : tensor<20x20xf32>
     %11 = stablehlo.add %7, %10 : tensor<20x20xf32>
     %12 = stablehlo.divide %9, %11 : tensor<20x20xf32>
-    %13 = stablehlo.add %8, %12 : tensor<20x20xf32>
+    %13 = stablehlo.add %12, %8 : tensor<20x20xf32>
     %14 = stablehlo.constant dense<-1259.13916> : tensor<20x20xf32>
     %15 = stablehlo.constant dense<2.000000e+00> : tensor<20x20xf32>
     %16 = stablehlo.add %7, %15 : tensor<20x20xf32>
@@ -53,18 +53,18 @@
     %47 = stablehlo.divide %44, %46 : tensor<20x20xf32>
     %48 = stablehlo.add %43, %47 : tensor<20x20xf32>
     %49 = stablehlo.constant dense<7.500000e+00> : tensor<20x20xf32>
-    %50 = stablehlo.add %49, %7 : tensor<20x20xf32>
+    %50 = stablehlo.add %7, %49 : tensor<20x20xf32>
     %51 = stablehlo.constant dense<2.01490307> : tensor<20x20xf32>
     %52 = stablehlo.divide %7, %49 : tensor<20x20xf32>
     %53 = stablehlo.log_plus_one %52 : tensor<20x20xf32>
-    %54 = stablehlo.add %51, %53 : tensor<20x20xf32>
+    %54 = stablehlo.add %53, %51 : tensor<20x20xf32>
     %55 = stablehlo.divide %50, %54 : tensor<20x20xf32>
     %56 = stablehlo.add %7, %2 : tensor<20x20xf32>
     %57 = stablehlo.subtract %56, %55 : tensor<20x20xf32>
     %58 = stablehlo.multiply %57, %54 : tensor<20x20xf32>
     %59 = stablehlo.log %48 : tensor<20x20xf32>
     %60 = stablehlo.constant dense<0.918938517> : tensor<20x20xf32>
-    %61 = stablehlo.add %60, %58 : tensor<20x20xf32>
+    %61 = stablehlo.add %58, %60 : tensor<20x20xf32>
     %62 = stablehlo.add %61, %59 : tensor<20x20xf32>
     %63 = stablehlo.abs %0 : tensor<20x20xf32>
     %64 = stablehlo.floor %63 : tensor<20x20xf32>
@@ -73,7 +73,7 @@
     %67 = stablehlo.subtract %5, %65 : tensor<20x20xf32>
     %68 = stablehlo.select %66, %67, %65 : tensor<20x20xi1>, tensor<20x20xf32>
     %69 = stablehlo.constant dense<3.14159274> : tensor<20x20xf32>
-    %70 = stablehlo.multiply %69, %68 : tensor<20x20xf32>
+    %70 = stablehlo.multiply %68, %69 : tensor<20x20xf32>
     %71 = stablehlo.sine %70 : tensor<20x20xf32>
     %72 = stablehlo.log %71 : tensor<20x20xf32>
     %73 = stablehlo.constant dense<1.14472985> : tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir b/stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
--- stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
+++ stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
@@ -72,12 +72,12 @@
     %24 = stablehlo.subtract %14, %23 : tensor<f64>
     %25 = stablehlo.minimum %18, %24 : tensor<f64>
     %26 = stablehlo.constant dense<0.000000e+00> : tensor<f64>
-    %27 = stablehlo.maximum %26, %25 : tensor<f64>
+    %27 = stablehlo.maximum %25, %26 : tensor<f64>
     %28 = stablehlo.constant dense<1.000000e+00> : tensor<f64>
     %29 = stablehlo.subtract %14, %28 : tensor<f64>
     %30 = stablehlo.minimum %19, %29 : tensor<f64>
     %31 = stablehlo.constant dense<0.000000e+00> : tensor<f64>
-    %32 = stablehlo.maximum %31, %30 : tensor<f64>
+    %32 = stablehlo.maximum %30, %31 : tensor<f64>
     %33 = stablehlo.convert %27 : (tensor<f64>) -> tensor<i64>
     %34 = stablehlo.convert %32 : (tensor<f64>) -> tensor<i64>
     %35 = stablehlo.constant dense<5> : tensor<i64>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
@@ -338,7 +338,7 @@
       %122 = stablehlo.add %120, %121 : tensor<f32>
       %123 = stablehlo.reshape %122 : (tensor<f32>) -> tensor<f32>
       %124 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-      %125 = stablehlo.maximum %124, %123 : tensor<f32>
+      %125 = stablehlo.maximum %123, %124 : tensor<f32>
       %126 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
       %127 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
       %128 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
@@ -346,7 +346,7 @@
        cond {
         %151 = stablehlo.multiply %iterArg_6, %iterArg_6 : tensor<f32>
         %152 = stablehlo.constant dense<3.310000e-02> : tensor<f32>
-        %153 = stablehlo.multiply %152, %151 : tensor<f32>
+        %153 = stablehlo.multiply %151, %152 : tensor<f32>
         %154 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
         %155 = stablehlo.subtract %154, %153 : tensor<f32>
         %156 = stablehlo.compare  GE, %iterArg_8, %155,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
@@ -656,13 +656,13 @@
           %289 = stablehlo.add %287, %288 : tensor<f32>
           %290 = stablehlo.reshape %289 : (tensor<f32>) -> tensor<f32>
           %291 = stablehlo.constant dense<-0.99999994> : tensor<f32>
-          %292 = stablehlo.maximum %291, %290 : tensor<f32>
+          %292 = stablehlo.maximum %290, %291 : tensor<f32>
           %293 = func.call @erf_inv(%292) : (tensor<f32>) -> tensor<f32>
           %294 = stablehlo.constant dense<1.41421354> : tensor<f32>
-          %295 = stablehlo.multiply %294, %293 : tensor<f32>
+          %295 = stablehlo.multiply %293, %294 : tensor<f32>
           %296 = stablehlo.multiply %295, %iterArg_9 : tensor<f32>
           %297 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
-          %298 = stablehlo.add %297, %296 : tensor<f32>
+          %298 = stablehlo.add %296, %297 : tensor<f32>
           stablehlo.return %iterArg_9, %248, %295, %298 : tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>
         }
         %181 = stablehlo.multiply %180#2, %180#2 : tensor<f32>
@@ -773,7 +773,7 @@
         %222 = stablehlo.add %220, %221 : tensor<f32>
         %223 = stablehlo.reshape %222 : (tensor<f32>) -> tensor<f32>
         %224 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-        %225 = stablehlo.maximum %224, %223 : tensor<f32>
+        %225 = stablehlo.maximum %223, %224 : tensor<f32>
         stablehlo.return %iterArg_3, %iterArg_4, %173, %181, %183, %225 : tensor<f32>, tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>, tensor<f32>
       }
       %130 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
@@ -341,7 +341,7 @@
       %122 = stablehlo.add %120, %121 : tensor<f32>
       %123 = stablehlo.reshape %122 : (tensor<f32>) -> tensor<f32>
       %124 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-      %125 = stablehlo.maximum %124, %123 : tensor<f32>
+      %125 = stablehlo.maximum %123, %124 : tensor<f32>
       %126 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
       %127 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
       %128 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
@@ -349,7 +349,7 @@
        cond {
         %151 = stablehlo.multiply %iterArg_6, %iterArg_6 : tensor<f32>
         %152 = stablehlo.constant dense<3.310000e-02> : tensor<f32>
-        %153 = stablehlo.multiply %152, %151 : tensor<f32>
+        %153 = stablehlo.multiply %151, %152 : tensor<f32>
         %154 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
         %155 = stablehlo.subtract %154, %153 : tensor<f32>
         %156 = stablehlo.compare  GE, %iterArg_8, %155,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
@@ -659,13 +659,13 @@
           %289 = stablehlo.add %287, %288 : tensor<f32>
           %290 = stablehlo.reshape %289 : (tensor<f32>) -> tensor<f32>
           %291 = stablehlo.constant dense<-0.99999994> : tensor<f32>
-          %292 = stablehlo.maximum %291, %290 : tensor<f32>
+          %292 = stablehlo.maximum %290, %291 : tensor<f32>
           %293 = func.call @erf_inv(%292) : (tensor<f32>) -> tensor<f32>
           %294 = stablehlo.constant dense<1.41421354> : tensor<f32>
-          %295 = stablehlo.multiply %294, %293 : tensor<f32>
+          %295 = stablehlo.multiply %293, %294 : tensor<f32>
           %296 = stablehlo.multiply %295, %iterArg_9 : tensor<f32>
           %297 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
-          %298 = stablehlo.add %297, %296 : tensor<f32>
+          %298 = stablehlo.add %296, %297 : tensor<f32>
           stablehlo.return %iterArg_9, %248, %295, %298 : tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>
         }
         %181 = stablehlo.multiply %180#2, %180#2 : tensor<f32>
@@ -776,7 +776,7 @@
         %222 = stablehlo.add %220, %221 : tensor<f32>
         %223 = stablehlo.reshape %222 : (tensor<f32>) -> tensor<f32>
         %224 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-        %225 = stablehlo.maximum %224, %223 : tensor<f32>
+        %225 = stablehlo.maximum %223, %224 : tensor<f32>
         stablehlo.return %iterArg_3, %iterArg_4, %173, %181, %183, %225 : tensor<f32>, tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>, tensor<f32>
       }
       %130 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
@@ -338,7 +338,7 @@
       %122 = stablehlo.add %120, %121 : tensor<f32>
       %123 = stablehlo.reshape %122 : (tensor<f32>) -> tensor<f32>
       %124 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-      %125 = stablehlo.maximum %124, %123 : tensor<f32>
+      %125 = stablehlo.maximum %123, %124 : tensor<f32>
       %126 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
       %127 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
       %128 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
@@ -346,7 +346,7 @@
        cond {
         %151 = stablehlo.multiply %iterArg_6, %iterArg_6 : tensor<f32>
         %152 = stablehlo.constant dense<3.310000e-02> : tensor<f32>
-        %153 = stablehlo.multiply %152, %151 : tensor<f32>
+        %153 = stablehlo.multiply %151, %152 : tensor<f32>
         %154 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
         %155 = stablehlo.subtract %154, %153 : tensor<f32>
         %156 = stablehlo.compare  GE, %iterArg_8, %155,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
@@ -656,13 +656,13 @@
           %289 = stablehlo.add %287, %288 : tensor<f32>
           %290 = stablehlo.reshape %289 : (tensor<f32>) -> tensor<f32>
           %291 = stablehlo.constant dense<-0.99999994> : tensor<f32>
-          %292 = stablehlo.maximum %291, %290 : tensor<f32>
+          %292 = stablehlo.maximum %290, %291 : tensor<f32>
           %293 = func.call @erf_inv(%292) : (tensor<f32>) -> tensor<f32>
           %294 = stablehlo.constant dense<1.41421354> : tensor<f32>
-          %295 = stablehlo.multiply %294, %293 : tensor<f32>
+          %295 = stablehlo.multiply %293, %294 : tensor<f32>
           %296 = stablehlo.multiply %295, %iterArg_9 : tensor<f32>
           %297 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
-          %298 = stablehlo.add %297, %296 : tensor<f32>
+          %298 = stablehlo.add %296, %297 : tensor<f32>
           stablehlo.return %iterArg_9, %248, %295, %298 : tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>
         }
         %181 = stablehlo.multiply %180#2, %180#2 : tensor<f32>
@@ -773,7 +773,7 @@
         %222 = stablehlo.add %220, %221 : tensor<f32>
         %223 = stablehlo.reshape %222 : (tensor<f32>) -> tensor<f32>
         %224 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-        %225 = stablehlo.maximum %224, %223 : tensor<f32>
+        %225 = stablehlo.maximum %223, %224 : tensor<f32>
         stablehlo.return %iterArg_3, %iterArg_4, %173, %181, %183, %225 : tensor<f32>, tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>, tensor<f32>
       }
       %130 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
@@ -341,7 +341,7 @@
       %122 = stablehlo.add %120, %121 : tensor<f32>
       %123 = stablehlo.reshape %122 : (tensor<f32>) -> tensor<f32>
       %124 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-      %125 = stablehlo.maximum %124, %123 : tensor<f32>
+      %125 = stablehlo.maximum %123, %124 : tensor<f32>
       %126 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
       %127 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
       %128 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
@@ -349,7 +349,7 @@
        cond {
         %151 = stablehlo.multiply %iterArg_6, %iterArg_6 : tensor<f32>
         %152 = stablehlo.constant dense<3.310000e-02> : tensor<f32>
-        %153 = stablehlo.multiply %152, %151 : tensor<f32>
+        %153 = stablehlo.multiply %151, %152 : tensor<f32>
         %154 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
         %155 = stablehlo.subtract %154, %153 : tensor<f32>
         %156 = stablehlo.compare  GE, %iterArg_8, %155,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
@@ -659,13 +659,13 @@
           %289 = stablehlo.add %287, %288 : tensor<f32>
           %290 = stablehlo.reshape %289 : (tensor<f32>) -> tensor<f32>
           %291 = stablehlo.constant dense<-0.99999994> : tensor<f32>
-          %292 = stablehlo.maximum %291, %290 : tensor<f32>
+          %292 = stablehlo.maximum %290, %291 : tensor<f32>
           %293 = func.call @erf_inv(%292) : (tensor<f32>) -> tensor<f32>
           %294 = stablehlo.constant dense<1.41421354> : tensor<f32>
-          %295 = stablehlo.multiply %294, %293 : tensor<f32>
+          %295 = stablehlo.multiply %293, %294 : tensor<f32>
           %296 = stablehlo.multiply %295, %iterArg_9 : tensor<f32>
           %297 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
-          %298 = stablehlo.add %297, %296 : tensor<f32>
+          %298 = stablehlo.add %296, %297 : tensor<f32>
           stablehlo.return %iterArg_9, %248, %295, %298 : tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>
         }
         %181 = stablehlo.multiply %180#2, %180#2 : tensor<f32>
@@ -776,7 +776,7 @@
         %222 = stablehlo.add %220, %221 : tensor<f32>
         %223 = stablehlo.reshape %222 : (tensor<f32>) -> tensor<f32>
         %224 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-        %225 = stablehlo.maximum %224, %223 : tensor<f32>
+        %225 = stablehlo.maximum %223, %224 : tensor<f32>
         stablehlo.return %iterArg_3, %iterArg_4, %173, %181, %183, %225 : tensor<f32>, tensor<f32>, tensor<2xui32>, tensor<f32>, tensor<f32>, tensor<f32>
       }
       %130 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
@@ -125,7 +125,7 @@
     %55 = stablehlo.add %53, %54 : tensor<bf16>
     %56 = stablehlo.reshape %55 : (tensor<bf16>) -> tensor<bf16>
     %57 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
-    %58 = stablehlo.maximum %57, %56 : tensor<bf16>
+    %58 = stablehlo.maximum %56, %57 : tensor<bf16>
     %59 = stablehlo.custom_call @check.eq(%58, %1) : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
     return %59 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
@@ -125,7 +125,7 @@
     %55 = stablehlo.add %53, %54 : tensor<f16>
     %56 = stablehlo.reshape %55 : (tensor<f16>) -> tensor<f16>
     %57 = stablehlo.constant dense<0.000000e+00> : tensor<f16>
-    %58 = stablehlo.maximum %57, %56 : tensor<f16>
+    %58 = stablehlo.maximum %56, %57 : tensor<f16>
     %59 = stablehlo.custom_call @check.eq(%58, %1) : (tensor<f16>, tensor<f16>) -> tensor<i1>
     return %59 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
@@ -110,7 +110,7 @@
     %40 = stablehlo.add %38, %39 : tensor<f32>
     %41 = stablehlo.reshape %40 : (tensor<f32>) -> tensor<f32>
     %42 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-    %43 = stablehlo.maximum %42, %41 : tensor<f32>
+    %43 = stablehlo.maximum %41, %42 : tensor<f32>
     %44 = stablehlo.custom_call @check.eq(%43, %1) : (tensor<f32>, tensor<f32>) -> tensor<i1>
     return %44 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/regularized_incomplete_beta__bfloat16.mlir b/stablehlo/stablehlo/testdata/regularized_incomplete_beta__bfloat16.mlir
--- stablehlo/stablehlo/testdata/regularized_incomplete_beta__bfloat16.mlir
+++ stablehlo/stablehlo/testdata/regularized_incomplete_beta__bfloat16.mlir
@@ -71,9 +71,9 @@
     %40 = stablehlo.multiply %38, %39 : tensor<9xf32>
     %41 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
     %42 = stablehlo.constant dense<2.000000e+00> : tensor<9xf32>
-    %43 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %43 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %44 = stablehlo.add %25, %43 : tensor<9xf32>
-    %45 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %45 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %46 = stablehlo.add %25, %45 : tensor<9xf32>
     %47 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %48 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
@@ -83,10 +83,10 @@
     %52 = stablehlo.subtract %35, %32 : tensor<9xf32>
     %53 = stablehlo.multiply %32, %52 : tensor<9xf32>
     %54 = stablehlo.multiply %53, %39 : tensor<9xf32>
-    %55 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %55 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %56 = stablehlo.add %25, %55 : tensor<9xf32>
     %57 = stablehlo.subtract %56, %48 : tensor<9xf32>
-    %58 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %58 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %59 = stablehlo.add %25, %58 : tensor<9xf32>
     %60 = stablehlo.multiply %57, %59 : tensor<9xf32>
     %61 = stablehlo.divide %54, %60 : tensor<9xf32>
@@ -225,9 +225,9 @@
       %502 = stablehlo.multiply %501, %iterArg_6 : tensor<9xf32>
       %503 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
       %504 = stablehlo.constant dense<2.000000e+00> : tensor<9xf32>
-      %505 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %505 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %506 = stablehlo.add %iterArg_4, %505 : tensor<9xf32>
-      %507 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %507 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %508 = stablehlo.add %iterArg_4, %507 : tensor<9xf32>
       %509 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
       %510 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
@@ -237,10 +237,10 @@
       %514 = stablehlo.subtract %iterArg_5, %496 : tensor<9xf32>
       %515 = stablehlo.multiply %496, %514 : tensor<9xf32>
       %516 = stablehlo.multiply %515, %iterArg_6 : tensor<9xf32>
-      %517 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %517 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %518 = stablehlo.add %iterArg_4, %517 : tensor<9xf32>
       %519 = stablehlo.subtract %518, %510 : tensor<9xf32>
-      %520 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %520 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %521 = stablehlo.add %iterArg_4, %520 : tensor<9xf32>
       %522 = stablehlo.multiply %519, %521 : tensor<9xf32>
       %523 = stablehlo.divide %516, %522 : tensor<9xf32>
@@ -322,7 +322,7 @@
     %79 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %80 = stablehlo.subtract %79, %74 : tensor<9xf32>
     %81 = stablehlo.select %77, %80, %74 : tensor<9xi1>, tensor<9xf32>
-    %82 = stablehlo.multiply %71, %81 : tensor<9xf32>
+    %82 = stablehlo.multiply %81, %71 : tensor<9xf32>
     %83 = stablehlo.sine %82 : tensor<9xf32>
     %84 = stablehlo.log %83 : tensor<9xf32>
     %85 = stablehlo.is_finite %84 : (tensor<9xf32>) -> tensor<9xi1>
@@ -340,17 +340,17 @@
     %97 = stablehlo.add %95, %96 : tensor<9xf32>
     %98 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %99 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %100 = stablehlo.add %99, %95 : tensor<9xf32>
+    %100 = stablehlo.add %95, %99 : tensor<9xf32>
     %101 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %102 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %103 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %104 = stablehlo.divide %95, %103 : tensor<9xf32>
     %105 = stablehlo.log_plus_one %104 : tensor<9xf32>
-    %106 = stablehlo.add %102, %105 : tensor<9xf32>
+    %106 = stablehlo.add %105, %102 : tensor<9xf32>
     %107 = stablehlo.divide %100, %106 : tensor<9xf32>
     %108 = stablehlo.subtract %97, %107 : tensor<9xf32>
     %109 = stablehlo.multiply %108, %106 : tensor<9xf32>
-    %110 = stablehlo.add %90, %109 : tensor<9xf32>
+    %110 = stablehlo.add %109, %90 : tensor<9xf32>
     %111 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %112 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %113 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -361,7 +361,7 @@
     %118 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %119 = stablehlo.add %117, %118 : tensor<9xf32>
     %120 = stablehlo.divide %114, %119 : tensor<9xf32>
-    %121 = stablehlo.add %112, %120 : tensor<9xf32>
+    %121 = stablehlo.add %120, %112 : tensor<9xf32>
     %122 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %123 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %124 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -453,7 +453,7 @@
     %210 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %211 = stablehlo.subtract %210, %205 : tensor<9xf32>
     %212 = stablehlo.select %208, %211, %205 : tensor<9xi1>, tensor<9xf32>
-    %213 = stablehlo.multiply %202, %212 : tensor<9xf32>
+    %213 = stablehlo.multiply %212, %202 : tensor<9xf32>
     %214 = stablehlo.sine %213 : tensor<9xf32>
     %215 = stablehlo.log %214 : tensor<9xf32>
     %216 = stablehlo.is_finite %215 : (tensor<9xf32>) -> tensor<9xi1>
@@ -471,17 +471,17 @@
     %228 = stablehlo.add %226, %227 : tensor<9xf32>
     %229 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %230 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %231 = stablehlo.add %230, %226 : tensor<9xf32>
+    %231 = stablehlo.add %226, %230 : tensor<9xf32>
     %232 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %233 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %234 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %235 = stablehlo.divide %226, %234 : tensor<9xf32>
     %236 = stablehlo.log_plus_one %235 : tensor<9xf32>
-    %237 = stablehlo.add %233, %236 : tensor<9xf32>
+    %237 = stablehlo.add %236, %233 : tensor<9xf32>
     %238 = stablehlo.divide %231, %237 : tensor<9xf32>
     %239 = stablehlo.subtract %228, %238 : tensor<9xf32>
     %240 = stablehlo.multiply %239, %237 : tensor<9xf32>
-    %241 = stablehlo.add %221, %240 : tensor<9xf32>
+    %241 = stablehlo.add %240, %221 : tensor<9xf32>
     %242 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %243 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %244 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -492,7 +492,7 @@
     %249 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %250 = stablehlo.add %248, %249 : tensor<9xf32>
     %251 = stablehlo.divide %245, %250 : tensor<9xf32>
-    %252 = stablehlo.add %243, %251 : tensor<9xf32>
+    %252 = stablehlo.add %251, %243 : tensor<9xf32>
     %253 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %254 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %255 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -586,7 +586,7 @@
     %343 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %344 = stablehlo.subtract %343, %338 : tensor<9xf32>
     %345 = stablehlo.select %341, %344, %338 : tensor<9xi1>, tensor<9xf32>
-    %346 = stablehlo.multiply %335, %345 : tensor<9xf32>
+    %346 = stablehlo.multiply %345, %335 : tensor<9xf32>
     %347 = stablehlo.sine %346 : tensor<9xf32>
     %348 = stablehlo.log %347 : tensor<9xf32>
     %349 = stablehlo.is_finite %348 : (tensor<9xf32>) -> tensor<9xi1>
@@ -604,17 +604,17 @@
     %361 = stablehlo.add %359, %360 : tensor<9xf32>
     %362 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %363 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %364 = stablehlo.add %363, %359 : tensor<9xf32>
+    %364 = stablehlo.add %359, %363 : tensor<9xf32>
     %365 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %366 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %367 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %368 = stablehlo.divide %359, %367 : tensor<9xf32>
     %369 = stablehlo.log_plus_one %368 : tensor<9xf32>
-    %370 = stablehlo.add %366, %369 : tensor<9xf32>
+    %370 = stablehlo.add %369, %366 : tensor<9xf32>
     %371 = stablehlo.divide %364, %370 : tensor<9xf32>
     %372 = stablehlo.subtract %361, %371 : tensor<9xf32>
     %373 = stablehlo.multiply %372, %370 : tensor<9xf32>
-    %374 = stablehlo.add %354, %373 : tensor<9xf32>
+    %374 = stablehlo.add %373, %354 : tensor<9xf32>
     %375 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %376 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %377 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -625,7 +625,7 @@
     %382 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %383 = stablehlo.add %381, %382 : tensor<9xf32>
     %384 = stablehlo.divide %378, %383 : tensor<9xf32>
-    %385 = stablehlo.add %376, %384 : tensor<9xf32>
+    %385 = stablehlo.add %384, %376 : tensor<9xf32>
     %386 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %387 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %388 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/regularized_incomplete_beta__float16.mlir b/stablehlo/stablehlo/testdata/regularized_incomplete_beta__float16.mlir
--- stablehlo/stablehlo/testdata/regularized_incomplete_beta__float16.mlir
+++ stablehlo/stablehlo/testdata/regularized_incomplete_beta__float16.mlir
@@ -71,9 +71,9 @@
     %40 = stablehlo.multiply %38, %39 : tensor<9xf32>
     %41 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
     %42 = stablehlo.constant dense<2.000000e+00> : tensor<9xf32>
-    %43 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %43 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %44 = stablehlo.add %25, %43 : tensor<9xf32>
-    %45 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %45 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %46 = stablehlo.add %25, %45 : tensor<9xf32>
     %47 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %48 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
@@ -83,10 +83,10 @@
     %52 = stablehlo.subtract %35, %32 : tensor<9xf32>
     %53 = stablehlo.multiply %32, %52 : tensor<9xf32>
     %54 = stablehlo.multiply %53, %39 : tensor<9xf32>
-    %55 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %55 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %56 = stablehlo.add %25, %55 : tensor<9xf32>
     %57 = stablehlo.subtract %56, %48 : tensor<9xf32>
-    %58 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %58 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %59 = stablehlo.add %25, %58 : tensor<9xf32>
     %60 = stablehlo.multiply %57, %59 : tensor<9xf32>
     %61 = stablehlo.divide %54, %60 : tensor<9xf32>
@@ -225,9 +225,9 @@
       %502 = stablehlo.multiply %501, %iterArg_6 : tensor<9xf32>
       %503 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
       %504 = stablehlo.constant dense<2.000000e+00> : tensor<9xf32>
-      %505 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %505 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %506 = stablehlo.add %iterArg_4, %505 : tensor<9xf32>
-      %507 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %507 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %508 = stablehlo.add %iterArg_4, %507 : tensor<9xf32>
       %509 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
       %510 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
@@ -237,10 +237,10 @@
       %514 = stablehlo.subtract %iterArg_5, %496 : tensor<9xf32>
       %515 = stablehlo.multiply %496, %514 : tensor<9xf32>
       %516 = stablehlo.multiply %515, %iterArg_6 : tensor<9xf32>
-      %517 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %517 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %518 = stablehlo.add %iterArg_4, %517 : tensor<9xf32>
       %519 = stablehlo.subtract %518, %510 : tensor<9xf32>
-      %520 = stablehlo.multiply %504, %496 : tensor<9xf32>
+      %520 = stablehlo.multiply %496, %504 : tensor<9xf32>
       %521 = stablehlo.add %iterArg_4, %520 : tensor<9xf32>
       %522 = stablehlo.multiply %519, %521 : tensor<9xf32>
       %523 = stablehlo.divide %516, %522 : tensor<9xf32>
@@ -322,7 +322,7 @@
     %79 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %80 = stablehlo.subtract %79, %74 : tensor<9xf32>
     %81 = stablehlo.select %77, %80, %74 : tensor<9xi1>, tensor<9xf32>
-    %82 = stablehlo.multiply %71, %81 : tensor<9xf32>
+    %82 = stablehlo.multiply %81, %71 : tensor<9xf32>
     %83 = stablehlo.sine %82 : tensor<9xf32>
     %84 = stablehlo.log %83 : tensor<9xf32>
     %85 = stablehlo.is_finite %84 : (tensor<9xf32>) -> tensor<9xi1>
@@ -340,17 +340,17 @@
     %97 = stablehlo.add %95, %96 : tensor<9xf32>
     %98 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %99 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %100 = stablehlo.add %99, %95 : tensor<9xf32>
+    %100 = stablehlo.add %95, %99 : tensor<9xf32>
     %101 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %102 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %103 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %104 = stablehlo.divide %95, %103 : tensor<9xf32>
     %105 = stablehlo.log_plus_one %104 : tensor<9xf32>
-    %106 = stablehlo.add %102, %105 : tensor<9xf32>
+    %106 = stablehlo.add %105, %102 : tensor<9xf32>
     %107 = stablehlo.divide %100, %106 : tensor<9xf32>
     %108 = stablehlo.subtract %97, %107 : tensor<9xf32>
     %109 = stablehlo.multiply %108, %106 : tensor<9xf32>
-    %110 = stablehlo.add %90, %109 : tensor<9xf32>
+    %110 = stablehlo.add %109, %90 : tensor<9xf32>
     %111 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %112 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %113 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -361,7 +361,7 @@
     %118 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %119 = stablehlo.add %117, %118 : tensor<9xf32>
     %120 = stablehlo.divide %114, %119 : tensor<9xf32>
-    %121 = stablehlo.add %112, %120 : tensor<9xf32>
+    %121 = stablehlo.add %120, %112 : tensor<9xf32>
     %122 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %123 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %124 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -453,7 +453,7 @@
     %210 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %211 = stablehlo.subtract %210, %205 : tensor<9xf32>
     %212 = stablehlo.select %208, %211, %205 : tensor<9xi1>, tensor<9xf32>
-    %213 = stablehlo.multiply %202, %212 : tensor<9xf32>
+    %213 = stablehlo.multiply %212, %202 : tensor<9xf32>
     %214 = stablehlo.sine %213 : tensor<9xf32>
     %215 = stablehlo.log %214 : tensor<9xf32>
     %216 = stablehlo.is_finite %215 : (tensor<9xf32>) -> tensor<9xi1>
@@ -471,17 +471,17 @@
     %228 = stablehlo.add %226, %227 : tensor<9xf32>
     %229 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %230 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %231 = stablehlo.add %230, %226 : tensor<9xf32>
+    %231 = stablehlo.add %226, %230 : tensor<9xf32>
     %232 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %233 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %234 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %235 = stablehlo.divide %226, %234 : tensor<9xf32>
     %236 = stablehlo.log_plus_one %235 : tensor<9xf32>
-    %237 = stablehlo.add %233, %236 : tensor<9xf32>
+    %237 = stablehlo.add %236, %233 : tensor<9xf32>
     %238 = stablehlo.divide %231, %237 : tensor<9xf32>
     %239 = stablehlo.subtract %228, %238 : tensor<9xf32>
     %240 = stablehlo.multiply %239, %237 : tensor<9xf32>
-    %241 = stablehlo.add %221, %240 : tensor<9xf32>
+    %241 = stablehlo.add %240, %221 : tensor<9xf32>
     %242 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %243 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %244 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -492,7 +492,7 @@
     %249 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %250 = stablehlo.add %248, %249 : tensor<9xf32>
     %251 = stablehlo.divide %245, %250 : tensor<9xf32>
-    %252 = stablehlo.add %243, %251 : tensor<9xf32>
+    %252 = stablehlo.add %251, %243 : tensor<9xf32>
     %253 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %254 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %255 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -586,7 +586,7 @@
     %343 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %344 = stablehlo.subtract %343, %338 : tensor<9xf32>
     %345 = stablehlo.select %341, %344, %338 : tensor<9xi1>, tensor<9xf32>
-    %346 = stablehlo.multiply %335, %345 : tensor<9xf32>
+    %346 = stablehlo.multiply %345, %335 : tensor<9xf32>
     %347 = stablehlo.sine %346 : tensor<9xf32>
     %348 = stablehlo.log %347 : tensor<9xf32>
     %349 = stablehlo.is_finite %348 : (tensor<9xf32>) -> tensor<9xi1>
@@ -604,17 +604,17 @@
     %361 = stablehlo.add %359, %360 : tensor<9xf32>
     %362 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %363 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %364 = stablehlo.add %363, %359 : tensor<9xf32>
+    %364 = stablehlo.add %359, %363 : tensor<9xf32>
     %365 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %366 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %367 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %368 = stablehlo.divide %359, %367 : tensor<9xf32>
     %369 = stablehlo.log_plus_one %368 : tensor<9xf32>
-    %370 = stablehlo.add %366, %369 : tensor<9xf32>
+    %370 = stablehlo.add %369, %366 : tensor<9xf32>
     %371 = stablehlo.divide %364, %370 : tensor<9xf32>
     %372 = stablehlo.subtract %361, %371 : tensor<9xf32>
     %373 = stablehlo.multiply %372, %370 : tensor<9xf32>
-    %374 = stablehlo.add %354, %373 : tensor<9xf32>
+    %374 = stablehlo.add %373, %354 : tensor<9xf32>
     %375 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %376 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %377 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -625,7 +625,7 @@
     %382 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %383 = stablehlo.add %381, %382 : tensor<9xf32>
     %384 = stablehlo.divide %378, %383 : tensor<9xf32>
-    %385 = stablehlo.add %376, %384 : tensor<9xf32>
+    %385 = stablehlo.add %384, %376 : tensor<9xf32>
     %386 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %387 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %388 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/regularized_incomplete_beta__float32.mlir b/stablehlo/stablehlo/testdata/regularized_incomplete_beta__float32.mlir
--- stablehlo/stablehlo/testdata/regularized_incomplete_beta__float32.mlir
+++ stablehlo/stablehlo/testdata/regularized_incomplete_beta__float32.mlir
@@ -71,9 +71,9 @@
     %40 = stablehlo.multiply %38, %39 : tensor<9xf32>
     %41 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
     %42 = stablehlo.constant dense<2.000000e+00> : tensor<9xf32>
-    %43 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %43 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %44 = stablehlo.add %25, %43 : tensor<9xf32>
-    %45 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %45 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %46 = stablehlo.add %25, %45 : tensor<9xf32>
     %47 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %48 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
@@ -83,10 +83,10 @@
     %52 = stablehlo.subtract %35, %32 : tensor<9xf32>
     %53 = stablehlo.multiply %32, %52 : tensor<9xf32>
     %54 = stablehlo.multiply %53, %39 : tensor<9xf32>
-    %55 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %55 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %56 = stablehlo.add %25, %55 : tensor<9xf32>
     %57 = stablehlo.subtract %56, %48 : tensor<9xf32>
-    %58 = stablehlo.multiply %42, %32 : tensor<9xf32>
+    %58 = stablehlo.multiply %32, %42 : tensor<9xf32>
     %59 = stablehlo.add %25, %58 : tensor<9xf32>
     %60 = stablehlo.multiply %57, %59 : tensor<9xf32>
     %61 = stablehlo.divide %54, %60 : tensor<9xf32>
@@ -222,9 +222,9 @@
       %498 = stablehlo.multiply %497, %iterArg_6 : tensor<9xf32>
       %499 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
       %500 = stablehlo.constant dense<2.000000e+00> : tensor<9xf32>
-      %501 = stablehlo.multiply %500, %492 : tensor<9xf32>
+      %501 = stablehlo.multiply %492, %500 : tensor<9xf32>
       %502 = stablehlo.add %iterArg_4, %501 : tensor<9xf32>
-      %503 = stablehlo.multiply %500, %492 : tensor<9xf32>
+      %503 = stablehlo.multiply %492, %500 : tensor<9xf32>
       %504 = stablehlo.add %iterArg_4, %503 : tensor<9xf32>
       %505 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
       %506 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
@@ -234,10 +234,10 @@
       %510 = stablehlo.subtract %iterArg_5, %492 : tensor<9xf32>
       %511 = stablehlo.multiply %492, %510 : tensor<9xf32>
       %512 = stablehlo.multiply %511, %iterArg_6 : tensor<9xf32>
-      %513 = stablehlo.multiply %500, %492 : tensor<9xf32>
+      %513 = stablehlo.multiply %492, %500 : tensor<9xf32>
       %514 = stablehlo.add %iterArg_4, %513 : tensor<9xf32>
       %515 = stablehlo.subtract %514, %506 : tensor<9xf32>
-      %516 = stablehlo.multiply %500, %492 : tensor<9xf32>
+      %516 = stablehlo.multiply %492, %500 : tensor<9xf32>
       %517 = stablehlo.add %iterArg_4, %516 : tensor<9xf32>
       %518 = stablehlo.multiply %515, %517 : tensor<9xf32>
       %519 = stablehlo.divide %512, %518 : tensor<9xf32>
@@ -319,7 +319,7 @@
     %76 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %77 = stablehlo.subtract %76, %71 : tensor<9xf32>
     %78 = stablehlo.select %74, %77, %71 : tensor<9xi1>, tensor<9xf32>
-    %79 = stablehlo.multiply %68, %78 : tensor<9xf32>
+    %79 = stablehlo.multiply %78, %68 : tensor<9xf32>
     %80 = stablehlo.sine %79 : tensor<9xf32>
     %81 = stablehlo.log %80 : tensor<9xf32>
     %82 = stablehlo.is_finite %81 : (tensor<9xf32>) -> tensor<9xi1>
@@ -337,17 +337,17 @@
     %94 = stablehlo.add %92, %93 : tensor<9xf32>
     %95 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %96 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %97 = stablehlo.add %96, %92 : tensor<9xf32>
+    %97 = stablehlo.add %92, %96 : tensor<9xf32>
     %98 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %99 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %100 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %101 = stablehlo.divide %92, %100 : tensor<9xf32>
     %102 = stablehlo.log_plus_one %101 : tensor<9xf32>
-    %103 = stablehlo.add %99, %102 : tensor<9xf32>
+    %103 = stablehlo.add %102, %99 : tensor<9xf32>
     %104 = stablehlo.divide %97, %103 : tensor<9xf32>
     %105 = stablehlo.subtract %94, %104 : tensor<9xf32>
     %106 = stablehlo.multiply %105, %103 : tensor<9xf32>
-    %107 = stablehlo.add %87, %106 : tensor<9xf32>
+    %107 = stablehlo.add %106, %87 : tensor<9xf32>
     %108 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %109 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %110 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -358,7 +358,7 @@
     %115 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %116 = stablehlo.add %114, %115 : tensor<9xf32>
     %117 = stablehlo.divide %111, %116 : tensor<9xf32>
-    %118 = stablehlo.add %109, %117 : tensor<9xf32>
+    %118 = stablehlo.add %117, %109 : tensor<9xf32>
     %119 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %120 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %121 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -450,7 +450,7 @@
     %207 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %208 = stablehlo.subtract %207, %202 : tensor<9xf32>
     %209 = stablehlo.select %205, %208, %202 : tensor<9xi1>, tensor<9xf32>
-    %210 = stablehlo.multiply %199, %209 : tensor<9xf32>
+    %210 = stablehlo.multiply %209, %199 : tensor<9xf32>
     %211 = stablehlo.sine %210 : tensor<9xf32>
     %212 = stablehlo.log %211 : tensor<9xf32>
     %213 = stablehlo.is_finite %212 : (tensor<9xf32>) -> tensor<9xi1>
@@ -468,17 +468,17 @@
     %225 = stablehlo.add %223, %224 : tensor<9xf32>
     %226 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %227 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %228 = stablehlo.add %227, %223 : tensor<9xf32>
+    %228 = stablehlo.add %223, %227 : tensor<9xf32>
     %229 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %230 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %231 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %232 = stablehlo.divide %223, %231 : tensor<9xf32>
     %233 = stablehlo.log_plus_one %232 : tensor<9xf32>
-    %234 = stablehlo.add %230, %233 : tensor<9xf32>
+    %234 = stablehlo.add %233, %230 : tensor<9xf32>
     %235 = stablehlo.divide %228, %234 : tensor<9xf32>
     %236 = stablehlo.subtract %225, %235 : tensor<9xf32>
     %237 = stablehlo.multiply %236, %234 : tensor<9xf32>
-    %238 = stablehlo.add %218, %237 : tensor<9xf32>
+    %238 = stablehlo.add %237, %218 : tensor<9xf32>
     %239 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %240 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %241 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -489,7 +489,7 @@
     %246 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %247 = stablehlo.add %245, %246 : tensor<9xf32>
     %248 = stablehlo.divide %242, %247 : tensor<9xf32>
-    %249 = stablehlo.add %240, %248 : tensor<9xf32>
+    %249 = stablehlo.add %248, %240 : tensor<9xf32>
     %250 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %251 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %252 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
@@ -583,7 +583,7 @@
     %340 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %341 = stablehlo.subtract %340, %335 : tensor<9xf32>
     %342 = stablehlo.select %338, %341, %335 : tensor<9xi1>, tensor<9xf32>
-    %343 = stablehlo.multiply %332, %342 : tensor<9xf32>
+    %343 = stablehlo.multiply %342, %332 : tensor<9xf32>
     %344 = stablehlo.sine %343 : tensor<9xf32>
     %345 = stablehlo.log %344 : tensor<9xf32>
     %346 = stablehlo.is_finite %345 : (tensor<9xf32>) -> tensor<9xi1>
@@ -601,17 +601,17 @@
     %358 = stablehlo.add %356, %357 : tensor<9xf32>
     %359 = stablehlo.constant dense<7.500000e+00> : tensor<f32>
     %360 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
-    %361 = stablehlo.add %360, %356 : tensor<9xf32>
+    %361 = stablehlo.add %356, %360 : tensor<9xf32>
     %362 = stablehlo.constant dense<2.01490307> : tensor<f32>
     %363 = stablehlo.constant dense<2.01490307> : tensor<9xf32>
     %364 = stablehlo.constant dense<7.500000e+00> : tensor<9xf32>
     %365 = stablehlo.divide %356, %364 : tensor<9xf32>
     %366 = stablehlo.log_plus_one %365 : tensor<9xf32>
-    %367 = stablehlo.add %363, %366 : tensor<9xf32>
+    %367 = stablehlo.add %366, %363 : tensor<9xf32>
     %368 = stablehlo.divide %361, %367 : tensor<9xf32>
     %369 = stablehlo.subtract %358, %368 : tensor<9xf32>
     %370 = stablehlo.multiply %369, %367 : tensor<9xf32>
-    %371 = stablehlo.add %351, %370 : tensor<9xf32>
+    %371 = stablehlo.add %370, %351 : tensor<9xf32>
     %372 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
     %373 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %374 = stablehlo.constant dense<676.520386> : tensor<f32>
@@ -622,7 +622,7 @@
     %379 = stablehlo.constant dense<1.000000e+00> : tensor<9xf32>
     %380 = stablehlo.add %378, %379 : tensor<9xf32>
     %381 = stablehlo.divide %375, %380 : tensor<9xf32>
-    %382 = stablehlo.add %373, %381 : tensor<9xf32>
+    %382 = stablehlo.add %381, %373 : tensor<9xf32>
     %383 = stablehlo.constant dense<-1259.13916> : tensor<f32>
     %384 = stablehlo.constant dense<-1259.13916> : tensor<9xf32>
     %385 = stablehlo.constant dense<1.000000e+00> : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/sinh_shape_bfloat16_20_20.mlir b/stablehlo/stablehlo/testdata/sinh_shape_bfloat16_20_20.mlir
--- stablehlo/stablehlo/testdata/sinh_shape_bfloat16_20_20.mlir
+++ stablehlo/stablehlo/testdata/sinh_shape_bfloat16_20_20.mlir
@@ -19,7 +19,7 @@
     %13 = stablehlo.add %10, %11 : tensor<20x20xf32>
     %14 = stablehlo.divide %10, %13 : tensor<20x20xf32>
     %15 = stablehlo.add %10, %14 : tensor<20x20xf32>
-    %16 = stablehlo.multiply %12, %15 : tensor<20x20xf32>
+    %16 = stablehlo.multiply %15, %12 : tensor<20x20xf32>
     %17 = stablehlo.abs %2 : tensor<20x20xf32>
     %18 = stablehlo.compare  LT, %17, %11 : (tensor<20x20xf32>, tensor<20x20xf32>) -> tensor<20x20xi1>
     %19 = stablehlo.select %18, %16, %9 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/sinh_shape_float16_20_20.mlir b/stablehlo/stablehlo/testdata/sinh_shape_float16_20_20.mlir
--- stablehlo/stablehlo/testdata/sinh_shape_float16_20_20.mlir
+++ stablehlo/stablehlo/testdata/sinh_shape_float16_20_20.mlir
@@ -19,7 +19,7 @@
     %13 = stablehlo.add %10, %11 : tensor<20x20xf32>
     %14 = stablehlo.divide %10, %13 : tensor<20x20xf32>
     %15 = stablehlo.add %10, %14 : tensor<20x20xf32>
-    %16 = stablehlo.multiply %12, %15 : tensor<20x20xf32>
+    %16 = stablehlo.multiply %15, %12 : tensor<20x20xf32>
     %17 = stablehlo.abs %2 : tensor<20x20xf32>
     %18 = stablehlo.compare  LT, %17, %11 : (tensor<20x20xf32>, tensor<20x20xf32>) -> tensor<20x20xi1>
     %19 = stablehlo.select %18, %16, %9 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/sinh_shape_float32_20_20.mlir b/stablehlo/stablehlo/testdata/sinh_shape_float32_20_20.mlir
--- stablehlo/stablehlo/testdata/sinh_shape_float32_20_20.mlir
+++ stablehlo/stablehlo/testdata/sinh_shape_float32_20_20.mlir
@@ -18,7 +18,7 @@
     %12 = stablehlo.add %9, %10 : tensor<20x20xf32>
     %13 = stablehlo.divide %9, %12 : tensor<20x20xf32>
     %14 = stablehlo.add %9, %13 : tensor<20x20xf32>
-    %15 = stablehlo.multiply %11, %14 : tensor<20x20xf32>
+    %15 = stablehlo.multiply %14, %11 : tensor<20x20xf32>
     %16 = stablehlo.abs %0 : tensor<20x20xf32>
     %17 = stablehlo.compare  LT, %16, %10 : (tensor<20x20xf32>, tensor<20x20xf32>) -> tensor<20x20xi1>
     %18 = stablehlo.select %17, %15, %8 : tensor<20x20xi1>, tensor<20x20xf32>
diff --ruN a/stablehlo/stablehlo/testdata/slice_in_dim_limit_neg_dynamic.mlir b/stablehlo/stablehlo/testdata/slice_in_dim_limit_neg_dynamic.mlir
--- stablehlo/stablehlo/testdata/slice_in_dim_limit_neg_dynamic.mlir
+++ stablehlo/stablehlo/testdata/slice_in_dim_limit_neg_dynamic.mlir
@@ -3,7 +3,7 @@
 module @jit_fun_flat_jax {
   func.func public @main(%arg0: tensor<i64>, %arg1: tensor<?x4xf32> {mhlo.sharding = ""}) -> tensor<?x4xf32> {
     %0 = stablehlo.constant dense<-1> : tensor<i64>
-    %1 = stablehlo.add %0, %arg0 : tensor<i64>
+    %1 = stablehlo.add %arg0, %0 : tensor<i64>
     %2 = stablehlo.constant dense<0> : tensor<1xi32>
     %3 = stablehlo.constant dense<0> : tensor<1xi32>
     %4 = stablehlo.concatenate %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
diff --ruN a/stablehlo/stablehlo/testdata/slice_in_dim_start_neg_dynamic.mlir b/stablehlo/stablehlo/testdata/slice_in_dim_start_neg_dynamic.mlir
--- stablehlo/stablehlo/testdata/slice_in_dim_start_neg_dynamic.mlir
+++ stablehlo/stablehlo/testdata/slice_in_dim_start_neg_dynamic.mlir
@@ -3,7 +3,7 @@
 module @jit_fun_flat_jax {
   func.func public @main(%arg0: tensor<i64>, %arg1: tensor<?x4xf32> {mhlo.sharding = ""}) -> tensor<1x4xf32> {
     %0 = stablehlo.constant dense<-1> : tensor<i64>
-    %1 = stablehlo.add %0, %arg0 : tensor<i64>
+    %1 = stablehlo.add %arg0, %0 : tensor<i64>
     %2 = stablehlo.convert %1 : (tensor<i64>) -> tensor<i32>
     %3 = stablehlo.reshape %2 : (tensor<i32>) -> tensor<1xi32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
@@ -29,7 +29,7 @@
     %20 = stablehlo.compare  LT, %8, %19,  SIGNED : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
     %21 = stablehlo.constant dense<3> : tensor<i64>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %23 = stablehlo.add %8, %22 : tensor<1xi64>
+    %23 = stablehlo.add %22, %8 : tensor<1xi64>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi64>
     %25 = stablehlo.convert %24 : (tensor<1xi64>) -> tensor<1xi32>
     %26 = stablehlo.broadcast_in_dim %25, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
@@ -46,7 +46,7 @@
     %37 = stablehlo.compare  LT, %9, %36,  SIGNED : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
     %38 = stablehlo.constant dense<3> : tensor<i64>
     %39 = stablehlo.broadcast_in_dim %38, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %40 = stablehlo.add %9, %39 : tensor<1xi64>
+    %40 = stablehlo.add %39, %9 : tensor<1xi64>
     %41 = stablehlo.select %37, %40, %9 : tensor<1xi1>, tensor<1xi64>
     %42 = stablehlo.convert %41 : (tensor<1xi64>) -> tensor<1xi32>
     %43 = stablehlo.broadcast_in_dim %42, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir b/stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
--- stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
@@ -34,7 +34,7 @@
     %25 = stablehlo.compare  LT, %16, %24,  SIGNED : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
     %26 = stablehlo.constant dense<2> : tensor<i64>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %28 = stablehlo.add %16, %27 : tensor<1xi64>
+    %28 = stablehlo.add %27, %16 : tensor<1xi64>
     %29 = stablehlo.select %25, %28, %16 : tensor<1xi1>, tensor<1xi64>
     %30 = stablehlo.convert %29 : (tensor<1xi64>) -> tensor<1xi32>
     %31 = stablehlo.broadcast_in_dim %30, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
@@ -49,7 +49,7 @@
     %40 = stablehlo.compare  LT, %17, %39,  SIGNED : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
     %41 = stablehlo.constant dense<2> : tensor<i64>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %43 = stablehlo.add %17, %42 : tensor<1xi64>
+    %43 = stablehlo.add %42, %17 : tensor<1xi64>
     %44 = stablehlo.select %40, %43, %17 : tensor<1xi1>, tensor<1xi64>
     %45 = stablehlo.convert %44 : (tensor<1xi64>) -> tensor<1xi32>
     %46 = stablehlo.broadcast_in_dim %45, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir b/stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
--- stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
@@ -47,7 +47,7 @@
     %38 = stablehlo.compare  LT, %29, %37,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %39 = stablehlo.constant dense<2> : tensor<i64>
     %40 = stablehlo.broadcast_in_dim %39, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %41 = stablehlo.add %29, %40 : tensor<2xi64>
+    %41 = stablehlo.add %40, %29 : tensor<2xi64>
     %42 = stablehlo.select %38, %41, %29 : tensor<2xi1>, tensor<2xi64>
     %43 = stablehlo.convert %42 : (tensor<2xi64>) -> tensor<2xi32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
@@ -62,7 +62,7 @@
     %53 = stablehlo.compare  LT, %30, %52,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %54 = stablehlo.constant dense<2> : tensor<i64>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %56 = stablehlo.add %30, %55 : tensor<2xi64>
+    %56 = stablehlo.add %55, %30 : tensor<2xi64>
     %57 = stablehlo.select %53, %56, %30 : tensor<2xi1>, tensor<2xi64>
     %58 = stablehlo.convert %57 : (tensor<2xi64>) -> tensor<2xi32>
     %59 = stablehlo.broadcast_in_dim %58, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
@@ -41,7 +41,7 @@
     %32 = stablehlo.compare  LT, %22, %31,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %33 = stablehlo.constant dense<2> : tensor<i64>
     %34 = stablehlo.broadcast_in_dim %33, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %35 = stablehlo.add %22, %34 : tensor<2xi64>
+    %35 = stablehlo.add %34, %22 : tensor<2xi64>
     %36 = stablehlo.select %32, %35, %22 : tensor<2xi1>, tensor<2xi64>
     %37 = stablehlo.convert %36 : (tensor<2xi64>) -> tensor<2xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
@@ -56,7 +56,7 @@
     %47 = stablehlo.compare  LT, %23, %46,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %48 = stablehlo.constant dense<2> : tensor<i64>
     %49 = stablehlo.broadcast_in_dim %48, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %50 = stablehlo.add %23, %49 : tensor<2xi64>
+    %50 = stablehlo.add %49, %23 : tensor<2xi64>
     %51 = stablehlo.select %47, %50, %23 : tensor<2xi1>, tensor<2xi64>
     %52 = stablehlo.convert %51 : (tensor<2xi64>) -> tensor<2xi32>
     %53 = stablehlo.broadcast_in_dim %52, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
@@ -45,7 +45,7 @@
     %36 = stablehlo.compare  LT, %22, %35,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %37 = stablehlo.constant dense<4> : tensor<i64>
     %38 = stablehlo.broadcast_in_dim %37, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %39 = stablehlo.add %22, %38 : tensor<2xi64>
+    %39 = stablehlo.add %38, %22 : tensor<2xi64>
     %40 = stablehlo.select %36, %39, %22 : tensor<2xi1>, tensor<2xi64>
     %41 = stablehlo.convert %40 : (tensor<2xi64>) -> tensor<2xi32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
@@ -64,7 +64,7 @@
     %55 = stablehlo.compare  LT, %23, %54,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %56 = stablehlo.constant dense<4> : tensor<i64>
     %57 = stablehlo.broadcast_in_dim %56, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %58 = stablehlo.add %23, %57 : tensor<2xi64>
+    %58 = stablehlo.add %57, %23 : tensor<2xi64>
     %59 = stablehlo.select %55, %58, %23 : tensor<2xi1>, tensor<2xi64>
     %60 = stablehlo.convert %59 : (tensor<2xi64>) -> tensor<2xi32>
     %61 = stablehlo.broadcast_in_dim %60, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
@@ -45,7 +45,7 @@
     %36 = stablehlo.compare  LT, %22, %35,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %37 = stablehlo.constant dense<4> : tensor<i64>
     %38 = stablehlo.broadcast_in_dim %37, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %39 = stablehlo.add %22, %38 : tensor<2xi64>
+    %39 = stablehlo.add %38, %22 : tensor<2xi64>
     %40 = stablehlo.select %36, %39, %22 : tensor<2xi1>, tensor<2xi64>
     %41 = stablehlo.convert %40 : (tensor<2xi64>) -> tensor<2xi32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
@@ -64,7 +64,7 @@
     %55 = stablehlo.compare  LT, %23, %54,  SIGNED : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
     %56 = stablehlo.constant dense<4> : tensor<i64>
     %57 = stablehlo.broadcast_in_dim %56, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %58 = stablehlo.add %23, %57 : tensor<2xi64>
+    %58 = stablehlo.add %57, %23 : tensor<2xi64>
     %59 = stablehlo.select %55, %58, %23 : tensor<2xi1>, tensor<2xi64>
     %60 = stablehlo.convert %59 : (tensor<2xi64>) -> tensor<2xi32>
     %61 = stablehlo.broadcast_in_dim %60, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo.mlir b/stablehlo/stablehlo/tests/ops_stablehlo.mlir
--- stablehlo/stablehlo/tests/ops_stablehlo.mlir
+++ stablehlo/stablehlo/tests/ops_stablehlo.mlir
@@ -894,6 +894,22 @@
   %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
   %1 = stablehlo.dynamic_broadcast_in_dim %arg0, %0, dims = [1] : (tensor<4xf32>, tensor<2xi64>) -> tensor<3x4xf32>
   return %1 : tensor<3x4xf32>
+}
+
+// -----
+
+func.func @dynamic_broadcast_in_dim_negative_size(%arg0: tensor<1xf32>, %shape: tensor<3xi64>) -> tensor<7x8x9xf32> {
+  // expected-error@+1 {{broadcast_dimensions contains invalid value -1 for result with rank 3}}
+  %0 = "stablehlo.dynamic_broadcast_in_dim"(%arg0, %shape) {broadcast_dimensions = dense<[-1]> : tensor<1xi64>} : (tensor<1xf32>, tensor<3xi64>) -> tensor<7x8x9xf32>
+  func.return %0 : tensor<7x8x9xf32>
+}
+
+// -----
+
+func.func @dynamic_broadcast_in_dim_too_large(%arg0: tensor<1xf32>, %shape: tensor<3xi64>) -> tensor<7x8x9xf32> {
+  // expected-error@+1 {{broadcast_dimensions contains invalid value 3 for result with rank 3}}
+  %0 = "stablehlo.dynamic_broadcast_in_dim"(%arg0, %shape) {broadcast_dimensions = dense<[3]> : tensor<1xi64>} : (tensor<1xf32>, tensor<3xi64>) -> tensor<7x8x9xf32>
+  func.return %0 : tensor<7x8x9xf32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
@@ -426,6 +426,172 @@
 
 // -----
 
+// CHECK-LABEL: func @dynamic_reduce_window_success_static_result_type
+func.func @dynamic_reduce_window_success_static_result_type(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<2x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = dense<[2, 1]> : tensor<2xi64>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = dense<[3, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_dimensions = dense<[2, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_strides = dense<[4, 1]> : tensor<2xi64>
+  //          CHECK-SAME: } : (tensor<3x2xf32>, tensor<f32>) -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %5 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_dynamic_result_type
+func.func @dynamic_reduce_window_success_dynamic_result_type(%arg0: tensor<?x2xf32>, %arg1: tensor<f32>) -> tensor<?x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = dense<[2, 1]> : tensor<2xi64>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = dense<[3, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_dimensions = dense<[2, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_strides = dense<[4, 1]> : tensor<2xi64>
+  //          CHECK-SAME: } : (tensor<?x2xf32>, tensor<f32>) -> tensor<?x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<?x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<?x2xf32>
+  func.return %5 : tensor<?x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_reduce_window.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %arg2, %0, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_strides
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_strides(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %arg2, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_base_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_base_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %arg2, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %arg2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_padding
+func.func @dynamic_reduce_window_inapplicable_dynamic_padding(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2x2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %arg2) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
 // CHECK-LABEL: func @dynamic_reshape_success
 func.func @dynamic_reshape_success(%arg0: tensor<4xf32>) -> tensor<1x4xf32> {
   // CHECK-NOT: stablehlo.dynamic_reshape
@@ -452,6 +618,185 @@
   %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
   %1 = stablehlo.dynamic_reshape %arg0, %0 : (tensor<4xf32>, tensor<2xi64>) -> tensor<1x?xf32>
   return %1 : tensor<1x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_success
+func.func @dynamic_rng_bit_generator_success(%arg0: tensor<2xui64>) -> tensor<1x4xf32> {
+  // CHECK-NOT: stablehlo.dynamic_rng_bit_generator
+  // CHECK: stablehlo.rng_bit_generator %arg0, algorithm = DEFAULT : (tensor<2xui64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_rng_bit_generator.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape(%arg0: tensor<2xui64>, %arg1: tensor<2xi64>) -> tensor<1x4xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %arg1) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type(%arg0: tensor<2xui64>) -> tensor<?x?xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<?x?xf32>)
+  return %1#1 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_success
+func.func @dynamic_top_k_success(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: chlo.top_k
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_failure_k_mismatch
+func.func @dynamic_top_k_failure_k_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: @stablehlo.dynamic_top_k
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_not_float
+func.func @dynamic_top_k_error_operand_not_float(%arg0: tensor<16xcomplex<f64>>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xcomplex<f64>>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_unranked
+func.func @dynamic_top_k_error_operand_unranked(%arg0: tensor<*xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<*xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_scalar_operand
+func.func @dynamic_top_k_error_scalar_operand(%arg0: tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<f32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_integer
+func.func @dynamic_top_k_error_k_not_integer(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3.> : tensor<f32>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_scalar
+func.func @dynamic_top_k_error_k_not_scalar(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3> : tensor<1xui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<1xui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O1
+// CHECK-LABEL: func @dynamic_top_k_error_values_not_float
+func.func @dynamic_top_k_error_values_not_float(%arg0: tensor<16xf32>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects values (result #0) to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O2
+// CHECK-LABEL: func @dynamic_top_k_error_indices_not_i32
+func.func @dynamic_top_k_error_indices_not_i32(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi64>) {
+  // expected-error@+2{{expects indices (result #1) to be a tensor of si32}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi64>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi64>
+}
+
+// -----
+
+// dynamic_top_k C1
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_rank
+func.func @dynamic_top_k_error_values_bad_rank(%arg0: tensor<16xf32>) -> (tensor<3x4xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values shape to match the operand shape in all but the last dimension}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3x4xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3x4xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C2
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_element_type
+func.func @dynamic_top_k_error_values_bad_element_type(%arg0: tensor<16xf32>) -> (tensor<3xf64>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values element type to be the same as the operand element type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf64>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf64>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C3
+// CHECK-LABEL: func @dynamic_top_k_error_values_last_dim_too_large
+func.func @dynamic_top_k_error_values_last_dim_too_large(%arg0: tensor<16xf32>) -> (tensor<17xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values last dimension to have size at least as large as operand last dimension}}
+  %k = stablehlo.constant dense<17> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<17xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<17xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C4
+// CHECK-LABEL: func @dynamic_top_k_error_indices_shape_mismatch
+func.func @dynamic_top_k_error_indices_shape_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<4xi32>) {
+  // expected-error@+2{{expects the indices shape to match the values shape}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<4xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<4xi32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/stablehlo_refine_shapes.mlir
@@ -607,12 +607,55 @@
 
 // -----
 
+// CHECK-LABEL: @main
+func.func @main(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<*xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window{{.*}} -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<*xf32>
+  func.return %5 : tensor<*xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
 // CHECK-LABEL: @refine_dynamic_reshape
 func.func @refine_dynamic_reshape(%arg0: tensor<4xf32>) -> tensor<*xf32> {
   // CHECK: stablehlo.dynamic_reshape{{.*}} -> tensor<1x4xf32>
   %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
   %1 = stablehlo.dynamic_reshape %arg0, %0 : (tensor<4xf32>, tensor<2xi64>) -> tensor<*xf32>
   func.return %1 : tensor<*xf32>
+}
+
+// -----
+
+// CHECK-LABEL: @refine_dynamic_rng_bit_generator
+func.func @refine_dynamic_rng_bit_generator(%arg0: tensor<2xui64>) -> (tensor<?xui64>, tensor<*xf32>) {
+  // CHECK: stablehlo.dynamic_rng_bit_generator{{.*}} -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<?xui64>, tensor<*xf32>)
+  func.return %1#0, %1#1 : tensor<?xui64>, tensor<*xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_dynamic_top_k
+func.func @refine_dynamic_top_k(%arg0: tensor<16xf32>) -> (tensor<?xf32>, tensor<?xi32>) {
+  // CHECK: stablehlo.dynamic_top_k{{.*}} -> (tensor<4xf32>, tensor<4xi32>)
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<?xf32>, tensor<?xi32>)
+  return %1#0, %1#1 : tensor<?xf32>, tensor<?xi32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/transforms/Passes.td b/stablehlo/stablehlo/transforms/Passes.td
--- stablehlo/stablehlo/transforms/Passes.td
+++ stablehlo/stablehlo/transforms/Passes.td
@@ -25,6 +25,7 @@
     For example, if the output_shape operand of DynamicReshapeOp is a constant
     value, then the operation can be transformed to ReshapeOp.
   }];
+  let dependentDialects = ["mlir::chlo::ChloDialect"];
 }
 
 def StablehloLegalizeToVhloPass : Pass<"stablehlo-legalize-to-vhlo", "ModuleOp"> {
diff --ruN a/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
@@ -24,6 +24,8 @@
 #include "mlir/Interfaces/InferTypeOpInterface.h"
 #include "mlir/Support/LogicalResult.h"
 #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/ExperimentalOps.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/transforms/Passes.h"
 
@@ -198,6 +200,54 @@
   }
 };
 
+struct CanonicalizeDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // ReduceWindowOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op, "expected static window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op, "expected static base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected static padding");
+    auto newOp = rewriter.create<ReduceWindowOp>(
+        op->getLoc(), op->getResultTypes(), op.getInputs(), op.getInitValues(),
+        rewriter.getI64TensorAttr(windowDimensions),
+        rewriter.getI64TensorAttr(windowStrides),
+        rewriter.getI64TensorAttr(baseDilations),
+        rewriter.getI64TensorAttr(windowDilations),
+        hlo::getPaddingAttr(&rewriter, padding));
+
+    // Inline the called computation into newOp.
+    // This is somewhat annoying because we also have to rewrite the original
+    // func::ReturnOp into stablehlo::ReturnOp.
+    rewriter.cloneRegionBefore(op.getBody(), newOp.getBody(),
+                               newOp.getBody().end());
+    auto funcReturnOp =
+        cast<func::ReturnOp>(newOp.getBody().front().getTerminator());
+    rewriter.setInsertionPointToEnd(&newOp.getBody().front());
+    rewriter.replaceOpWithNewOp<stablehlo::ReturnOp>(
+        funcReturnOp, funcReturnOp.getOperands());
+    rewriter.replaceOp(op, newOp->getResults());
+    return success();
+  }
+};
+
 struct CanonicalizeDynamicReshapeOpPattern
     : public OpRewritePattern<DynamicReshapeOp> {
   using OpRewritePattern::OpRewritePattern;
@@ -210,6 +260,56 @@
     if (!op.getType().hasStaticShape())
       return rewriter.notifyMatchFailure(op, "expected static result type");
     rewriter.replaceOpWithNewOp<ReshapeOp>(op, op.getType(), op.getOperand());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // This pattern ignores and discards the output_shape operand. We rely on
+    // the verifier to make sure that its value is consistent with result type.
+    if (!succeeded(hlo::matchInts(op.getOutputShape())))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getOutput().getType().cast<ShapedType>().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static output type");
+    rewriter.replaceOpWithNewOp<RngBitGeneratorOp>(
+        op, op->getResultTypes(), op.getRngAlgorithm(), op.getInitialState());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicTopKOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(impl, "expected constant k");
+
+    // We rely on many of the properties checked by verification.
+    auto valuesType = op.getValues().getType().cast<ShapedType>();
+    auto valuesLastDimSize = valuesType.getShape()[valuesType.getRank() - 1];
+    if (hlo::isDynamicDimSize(valuesLastDimSize) ||
+        valuesLastDimSize != k[0])
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected value of k to match the values last dimension size of "
+          "static values type (result #0)");
+
+    rewriter.replaceOpWithNewOp<chlo::TopKOp>(
+        op, op->getResultTypes(), op.getOperand(), k[0]);
     return success();
   }
 };
@@ -320,7 +420,10 @@
     patterns.add<CanonicalizeDynamicGatherOpPattern>(&getContext());
     patterns.add<CanonicalizeDynamicIotaOpPattern>(&getContext());
     patterns.add<CanonicalizeDynamicPadOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicReduceWindowOpPattern>(&getContext());
     patterns.add<CanonicalizeDynamicReshapeOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicTopKOpPattern>(&getContext());
     patterns.add<CanonicalizeRealDynamicSliceOpToDynamicSliceOpPattern>(
         &getContext());
     patterns.add<CanonicalizeRealDynamicSliceOpToSliceOpPattern>(&getContext());
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -43,6 +43,7 @@
 #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
 #include "stablehlo/dialect/Base.h"
 #include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/ExperimentalOps.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/dialect/TypeInference.h"
 #include "stablehlo/transforms/Passes.h"
@@ -844,12 +845,97 @@
   }
 };
 
+struct RefineDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected constant padding");
+
+    SmallVector<ShapedTypeComponents> inferredReturnTypes;
+    if (failed(hlo::inferReduceWindowOp(
+            /*location=*/{}, op.getInputs(), op.getInitValues(),
+            rewriter.getI64TensorAttr(windowDimensions),
+            rewriter.getI64TensorAttr(windowStrides),
+            rewriter.getI64TensorAttr(baseDilations),
+            rewriter.getI64TensorAttr(windowDilations),
+            hlo::getPaddingAttr(&rewriter, padding), inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferReduceWindowOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
 struct RefineDynamicReshapeOpPattern
     : public OpRewritePattern<DynamicReshapeOp> {
   using OpRewritePattern::OpRewritePattern;
   LogicalResult matchAndRewrite(DynamicReshapeOp op,
                                 PatternRewriter& rewriter) const override {
     return refineReturnShape(rewriter, op, op.getOutputShape());
+  }
+};
+
+struct RefineDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    auto initialStateType = op.getInitialState().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape;
+    if (failed(hlo::matchInts(op.getOutputShape(), outputShape)))
+      return rewriter.notifyMatchFailure(op, "expected constant output_shape");
+
+    // We only need to refine the shape of `output` (the second result).
+    // The shape of `output_state` (the first result) is determined by the shape
+    // of `initial_state`, so we ignore it and provide an empty refinement.
+    return refineReturnTypes(rewriter, op, {{initialStateType}, {outputShape}});
+  }
+};
+
+struct RefineDynamicTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(op, "expected constant k");
+
+    outputShape[operandType.getRank() - 1] = k[0];
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
   }
 };
 
@@ -1181,7 +1267,10 @@
     patterns.add<RefineDynamicConvOpPattern>(&getContext());
     patterns.add<RefineDynamicIotaOpPattern>(&getContext());
     patterns.add<RefineDynamicPadOpPattern>(&getContext());
+    patterns.add<RefineDynamicReduceWindowOpPattern>(&getContext());
     patterns.add<RefineDynamicReshapeOpPattern>(&getContext());
+    patterns.add<RefineDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<RefineDynamicTopKOpPattern>(&getContext());
     patterns.add<RefineInferTypeOpInterfacePattern>(&getContext());
     patterns.add<RefineRealDynamicSliceOpPattern>(&getContext());
     patterns.add<RefineReduceScatterOpPattern>(&getContext());

