diff --ruN a/stablehlo/CMakeLists.txt b/stablehlo/CMakeLists.txt
--- stablehlo/CMakeLists.txt
+++ stablehlo/CMakeLists.txt
@@ -13,131 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-cmake_minimum_required(VERSION 3.15.0)
 
-if(POLICY CMP0068)
-  cmake_policy(SET CMP0068 NEW)
-  set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)
-endif()
-
-if(POLICY CMP0075)
-  cmake_policy(SET CMP0075 NEW)
-endif()
-
-if(POLICY CMP0077)
-  cmake_policy(SET CMP0077 NEW)
-endif()
-
-# CMP0116: Ninja generators transform `DEPFILE`s from `add_custom_command()`
-# New in CMake 3.20. https://cmake.org/cmake/help/latest/policy/CMP0116.html
-if(POLICY CMP0116)
-  cmake_policy(SET CMP0116 OLD)
-endif()
+# This build of StableHLO is meant to be embedded in MLIR-HLO.
+# As a result, its root CMakeLists.txt is different from the original
+# CMakeLists.txt from https://github.com/openxla/stablehlo.
+# All other files of this build of StableHLO except for this one are the same
+# as the original files.
+# To get access to a standalone build of StableHLO, check out the
+# openxla/stablehlo repository.
 
 #-------------------------------------------------------------------------------
 # Options and settings
 #-------------------------------------------------------------------------------
-option(STABLEHLO_BUILD_EMBEDDED "Build StableHLO as part of another project" OFF)
-option(STABLEHLO_ENABLE_BINDINGS_PYTHON "Enables StableHLO Python bindings" OFF)
-option(STABLEHLO_ENABLE_STRICT_BUILD "Build StableHLO with strict warnings and warnings as errors" OFF)
 
-#-------------------------------------------------------------------------------
-# Project setup and globals
-#-------------------------------------------------------------------------------
-set(STABLEHLO_EXTERNAL_PROJECT_BUILD OFF)
-
-if(NOT (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR) AND NOT MLIR_BINARY_DIR)
-  # Building as part of LLVM via the external project mechanism.
-  set(STABLEHLO_EXTERNAL_PROJECT_BUILD ON)
-else()
-  # Building standalone.
-  project(stablehlo LANGUAGES CXX C)
-  set(CMAKE_C_STANDARD 11)
-  set(CMAKE_CXX_STANDARD 17)
-endif()
-
-# Build with ccache if the package is present
-set(LLVM_CCACHE_BUILD OFF CACHE BOOL "Set to ON for a ccache enabled build")
-if(LLVM_CCACHE_BUILD)
-  find_program(CCACHE_PROGRAM ccache)
-  if(CCACHE_PROGRAM)
-      set(LLVM_CCACHE_MAXSIZE "" CACHE STRING "Size of ccache")
-      set(LLVM_CCACHE_DIR "" CACHE STRING "Directory to keep ccached data")
-      set(LLVM_CCACHE_PARAMS "CCACHE_CPP2=yes CCACHE_HASHDIR=yes"
-          CACHE STRING "Parameters to pass through to ccache")
-
-      set(CCACHE_PROGRAM "${LLVM_CCACHE_PARAMS} ${CCACHE_PROGRAM}")
-      if (LLVM_CCACHE_MAXSIZE)
-        set(CCACHE_PROGRAM "CCACHE_MAXSIZE=${LLVM_CCACHE_MAXSIZE} ${CCACHE_PROGRAM}")
-      endif()
-      if (LLVM_CCACHE_DIR)
-        set(CCACHE_PROGRAM "CCACHE_DIR=${LLVM_CCACHE_DIR} ${CCACHE_PROGRAM}")
-      endif()
-      set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM})
-  else()
-    message(FATAL_ERROR "Unable to find the program ccache. Set LLVM_CCACHE_BUILD to OFF")
-  endif()
-endif()
-
-#-------------------------------------------------------------------------------
-# MLIR/LLVM Configuration
-#-------------------------------------------------------------------------------
-if (STABLEHLO_ENABLE_STRICT_BUILD)
-  set(LLVM_ENABLE_WARNINGS ON)
-  set(LLVM_ENABLE_WERROR ON)
-  set(LLVM_ENABLE_PEDANTIC ON)
-endif()
-
-# Find MLIR to install if we are building standalone. If building as part of
-# another project, let it handle the MLIR dependency. The dependent project
-# might use a bundled version of MLIR instead of installing, for instance.
-if(STABLEHLO_EXTERNAL_PROJECT_BUILD)
-  message(STATUS "Building StableHLO as an external LLVM project")
-  set(MLIR_MAIN_SRC_DIR ${LLVM_MAIN_SRC_DIR}/../mlir ) # --src-root
-  set(MLIR_INCLUDE_DIR ${MLIR_MAIN_SRC_DIR}/include ) # --includedir
-  set(MLIR_GENERATED_INCLUDE_DIR ${LLVM_BINARY_DIR}/tools/mlir/include)
-  include_directories(SYSTEM ${MLIR_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_GENERATED_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_TABLEGEN_OUTPUT_DIR})
-
-  set(BACKEND_PACKAGE_STRING "${PACKAGE_STRING}")
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_MAIN_SRC_DIR}/cmake/modules")
-elseif(NOT STABLEHLO_BUILD_EMBEDDED)
-  message(STATUS "Building StableHLO with an installed MLIR")
-  find_package(MLIR REQUIRED CONFIG)
-  message(STATUS "Using MLIRConfig.cmake in: ${MLIR_DIR}")
-  message(STATUS "Using LLVMConfig.cmake in: ${LLVM_DIR}")
-  set(LLVM_RUNTIME_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/bin)
-  set(LLVM_LIBRARY_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/lib)
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_CMAKE_DIR}")
-  list(APPEND CMAKE_MODULE_PATH "${LLVM_CMAKE_DIR}")
-else()
-  message(STATUS "Building StableHLO embedded in another project")
-endif()
-
-if(LLVM_ENABLE_ZLIB)
-  find_package(ZLIB)
-endif()
-
-include(TableGen)
-include(AddLLVM)
-include(AddMLIR)
-include(HandleLLVMOptions)
-include_directories(${LLVM_INCLUDE_DIRS})
-include_directories(${MLIR_INCLUDE_DIRS})
-include_directories(${CMAKE_CURRENT_SOURCE_DIR})
-include_directories(${CMAKE_CURRENT_BINARY_DIR})
-link_directories(${LLVM_BUILD_LIBRARY_DIR})
-add_definitions(${LLVM_DEFINITIONS})
-
-#-------------------------------------------------------------------------------
-# Python configuration
-#-------------------------------------------------------------------------------
-
-if(STABLEHLO_ENABLE_BINDINGS_PYTHON)
-  include(MLIRDetectPythonEnv)
-  mlir_configure_python_dev_packages()
-endif()
+set(STABLEHLO_ENABLE_BINDINGS_PYTHON ${MHLO_ENABLE_BINDINGS_PYTHON})
 
 #-------------------------------------------------------------------------------
 # Directory setup
diff --ruN a/stablehlo/stablehlo/CMakeLists.txt b/stablehlo/stablehlo/CMakeLists.txt
--- stablehlo/stablehlo/CMakeLists.txt
+++ stablehlo/stablehlo/CMakeLists.txt
@@ -15,6 +15,7 @@
 add_subdirectory(api)
 add_subdirectory(conversions)
 add_subdirectory(dialect)
+add_subdirectory(experimental)
 add_subdirectory(integrations)
 add_subdirectory(reference)
 add_subdirectory(tests)
diff --ruN a/stablehlo/stablehlo/api/PortableApi.h b/stablehlo/stablehlo/api/PortableApi.h
--- stablehlo/stablehlo/api/PortableApi.h
+++ stablehlo/stablehlo/api/PortableApi.h
@@ -27,7 +27,7 @@
 
 /// Return the current version for portable API.
 /// Increments on all meaningful changes to this file.
-inline int64_t getApiVersion() { return 5; }
+inline int64_t getApiVersion() { return 6; }
 
 // Get the current StableHLO version.
 //
diff --ruN a/stablehlo/stablehlo/experimental/BUILD.bazel b/stablehlo/stablehlo/experimental/BUILD.bazel
--- stablehlo/stablehlo/experimental/BUILD.bazel
+++ stablehlo/stablehlo/experimental/BUILD.bazel
@@ -0,0 +1,114 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+cc_library(
+    name = "experimental_base",
+    srcs = [
+        "dialect/Base.cpp",
+    ],
+    hdrs = [
+        "dialect/Base.h",
+    ],
+    deps = [
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+    ],
+)
+
+cc_library(
+    name = "experimental_stablehlo_ops",
+    srcs = [
+        "dialect/StablehloOps.cpp",
+    ],
+    hdrs = [
+        "dialect/StablehloOps.h",
+    ],
+    deps = [
+        ":experimental_base",
+        "//:stablehlo_ops",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+gentbl_cc_library(
+    name = "experimental_stablehlo_pass_inc_gen",
+    tbl_outs = [
+        (
+            [
+                "-gen-pass-decls",
+            ],
+            "transforms/Passes.h.inc",
+        ),
+    ],
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "transforms/Passes.td",
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+cc_library(
+    name = "experimental_stablehlo_passes",
+    srcs = [
+        "transforms/StablehloCanonicalizeDynamism.cpp",
+        "transforms/StablehloRefineShapes.cpp",
+    ],
+    hdrs = [
+        "transforms/Passes.h",
+    ],
+    deps = [
+        ":experimental_stablehlo_ops",
+        ":experimental_stablehlo_pass_inc_gen",
+        "//:base",
+        "//:chlo_ops",
+        "//:stablehlo_ops",
+        "//:stablehlo_ops_inc_gen",
+        "//:stablehlo_passes",
+        "//:stablehlo_type_inference",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:InferTypeOpInterface",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+    ],
+)
+
+cc_binary(
+    name = "experimental-stablehlo-opt",
+    srcs = [
+        "tools/StablehloOptMain.cpp",
+    ],
+    deps = [
+        ":experimental_stablehlo_passes",
+        "//:interpreter_ops",
+        "//:register",
+        "//:stablehlo_passes",
+        "//:test_utils",
+        "//:tosa_passes",
+        "@llvm-project//mlir:AllExtensions",
+        "@llvm-project//mlir:AllPassesAndDialects",
+        "@llvm-project//mlir:MlirOptLib",
+        "@llvm-project//mlir:TosaDialect",
+    ],
+)
diff --ruN a/stablehlo/stablehlo/experimental/CMakeLists.txt b/stablehlo/stablehlo/experimental/CMakeLists.txt
--- stablehlo/stablehlo/experimental/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/CMakeLists.txt
@@ -0,0 +1,18 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_subdirectory(dialect)
+add_subdirectory(tests)
+add_subdirectory(tools)
+add_subdirectory(transforms)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.cpp b/stablehlo/stablehlo/experimental/dialect/Base.cpp
--- stablehlo/stablehlo/experimental/dialect/Base.cpp
+++ stablehlo/stablehlo/experimental/dialect/Base.cpp
@@ -0,0 +1,39 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/Base.h"
+
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext* context,
+                                    ArrayRef<int64_t> values) {
+  return DenseIntElementsAttr::get(
+      RankedTensorType::get({static_cast<int64_t>(values.size()) / 2, 2},
+                            IntegerType::get(context, 64)),
+      values);
+}
+
+DenseIntElementsAttr getPaddingAttr(Builder* builder,
+                                    ArrayRef<int64_t> values) {
+  return getPaddingAttr(builder->getContext(), values);
+}
+
+}  // namespace hlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.h b/stablehlo/stablehlo/experimental/dialect/Base.h
--- stablehlo/stablehlo/experimental/dialect/Base.h
+++ stablehlo/stablehlo/experimental/dialect/Base.h
@@ -0,0 +1,35 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+
+#include "llvm/ADT/ArrayRef.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext *context,
+                                    ArrayRef<int64_t> value);
+DenseIntElementsAttr getPaddingAttr(Builder *builder, ArrayRef<int64_t> value);
+
+}  // namespace hlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
diff --ruN a/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt b/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
--- stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
@@ -0,0 +1,42 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_mlir_library(ExperimentalStablehloBase
+  PARTIAL_SOURCES_INTENDED
+  Base.cpp
+
+  LINK_LIBS PUBLIC
+  MLIRIR
+)
+
+add_mlir_dialect_library(ExperimentalStablehloOps
+  PARTIAL_SOURCES_INTENDED
+  StablehloOps.cpp
+
+  DEPENDS
+  StablehloOpsIncGen
+
+  LINK_LIBS PUBLIC
+  ExperimentalStablehloBase
+  MLIRFuncDialect
+  MLIRIR
+  MLIRSupport
+  StablehloOps
+)
+
+target_include_directories(ExperimentalStablehloOps INTERFACE
+  $<BUILD_INTERFACE:${STABLEHLO_SOURCE_DIR}>
+  $<BUILD_INTERFACE:${STABLEHLO_BINARY_DIR}>
+)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp b/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
@@ -0,0 +1,641 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+
+#include <cstdint>
+#include <optional>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/Types.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+LogicalResult DynamicReduceWindowOpAdaptor::verify() {
+  // Before checking the constraints inherited from ReduceWindowOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2 * op_->getNumResults() + 5)
+    return op_.emitError("expects size(operands) = 2 * size(results) + 5");
+  if (op_->getNumResults() == 0)
+    return op_.emitError("expects size(results) > 0");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_reduce_window".
+    // called_computations carries the body.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "called_computations")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_reduce_window")
+    return op_.emitError() << "expects @stablehlo.dynamic_reduce_window";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto numInputs = getInputs().size();
+  auto inputs = op_.getInputs().slice(0, numInputs);
+  auto initValues = op_.getInputs().slice(numInputs, numInputs);
+  auto windowDimensions = op_.getInputs()[op_.getInputs().size() - 5];
+  auto windowStrides = op_.getInputs()[op_.getInputs().size() - 4];
+  auto baseDilations = op_.getInputs()[op_.getInputs().size() - 3];
+  auto windowDilations = op_.getInputs()[op_.getInputs().size() - 2];
+  auto padding = op_.getInputs()[op_.getInputs().size() - 1];
+  auto results = op_.getResults();
+
+  // reduce_window_c1
+  // This constraint hold automatically thanks to the checks that we have
+  // performed above.
+
+  // reduce_window_i1
+  SmallVector<ShapedType> inputTypes;
+  for (auto [index, input] : llvm::enumerate(inputs)) {
+    auto inputType = input.getType().dyn_cast<ShapedType>();
+    inputTypes.push_back(inputType);
+    if (!inputType)
+      return op_.emitError()
+             << "expects inputs (e.g. operand #" << index << ") to be tensors";
+  }
+
+  // reduce_window_i2
+  SmallVector<ShapedType> initValueTypes;
+  for (auto [index, initValue] : llvm::enumerate(initValues)) {
+    auto initValueType = initValue.getType().dyn_cast<ShapedType>();
+    initValueTypes.push_back(initValueType);
+    if (!initValueType || !initValueType.hasRank() ||
+        initValueType.getRank() != 0)
+      return op_.emitError() << "expects init_values (e.g. operand #"
+                             << numInputs + index << ") "
+                             << "to be 0-dimensional tensors";
+  }
+
+  // reduce_window_i3...reduce_window_i7
+  auto checkRank = [&](StringRef name, int64_t index, Value dynamicAttr,
+                       int64_t expectedRank) -> LogicalResult {
+    auto type = dynamicAttr.getType().dyn_cast<ShapedType>();
+    if (!type || !type.hasRank() || type.getRank() != expectedRank ||
+        !type.getElementType().isIntOrIndex()) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to be a " << expectedRank << "-dimensional tensor "
+             << "of integer or index type";
+    }
+    return success();
+  };
+  if (failed(checkRank("window_dimensions", -5, windowDimensions, 1)) ||
+      failed(checkRank("window_strides", -4, windowStrides, 1)) ||
+      failed(checkRank("base_dilations", -3, baseDilations, 1)) ||
+      failed(checkRank("window_dilations", -2, windowDilations, 1)) ||
+      failed(checkRank("padding", -1, padding, 2)))
+    return failure();
+
+  // reduce_window_i7
+  auto paddingType = getPadding().getType().dyn_cast<ShapedType>();
+  if (!paddingType || !paddingType.hasRank() || paddingType.getRank() != 2 ||
+      paddingType.getDimSize(1) != 2 ||
+      !paddingType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects padding_type (operand #" << op_.getNumOperands() - 1
+           << ") to be a 2-dimensional tensor of integer or index type";
+
+  // reduce_window_c2
+  std::optional<ArrayRef<int64_t>> inputShape;
+  for (auto inputType : inputTypes) {
+    if (!inputType.hasRank()) continue;
+    if (!inputShape) inputShape = inputType.getShape();
+    if (failed(verifyCompatibleShape(inputType.getShape(), *inputShape)))
+      return op_.emitError() << "expects all inputs (operands 0.." << numInputs
+                             << ") to have compatible shapes";
+  }
+
+  // reduce_window_c3
+  for (auto [inputType, initValueType] :
+       llvm::zip(inputTypes, initValueTypes)) {
+    if (inputType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects inputs (operands 0.." << numInputs
+                             << ") and init_values (operands " << numInputs
+                             << ".." << numInputs * 2 << ") to have pairwise "
+                             << "the same element types";
+  }
+
+  // reduce_window_c4...reduce_window_c12
+  // In this range, we only verify the constraints with even numbers.
+  // Verifying the constraints with odd numbers would require knowing the
+  // actual values of window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+  auto checkShape = [&](StringRef name, int64_t index, Value dynamicAttr,
+                        ArrayRef<int64_t> expectedShape) -> LogicalResult {
+    auto type = dynamicAttr.getType().cast<ShapedType>();
+    if (type.getShape() != expectedShape) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to have shape [" << expectedShape << "]";
+    }
+    return success();
+  };
+  if (inputShape) {
+    auto inputRank = static_cast<int64_t>(inputShape->size());
+    if (failed(checkShape("window_dimensions", -5, windowDimensions,
+                          {inputRank})) ||
+        failed(checkShape("window_strides", -4, windowStrides, {inputRank})) ||
+        failed(checkShape("base_dilations", -3, baseDilations, {inputRank})) ||
+        failed(
+            checkShape("window_dilations", -2, windowDilations, {inputRank})) ||
+        failed(checkShape("padding", -1, padding, {inputRank, 2})))
+      return failure();
+  }
+
+  // reduce_window_c13
+  if (op_.getCalledComputations().size() != 1)
+    return op_.emitError() << "expects called_computations to have 1 element";
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  if (!bodyFunc)
+    return op_.emitError() << "expects called_computations to refer to "
+                           << "a function that exists within a parent module";
+
+  // reduce_window_c13
+  SmallVector<Type> expectedBodyInputs;
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  SmallVector<Type> expectedBodyOutputs;
+  llvm::append_range(expectedBodyOutputs, initValueTypes);
+  auto expectedBodyType = FunctionType::get(
+      op_.getContext(), expectedBodyInputs, expectedBodyOutputs);
+  if (bodyFunc.getFunctionType() != expectedBodyType)
+    return op_.emitError() << "expects body to have type " << expectedBodyType;
+
+  // reduce_window_c14
+  SmallVector<ShapedType> resultTypes;
+  std::optional<ArrayRef<int64_t>> resultShape;
+  for (auto result : results) {
+    auto resultType = result.getType().dyn_cast<ShapedType>();
+    resultTypes.push_back(resultType);
+    if (!resultType) return op_.emitError() << "expects results to be tensors";
+
+    if (!resultType.hasRank()) continue;
+    if (!resultShape) resultShape = resultType.getShape();
+    if (failed(verifyCompatibleShape(resultType.getShape(), *resultShape)))
+      return op_.emitError() << "expects all results to have compatible shapes";
+  }
+
+  // reduce_window_c15
+  // Verifying this constraint would require knowing the actual values of
+  // window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+
+  // reduce_window_c16
+  for (auto [resultType, initValueType] :
+       llvm::zip(resultTypes, initValueTypes)) {
+    if (resultType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects results and init_values (operands "
+                             << numInputs << ".." << numInputs * 2 << ") "
+                             << "to have pairwise the same element types";
+  }
+
+  return success();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInputs() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(0, numInputs);
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInitValues() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(numInputs, numInputs);
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDimensions() {
+  return op_.getInputs()[op_.getInputs().size() - 5]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowStrides() {
+  return op_.getInputs()[op_.getInputs().size() - 4]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getBaseDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 3]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 2]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getPadding() {
+  return op_.getInputs()[op_.getInputs().size() - 1]
+      .cast<TypedValue<ShapedType>>();
+}
+
+Region& DynamicReduceWindowOpAdaptor::getBody() {
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  return bodyFunc.getBody();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getResults() {
+  return op_.getResults();
+}
+
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_reduce_window") return {};
+  return DynamicReduceWindowOpAdaptor(op);
+}
+
+LogicalResult DynamicRngBitGeneratorOpAdaptor::verify() {
+  // Before checking the constraints inherited from RngBitGeneratorOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_rng_bit_generator".
+    // rng_algorithm comes from the operation.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "rng_algorithm")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return op_.emitError() << "expects @stablehlo.dynamic_rng_bit_generator";
+  if (!op_->hasAttr("rng_algorithm"))
+    return op_.emitError() << "expects an rng_algorithm";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto rngAlgorithmAttr = op_->getAttr("rng_algorithm");
+  auto initialState = op_.getInputs()[0];
+  auto outputShape = op_.getInputs()[1];
+  auto outputState = op_.getResults()[0];
+  auto output = op_.getResults()[1];
+
+  // dynamic_rng_bit_generator_i1
+  if (!rngAlgorithmAttr.isa<RngAlgorithmAttr>())
+    return op_.emitError()
+           << "expects a #stablehlo<rng_algorithm ...> rng_algorithm";
+
+  // dynamic_rng_bit_generator_i2
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto initialStateType = initialState.getType().dyn_cast<ShapedType>();
+  if (!initialStateType || !initialStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects initial_state (operand #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_i3
+  auto outputShapeType = outputShape.getType().dyn_cast<ShapedType>();
+  if (!outputShapeType || !outputShapeType.hasRank() ||
+      outputShapeType.getRank() != 1 ||
+      !outputShapeType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects output_shape (operand #1) "
+           << "to be a 1-dimensional tensor of integer or index type";
+
+  // dynamic_rng_bit_generator_o1
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto outputStateType = outputState.getType().dyn_cast<ShapedType>();
+  if (!outputStateType || !outputStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output_state (result #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_o2
+  auto outputType = output.getType().dyn_cast<ShapedType>();
+  if (!outputType || !outputType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output (result #1) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_c1
+  if (!hlo::isCompatibleForHloTypeInference(initialStateType, outputStateType))
+    return op_.emitError()
+           << "expects initial_state (operand #0) and output_state (result #0) "
+           << "to have compatible shapes";
+
+  // dynamic_rng_bit_generator_c2
+  // TODO(#486): Verify rng_algorithm in RngBitGeneratorOp.
+
+  // dynamic_rng_bit_generator_c3
+  if (!hlo::isCompatibleForHloTypeInference(outputShape, outputType))
+    return op_.emitError() << "expects output (result #1) to have shape  "
+                           << "compatible with output_shape (operand #2)";
+
+  return success();
+}
+
+RngAlgorithm DynamicRngBitGeneratorOpAdaptor::getRngAlgorithm() {
+  return op_->getAttr("rng_algorithm").cast<RngAlgorithmAttr>().getValue();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getInitialState() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputShape() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputState() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutput() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return {};
+  return DynamicRngBitGeneratorOpAdaptor(op);
+}
+
+LogicalResult DynamicTopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_top_k".
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_top_k")
+    return op_.emitError() << "expects @stablehlo.dynamic_top_k";
+
+  auto operand = op_.getInputs()[0];
+  auto k = op_.getInputs()[1];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+
+  // dynamic_top_k_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_i2
+  auto kType = k.getType().dyn_cast<ShapedType>();
+  if (!kType || !kType.hasRank() || kType.getRank() != 0 ||
+      !kType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects k (operand #1) "
+           << "to be a 0-dimensional tensor of integer or index type";
+
+  // dynamic_top_k_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // dynamic_top_k_c1
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] =
+      valuesType.getDimSize(valuesType.getRank() - 1);
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension";
+
+  // dynamic_top_k_c2
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // dynamic_top_k_c3
+  if (!operandType.isDynamicDim(operandLastDim) &&
+      !valuesType.isDynamicDim(operandLastDim) &&
+      operandType.getDimSize(operandLastDim) <
+          valuesType.getDimSize(operandLastDim))
+    return op_.emitError() << "expects the values last dimension to have size "
+                              "at least as large "
+                           << "as operand last dimension";
+
+  // dynamic_top_k_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getK() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_top_k") return {};
+  return DynamicTopKOpAdaptor(op);
+}
+
+LogicalResult TopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 1)
+    return op_.emitError("expects size(operands) = 1");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "mhlo.topk")
+    return op_.emitError() << "expects @mhlo.topk";
+
+  auto operand = op_.getInputs()[0];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+  DictionaryAttr topkAttributes =
+      op_->getAttrOfType<DictionaryAttr>("mhlo.attributes");
+  if (!topkAttributes) {
+    return op_.emitError()
+           << "mhlo.attributes missing or not a dictionary attribute";
+  }
+
+  IntegerAttr k_attr = topkAttributes.get("k").dyn_cast_or_null<IntegerAttr>();
+  if (!k_attr) {
+    return op_.emitError() << "mhlo.attributes.k not present or not an integer";
+  }
+  int64_t k = k_attr.getInt();
+
+  // mhlo.topk_c5
+  if (k < 0) return op_.emitError() << "expects k >= 0";
+
+  // mhlo.topk_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // mhlo.topk_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // mhlo.topk_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // mhlo.topk_c1 && mhlo.topk_c2
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] = k;
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension, and "
+                              "that the last dimension of the values shape "
+                              "has a size k";
+
+  // mhlo.topk_c3
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // mhlo.topk_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> TopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> TopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> TopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+int64_t TopKOpAdaptor::getK() {
+  DictionaryAttr topkAttributes =
+      op_->getAttrOfType<DictionaryAttr>("mhlo.attributes");
+  return topkAttributes.get("k").cast<mlir::IntegerAttr>().getInt();
+}
+
+bool TopKOpAdaptor::getLargest() {
+  DictionaryAttr topkAttributes =
+      op_->getAttrOfType<DictionaryAttr>("mhlo.attributes");
+  IntegerAttr largest =
+      topkAttributes.get("largest").dyn_cast_or_null<mlir::IntegerAttr>();
+
+  return (!largest) ? true : largest.getInt();
+}
+
+std::optional<TopKOpAdaptor> getTopKOp(CustomCallOp op) {
+  if (op.getCallTargetName() != "mhlo.topk") return {};
+  return TopKOpAdaptor(op);
+}
+
+LogicalResult TanOpAdaptor::verify() {
+  if (op_->getNumOperands() != 1)
+    return op_.emitError("expects size(operands) = 1");
+  if (op_->getNumResults() != 1)
+    return op_.emitError("expects size(results) = 1");
+  auto operand = op_.getInputs()[0];
+  auto result = op_.getResults()[0];
+
+  // tan_c1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  auto resultType = result.getType().dyn_cast<ShapedType>();
+  if (!hlo::isCompatibleForHloTypeInference(operandType, resultType))
+    return op_.emitError()
+           << "expects operand and result to have compatible shapes";
+  return success();
+}
+
+TypedValue<ShapedType> TanOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<TanOpAdaptor> getTanOp(CustomCallOp op) {
+  if (op.getCallTargetName() != "mhlo.tan") return {};
+  return TanOpAdaptor(op);
+}
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.h b/stablehlo/stablehlo/experimental/dialect/StablehloOps.h
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.h
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.h
@@ -0,0 +1,347 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+
+// This file supports XLA-specific experiments with the StableHLO opset.
+// These experiments are not yet ready to be upstreamed to openxla/stablehlo
+// and are incubating towards the respective StableHLO RFCs.
+//
+// Custom calls (which are the implementation vehicle of these experiments)
+// don't have compatibility guarantees within the StableHLO process, but
+// the StableHLO team at Google provides out-of-band guarantees for these
+// custom calls, with the same compatibility window as StableHLO upstream.
+
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LogicalResult.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/Base.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+// The DynamicReduceWindowOp experiment provides a dynamic version of
+// ReduceWindowOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicReduceWindowOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_reduce_window` custom call.
+// This custom call has the following operands which represent a dynamic version
+// of operands and attributes of ReduceWindowOp:
+//   * [0:N]   => inputs
+//   * [N:2*N] => init_values
+//   * [-5]    => window_dimensions
+//   * [-4]    => window_strides
+//   * [-3]    => base_dilations
+//   * [-2]    => window_dilations
+//   * [-1]    => padding
+// Additionally, to represent the body of DynamicReduceWindowOp, the custom call
+// has a satellite function attached to the custom call via called_computations.
+//
+// Semantics of DynamicReduceWindowOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window
+// with the following exceptions:
+//   1) All tensor constants, i.e. window_dimensions, window_strides,
+//      base_dilations, window_dilations and padding, become tensors of
+//      integer type.
+//   2) As a result, some of the constraints can no longer be validated
+//      statically. However, this operation still expects these constraints
+//      to hold dynamically, and if they don't hold, the behavior is undefined.
+class DynamicReduceWindowOpAdaptor {
+ public:
+  DynamicReduceWindowOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::ReduceWindowOp, except that all the
+  // std::optional<DenseIntElementsAttr> attributes have turned into values.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  ValueRange getInputs();
+  ValueRange getInitValues();
+  TypedValue<ShapedType> getWindowDimensions();
+  TypedValue<ShapedType> getWindowStrides();
+  TypedValue<ShapedType> getBaseDilations();
+  TypedValue<ShapedType> getWindowDilations();
+  TypedValue<ShapedType> getPadding();
+  Region& getBody();
+  ValueRange getResults();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicReduceWindowOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_reduce_window".
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op);
+
+// The DynamicRngBitGeneratorOp experiment provides a dynamic version of
+// RngBitGeneratorOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicRngBitGeneratorOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator` custom call.
+// This custom call has the regular operand of RngBitGeneratorOp plus an
+// additional `output_shape` operand that determines the shape of the output:
+//   * [0] => initial_state
+//   * [1] => output_shape
+//
+// Semantics of DynamicRngBitGeneratorOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator
+// extended with an additional input (I3) and an additional constraint (C3):
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `rng_algorithm` | enum of `DEFAULT`, `THREE_FRY`, and `PHILOX` |
+// | (I2)  | `initial_state` | 1-dimensional tensor of type `ui64`          |
+// | (I3)  | `output_shape`  | 1-dimensional tensor of integer type         |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `output_state` | 1-dimensional tensor of type `ui64`      |
+// | `output`       | tensor of integer or floating-point type |
+//
+// #### Constraints
+//
+// * (C1) `type(initial_state) = type(output_state)`.
+// * (C2) `size(initial_state)` is defined as:
+//   * implementation-defined if `rng_algorithm = DEFAULT`.
+//   * `2` if `rng_algorithm = THREE_FRY`.
+//   * `2` or `3` if `rng_algorithm = PHILOX`.
+// * (C3) `shape(output) = output_shape`.
+class DynamicRngBitGeneratorOpAdaptor {
+ public:
+  DynamicRngBitGeneratorOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::RngBitGeneratorOp, extended with the
+  // additional `output_shape` operand.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  RngAlgorithm getRngAlgorithm();
+  TypedValue<ShapedType> getInitialState();
+  TypedValue<ShapedType> getOutputShape();
+  TypedValue<ShapedType> getOutputState();
+  TypedValue<ShapedType> getOutput();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicRngBitGeneratorOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_rng_bit_generator".
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op);
+
+// The DynamicTopKOp experiment provides a dynamic version of
+// TopKOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicTopKOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_top_k` custom call.
+// This custom call has the regular operand of TopKOp plus an
+// additional `k` operand that determines the shape of the output.
+//
+// Semantics of DynamicTopKOp are inherited from semantics of Chlo.TopKOp.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | 0-dimensional tensor of integer or index type|
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `element_type(values) = element_type(operand)`
+// * (C3) `shape(values)[-1] <= shape(operand)[-1]`
+// * (C4) `shape(indices) = shape(values)`
+class DynamicTopKOpAdaptor {
+ public:
+  DynamicTopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getK();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicTopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_top_k".
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op);
+
+///////////////////
+// MHLO Op Wrappers
+// There are some ops in MHLO which have experimental support in StableHLO
+// programs by representing them as custom_calls with the target `mhlo.op_name`.
+// The level of support of these ops is similar to the other custom_calls in
+// this file. Generally these ops will be added to StableHLO and their
+// experimental support can be deprecated in favor of op's type inference.
+///////////////////
+
+// The TopK experiment provides a StableHLO adapter to MHLO TopKOp.
+// In the future we expect stablehlo.top_k to be added which will use the same
+// refinement rules.
+//
+// Within this experiment, TopKOp is represented via the serialized MHLO
+// `stablehlo.custom_call @mhlo.topk` custom call.
+//
+// The semantics of experimental TopKOp are inherited from the semantics of
+// mhlo.topk.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | constant of type si64                        |
+// | (I3)  | `largest`       | constant of type i1                          |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `shape(values)[-1] = k`
+// * (C3) `element_type(values) = element_type(operand)`
+// * (C4) `shape(indices) = shape(values)`
+// * (C5) `k >= 0`
+//
+class TopKOpAdaptor {
+ public:
+  TopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+  int64_t getK();
+  bool getLargest();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a TopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "mhlo.topk".
+std::optional<TopKOpAdaptor> getTopKOp(CustomCallOp op);
+
+// The TanOp experiment provides a StableHLO adapter to MHLO TanOp.
+// In the future we expect stablehlo.tan to be added which will use the same
+// refinement rules.
+//
+// Within this experiment, TanOp is represented via the serialized MHLO
+// `stablehlo.custom_call @mhlo.tan` custom call. S
+//
+// Semantics of experimental TanOp are inherited from semantics of mhlo.tan.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of floating-point or complex type     |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `result`       | tensor of floating-point or complex type |
+//
+// #### Constraints
+//
+// * (C1) baseline_type(operand) = baseline_type(result)
+//
+class TanOpAdaptor {
+ public:
+  TanOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a TanOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "mhlo.tan".
+std::optional<TanOpAdaptor> getTanOp(CustomCallOp op);
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
diff --ruN a/stablehlo/stablehlo/experimental/tests/BUILD.bazel b/stablehlo/stablehlo/experimental/tests/BUILD.bazel
--- stablehlo/stablehlo/experimental/tests/BUILD.bazel
+++ stablehlo/stablehlo/experimental/tests/BUILD.bazel
@@ -0,0 +1,59 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@bazel_skylib//rules:expand_template.bzl", "expand_template")
+load("@llvm-project//llvm:lit_test.bzl", "lit_test", "package_path")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+# Equivalent of configure_lit_site_cfg from CMakeLists.txt.
+expand_template(
+    name = "lit_site_cfg_py_gen",
+    testonly = True,
+    out = "lit.site.cfg.py",
+    substitutions = {
+        "@LIT_SITE_CFG_IN_HEADER@": "# Autogenerated, do not edit.",
+        "@LLVM_TOOLS_DIR@": package_path("@llvm-project//llvm:BUILD"),
+        "\"@STABLEHLO_TOOLS_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+        "\"@STABLEHLO_SOURCE_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+    },
+    template = "lit.site.cfg.py.in",
+)
+
+# Equivalent of add_lit_testsuite from CMakeLists.txt.
+[
+    lit_test(
+        name = "%s.test" % src,
+        size = "small",
+        srcs = [src],
+        data = [
+            "lit.cfg.py",
+            "lit.site.cfg.py",
+            "//:stablehlo-opt",
+            "//:stablehlo-translate",
+            "//stablehlo/experimental:experimental-stablehlo-opt",
+            "@llvm-project//llvm:FileCheck",
+            "@llvm-project//llvm:not",
+        ] + glob(["%s.bc" % src]),
+        tags = ["stablehlo_tests"],
+    )
+    for src in glob(["**/*.mlir"])
+]
+
+test_suite(
+    name = "experimental_stablehlo_tests",
+    tags = ["experimental_stablehlo_tests"],
+)
diff --ruN a/stablehlo/stablehlo/experimental/tests/CMakeLists.txt b/stablehlo/stablehlo/experimental/tests/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tests/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tests/CMakeLists.txt
@@ -0,0 +1,29 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+configure_lit_site_cfg(
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.site.cfg.py.in
+  ${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg.py
+  MAIN_CONFIG
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.cfg.py
+)
+add_lit_testsuite(check-experimental-stablehlo-tests "Running the experimental/tests/ suite"
+  ${CMAKE_CURRENT_BINARY_DIR}
+  DEPENDS
+  FileCheck
+  experimental-stablehlo-opt
+  stablehlo-translate
+)
+add_dependencies(check-stablehlo-quick check-experimental-stablehlo-tests)
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.cfg.py b/stablehlo/stablehlo/experimental/tests/lit.cfg.py
--- stablehlo/stablehlo/experimental/tests/lit.cfg.py
+++ stablehlo/stablehlo/experimental/tests/lit.cfg.py
@@ -0,0 +1,46 @@
+"""Lit configuration to drive test in this repo."""
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# -*- Python -*-
+# pylint: disable=undefined-variable
+
+import os
+
+import lit.formats
+from lit.llvm import llvm_config
+
+# Populate Lit configuration with the minimal required metadata.
+# Some metadata is populated in lit.site.cfg.py.in.
+config.name = 'STABLEHLO_TESTS_SUITE'
+config.test_format = lit.formats.ShTest(not llvm_config.use_lit_shell)
+config.suffixes = ['.mlir']
+config.test_source_root = os.path.dirname(__file__)
+
+# Disallow reusing variables across CHECK-LABEL matches.
+# A variable can eschew this (be made "global") by prefixing its name with $.
+config.environment['FILECHECK_OPTS'] = '-enable-var-scope'
+
+# Make LLVM and StableHLO tools available in RUN directives
+tools = [
+  'FileCheck',
+  'experimental-stablehlo-opt',
+  'stablehlo-translate',
+  'not',
+]
+tool_dirs = [
+  config.llvm_tools_dir,
+  config.stablehlo_tools_dir,
+]
+llvm_config.add_tool_substitutions(tools, tool_dirs)
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in b/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
--- stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
+++ stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
@@ -0,0 +1,21 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+@LIT_SITE_CFG_IN_HEADER@
+
+import lit.llvm
+lit.llvm.initialize(lit_config, config)
+config.llvm_tools_dir = "@LLVM_TOOLS_DIR@"
+config.stablehlo_tools_dir = "@STABLEHLO_TOOLS_DIR@"
+lit_config.load_config(config, "@STABLEHLO_SOURCE_DIR@" + "/stablehlo/experimental/tests/lit.cfg.py")
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
@@ -0,0 +1,344 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-canonicalize-dynamism --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_static_result_type
+func.func @dynamic_reduce_window_success_static_result_type(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<2x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = array<i64: 2, 1>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = array<i64: 3, 1>,
+  //          CHECK-SAME:   window_dimensions = array<i64: 2, 1>,
+  //          CHECK-SAME:   window_strides = array<i64: 4, 1>
+  //          CHECK-SAME: } : (tensor<3x2xf32>, tensor<f32>) -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %5 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_dynamic_result_type
+func.func @dynamic_reduce_window_success_dynamic_result_type(%arg0: tensor<?x2xf32>, %arg1: tensor<f32>) -> tensor<?x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = array<i64: 2, 1>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = array<i64: 3, 1>,
+  //          CHECK-SAME:   window_dimensions = array<i64: 2, 1>,
+  //          CHECK-SAME:   window_strides = array<i64: 4, 1>
+  //          CHECK-SAME: } : (tensor<?x2xf32>, tensor<f32>) -> tensor<?x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<?x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<?x2xf32>
+  func.return %5 : tensor<?x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_reduce_window.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %arg2, %0, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_strides
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_strides(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %arg2, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_base_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_base_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %arg2, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %arg2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_padding
+func.func @dynamic_reduce_window_inapplicable_dynamic_padding(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2x2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %arg2) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_success
+func.func @dynamic_rng_bit_generator_success(%arg0: tensor<2xui64>) -> tensor<1x4xf32> {
+  // CHECK-NOT: stablehlo.dynamic_rng_bit_generator
+  // CHECK: stablehlo.rng_bit_generator %arg0, algorithm = DEFAULT : (tensor<2xui64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_rng_bit_generator.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape(%arg0: tensor<2xui64>, %arg1: tensor<2xi64>) -> tensor<1x4xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %arg1) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type(%arg0: tensor<2xui64>) -> tensor<?x?xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<?x?xf32>)
+  return %1#1 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_success
+func.func @dynamic_top_k_success(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: chlo.top_k
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_failure_k_mismatch
+func.func @dynamic_top_k_failure_k_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: @stablehlo.dynamic_top_k
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_not_float
+func.func @dynamic_top_k_error_operand_not_float(%arg0: tensor<16xcomplex<f64>>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xcomplex<f64>>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_unranked
+func.func @dynamic_top_k_error_operand_unranked(%arg0: tensor<*xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<*xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_scalar_operand
+func.func @dynamic_top_k_error_scalar_operand(%arg0: tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<f32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_integer
+func.func @dynamic_top_k_error_k_not_integer(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3.> : tensor<f32>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_scalar
+func.func @dynamic_top_k_error_k_not_scalar(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3> : tensor<1xui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<1xui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O1
+// CHECK-LABEL: func @dynamic_top_k_error_values_not_float
+func.func @dynamic_top_k_error_values_not_float(%arg0: tensor<16xf32>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects values (result #0) to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O2
+// CHECK-LABEL: func @dynamic_top_k_error_indices_not_i32
+func.func @dynamic_top_k_error_indices_not_i32(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi64>) {
+  // expected-error@+2{{expects indices (result #1) to be a tensor of si32}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi64>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi64>
+}
+
+// -----
+
+// dynamic_top_k C1
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_rank
+func.func @dynamic_top_k_error_values_bad_rank(%arg0: tensor<16xf32>) -> (tensor<3x4xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values shape to match the operand shape in all but the last dimension}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3x4xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3x4xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C2
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_element_type
+func.func @dynamic_top_k_error_values_bad_element_type(%arg0: tensor<16xf32>) -> (tensor<3xf64>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values element type to be the same as the operand element type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf64>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf64>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C3
+// CHECK-LABEL: func @dynamic_top_k_error_values_last_dim_too_large
+func.func @dynamic_top_k_error_values_last_dim_too_large(%arg0: tensor<16xf32>) -> (tensor<17xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values last dimension to have size at least as large as operand last dimension}}
+  %k = stablehlo.constant dense<17> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<17xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<17xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C4
+// CHECK-LABEL: func @dynamic_top_k_error_indices_shape_mismatch
+func.func @dynamic_top_k_error_indices_shape_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<4xi32>) {
+  // expected-error@+2{{expects the indices shape to match the values shape}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<4xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<4xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
@@ -0,0 +1,196 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-refine-shapes --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: @main
+func.func @main(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<*xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window{{.*}} -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<*xf32>
+  func.return %5 : tensor<*xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: @refine_dynamic_rng_bit_generator
+func.func @refine_dynamic_rng_bit_generator(%arg0: tensor<2xui64>) -> (tensor<?xui64>, tensor<*xf32>) {
+  // CHECK: stablehlo.dynamic_rng_bit_generator{{.*}} -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<?xui64>, tensor<*xf32>)
+  func.return %1#0, %1#1 : tensor<?xui64>, tensor<*xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_dynamic_top_k
+func.func @refine_dynamic_top_k(%arg0: tensor<16xf32>) -> (tensor<?xf32>, tensor<?xi32>) {
+  // CHECK: stablehlo.dynamic_top_k{{.*}} -> (tensor<4xf32>, tensor<4xi32>)
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<?xf32>, tensor<?xi32>)
+  return %1#0, %1#1 : tensor<?xf32>, tensor<?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_topk
+func.func @refine_mhlo_topk(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // CHECK: mhlo.topk{{.*}} -> (tensor<5x4xf32>, tensor<5x4xi32>)
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_too_many_operands
+func.func @refine_mhlo_error_too_many_operands(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects size(operands) = 1}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0, %arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>, tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_too_few_results
+func.func @refine_mhlo_error_too_few_results(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>) {
+  // expected-error@+1{{expects size(results) = 2}}
+  %0 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>)
+  return %0 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_wrong_output_1_type
+func.func @refine_mhlo_error_wrong_output_1_type(%arg0: tensor<5x16xf32>) -> (tensor<f32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects values (result #0) to be a tensor of integer or floating-point type of rank at least 1}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<f32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<f32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_wrong_output_2_type
+func.func @refine_mhlo_error_wrong_output_2_type(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>) {
+  // expected-error@+1{{expects indices (result #1) to be a tensor of si32 of rank at least 1}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xf32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c1_wrong_output_shape
+func.func @refine_mhlo_error_c1_wrong_output_shape(%arg0: tensor<5x16xf32>) -> (tensor<?x?x?xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects the values shape to match the operand}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c2_last_dim_not_k
+func.func @refine_mhlo_error_c2_last_dim_not_k(%arg0: tensor<5x16xf32>) -> (tensor<?x5xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects the values shape to match the operand}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x5xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x5xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c3_wrong_output_type
+func.func @refine_mhlo_error_c3_wrong_output_type(%arg0: tensor<5x16xf32>) -> (tensor<?x?xi32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects the values element type to be the same as the operand element type}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xi32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xi32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c4_outputs_shape_mismatch
+func.func @refine_mhlo_error_c4_outputs_shape_mismatch(%arg0: tensor<5x16xf32>) -> (tensor<?x4xf32>, tensor<?x5xi32>) {
+  // expected-error@+1{{expects the indices shape to match the values shape}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x4xf32>, tensor<?x5xi32>)
+  return %0#0, %0#1 : tensor<?x4xf32>, tensor<?x5xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_mhlo_error_c5_negative_k
+func.func @refine_mhlo_error_c5_negative_k(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // expected-error@+1{{expects k >= 0}}
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = -4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+
+// -----
+
+// CHECK-LABEL: @refine_tan
+func.func @refine_tan(%arg0: tensor<16xf32>) -> tensor<?xf32> {
+  // CHECK: mhlo.tan{{.*}} -> tensor<16xf32>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "mhlo.tan",
+    mhlo.attributes = {},
+    mhlo.version = 1 : i64
+  } : (tensor<16xf32>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// -----
+
+// tan_op I1
+// CHECK-LABEL: func @tan_op_error_too_many_inputs
+func.func @tan_op_error_too_many_inputs(%arg0: tensor<1xf32>) -> (tensor<1xf32>) {
+  // expected-error@+1{{expects size(operands) = 1}}
+  %0 = "stablehlo.custom_call"(%arg0, %arg0) {call_target_name = "mhlo.tan", mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>
+  return %0 : tensor<1xf32>
+}
+
+// -----
+
+// tan_op O1
+// CHECK-LABEL: func @tan_op_error_too_many_outputs
+func.func @tan_op_error_too_many_outputs(%arg0: tensor<1xf32>) -> (tensor<1xf32>) {
+  // expected-error@+1{{expects size(results) = 1}}
+  %0:2 = "stablehlo.custom_call"(%arg0) {call_target_name = "mhlo.tan", mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1xf32>) -> (tensor<1xf32>, tensor<1xf32>)
+  return %0#1 : tensor<1xf32>
+}
+
+// -----
+
+// tan_op C1
+// CHECK-LABEL: func @tan_op_error_values_bad_element_type
+func.func @tan_op_error_values_bad_element_type(%arg0: tensor<1xf32>) -> (tensor<1xi32>) {
+  // expected-error@+1{{expects operand and result to have compatible shapes}}
+  %0 = "stablehlo.custom_call"(%arg0) {call_target_name = "mhlo.tan", mhlo.attributes = {}, mhlo.version = 1 : i64} : (tensor<1xf32>) -> tensor<1xi32>
+  return %0 : tensor<1xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tools/CMakeLists.txt b/stablehlo/stablehlo/experimental/tools/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tools/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tools/CMakeLists.txt
@@ -0,0 +1,41 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_OPTIONAL_SOURCES
+  StablehloOptMain.cpp
+)
+
+# stablehlo-opt
+get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)
+get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)
+get_property(extension_libs GLOBAL PROPERTY MLIR_EXTENSION_LIBS)
+set(LIBS
+        ${dialect_libs}
+        ${conversion_libs}
+        ${extension_libs}
+        ExperimentalStablehloPasses
+        MLIROptLib
+        StablehloRegister
+        StablehloTestUtils
+        StablehloPasses
+        InterpreterOps
+        StablehloTOSATransforms
+        )
+add_llvm_executable(experimental-stablehlo-opt StablehloOptMain.cpp)
+llvm_update_compile_flags(experimental-stablehlo-opt)
+target_link_libraries(experimental-stablehlo-opt PRIVATE ${LIBS})
+
+mlir_check_all_link_libraries(experimental-stablehlo-opt)
+
diff --ruN a/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp b/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
--- stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
+++ stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
@@ -0,0 +1,46 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/Dialect/Tosa/Transforms/Passes.h"
+#include "mlir/InitAllDialects.h"
+#include "mlir/InitAllExtensions.h"
+#include "mlir/InitAllPasses.h"
+#include "mlir/Tools/mlir-opt/MlirOptMain.h"
+#include "stablehlo/conversions/tosa/transforms/Passes.h"
+#include "stablehlo/dialect/Register.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/reference/InterpreterOps.h"
+#include "stablehlo/tests/TestUtils.h"
+#include "stablehlo/transforms/Passes.h"
+
+int main(int argc, char **argv) {
+  mlir::registerAllPasses();
+  mlir::hlo::registerAllTestPasses();
+  mlir::stablehlo::registerPassPipelines();
+  mlir::stablehlo::registerPasses();
+  mlir::stablehlo::experimental::registerPasses();
+  mlir::tosa::registerStablehloLegalizeToTosaPassPass();
+  mlir::tosa::registerStablehloPrepareForTosaPassPass();
+
+  mlir::DialectRegistry registry;
+  mlir::registerAllDialects(registry);
+  mlir::registerAllExtensions(registry);
+  mlir::stablehlo::registerAllDialects(registry);
+  registry.insert<mlir::stablehlo::interpreter::InterpreterDialect>();
+
+  return failed(
+      mlir::MlirOptMain(argc, argv, "Experimental StableHLO optimizer driver\n", registry));
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt b/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
--- stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
@@ -0,0 +1,39 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_TARGET_DEFINITIONS Passes.td)
+mlir_tablegen(Passes.h.inc -gen-pass-decls)
+add_public_tablegen_target(ExperimentalPassesIncGen)
+
+add_mlir_dialect_library(ExperimentalStablehloPasses
+  PARTIAL_SOURCES_INTENDED
+  StablehloCanonicalizeDynamism.cpp
+  StablehloRefineShapes.cpp
+
+  DEPENDS
+  ExperimentalPassesIncGen
+
+  LINK_LIBS PUBLIC
+  ChloOps
+  MLIRFuncDialect
+  MLIRIR
+  MLIRInferTypeOpInterface
+  MLIRSupport
+  MLIRTransformUtils
+  ExperimentalStablehloOps
+  StablehloBase
+  StablehloOps
+  StablehloPasses
+  StablehloTypeInference
+)
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.h b/stablehlo/stablehlo/experimental/transforms/Passes.h
--- stablehlo/stablehlo/experimental/transforms/Passes.h
+++ stablehlo/stablehlo/experimental/transforms/Passes.h
@@ -0,0 +1,37 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+#define STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+
+#include <memory>
+
+#include "mlir/Pass/Pass.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+  
+#define GEN_PASS_DECL_STABLEHLOCANONICALIZEDYNAMISMPASS
+#define GEN_PASS_DECL_STABLEHLOREFINESHAPESPASS
+#define GEN_PASS_REGISTRATION
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.td b/stablehlo/stablehlo/experimental/transforms/Passes.td
--- stablehlo/stablehlo/experimental/transforms/Passes.td
+++ stablehlo/stablehlo/experimental/transforms/Passes.td
@@ -0,0 +1,31 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def StablehloCanonicalizeDynamismPass : Pass<"experimental-stablehlo-canonicalize-dynamism", "func::FuncOp"> {
+  let summary = "(Experimental) Canonicalizes dynamic StableHLO ops into static ops.";
+  let description = [{
+    Experimental version of the --stablehlo-canonicalize-dynamism pass.
+  }];
+  let dependentDialects = ["mlir::chlo::ChloDialect"];
+}
+
+def StablehloRefineShapesPass : Pass<"experimental-stablehlo-refine-shapes", "ModuleOp"> {
+  let summary = "(Experimental) Refines shapes across a StableHLO program.";
+  let description = [{
+    Experimental version of the --stablehlo-refine-shapes pass.
+  }];
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
@@ -0,0 +1,167 @@
+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2023 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOCANONICALIZEDYNAMISMPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+struct CanonicalizeDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // ReduceWindowOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op, "expected static window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op, "expected static base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected static padding");
+    auto newOp = rewriter.create<ReduceWindowOp>(
+        op->getLoc(), op->getResultTypes(), op.getInputs(), op.getInitValues(),
+        rewriter.getDenseI64ArrayAttr(windowDimensions),
+        rewriter.getDenseI64ArrayAttr(windowStrides),
+        rewriter.getDenseI64ArrayAttr(baseDilations),
+        rewriter.getDenseI64ArrayAttr(windowDilations),
+        hlo::getPaddingAttr(&rewriter, padding));
+
+    // Inline the called computation into newOp.
+    // This is somewhat annoying because we also have to rewrite the original
+    // func::ReturnOp into stablehlo::ReturnOp.
+    rewriter.cloneRegionBefore(op.getBody(), newOp.getBody(),
+                               newOp.getBody().end());
+    auto funcReturnOp =
+        cast<func::ReturnOp>(newOp.getBody().front().getTerminator());
+    rewriter.setInsertionPointToEnd(&newOp.getBody().front());
+    rewriter.replaceOpWithNewOp<stablehlo::ReturnOp>(
+        funcReturnOp, funcReturnOp.getOperands());
+    rewriter.replaceOp(op, newOp->getResults());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // This pattern ignores and discards the output_shape operand. We rely on
+    // the verifier to make sure that its value is consistent with result type.
+    if (!succeeded(hlo::matchInts(op.getOutputShape())))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getOutput().getType().cast<ShapedType>().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static output type");
+    rewriter.replaceOpWithNewOp<RngBitGeneratorOp>(
+        op, op->getResultTypes(), op.getRngAlgorithm(), op.getInitialState());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicTopKOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(impl, "expected constant k");
+
+    // We rely on many of the properties checked by verification.
+    auto valuesType = op.getValues().getType().cast<ShapedType>();
+    auto valuesLastDimSize = valuesType.getShape()[valuesType.getRank() - 1];
+    if (hlo::isDynamicDimSize(valuesLastDimSize) ||
+        valuesLastDimSize != k[0])
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected value of k to match the values last dimension size of "
+          "static values type (result #0)");
+
+    rewriter.replaceOpWithNewOp<chlo::TopKOp>(
+        op, op->getResultTypes(), op.getOperand(), k[0]);
+    return success();
+  }
+};
+
+struct StablehloCanonicalizeDynamismPass
+    : public impl::StablehloCanonicalizeDynamismPassBase<
+          StablehloCanonicalizeDynamismPass> {
+  using StablehloCanonicalizeDynamismPassBase::
+      StablehloCanonicalizeDynamismPassBase;
+
+  void runOnOperation() override {
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 2;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    populateStablehloCanonicalizeDynamismPatterns(&patterns, &getContext());
+    patterns.add<CanonicalizeDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicTopKOpPattern>(&getContext());
+    if (failed(applyPatternsAndFoldGreedily(getOperation(), std::move(patterns),
+                                            config))) {
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
@@ -0,0 +1,199 @@
+/* Copyright 2022 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/transforms/StablehloRefineShapes.h"
+
+#include <cstdint>
+
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/Base.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/dialect/TypeInference.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOREFINESHAPESPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+struct RefineDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected constant padding");
+
+    SmallVector<ShapedTypeComponents> inferredReturnTypes;
+    if (failed(hlo::inferReduceWindowOp(
+            /*location=*/{}, op.getInputs(), op.getInitValues(),
+            llvm::to_vector(rewriter.getI64TensorAttr(windowDimensions)
+                                .getValues<int64_t>()),
+            llvm::to_vector(
+                rewriter.getI64TensorAttr(windowStrides).getValues<int64_t>()),
+            llvm::to_vector(
+                rewriter.getI64TensorAttr(baseDilations).getValues<int64_t>()),
+            llvm::to_vector(rewriter.getI64TensorAttr(windowDilations)
+                                .getValues<int64_t>()),
+            hlo::getPaddingAttr(&rewriter, padding), op.getBody(),
+            inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferReduceWindowOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
+struct RefineDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    auto initialStateType = op.getInitialState().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape;
+    if (failed(hlo::matchInts(op.getOutputShape(), outputShape)))
+      return rewriter.notifyMatchFailure(op, "expected constant output_shape");
+
+    // We only need to refine the shape of `output` (the second result).
+    // The shape of `output_state` (the first result) is determined by the shape
+    // of `initial_state`, so we ignore it and provide an empty refinement.
+    return refineReturnTypes(rewriter, op, {{initialStateType}, {outputShape}});
+  }
+};
+
+struct RefineDynamicTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(op, "expected constant k");
+
+    outputShape[operandType.getRank() - 1] = k[0];
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
+  }
+};
+
+struct RefineTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    TopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    outputShape.back() = op.getK();
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
+  }
+};
+
+struct RefineTanOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getTanOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    TanOpAdaptor op = *maybeOp;
+    return refineReturnShape(rewriter, op,
+                             op.getOperand().getType().getShape());
+  }
+};
+
+struct StablehloRefineShapesPass
+    : public impl::StablehloRefineShapesPassBase<StablehloRefineShapesPass> {
+  using StablehloRefineShapesPassBase::StablehloRefineShapesPassBase;
+
+  void runOnOperation() override {
+    auto func = getStablehloRefineShapesTarget(getOperation());
+    if (!func) return signalPassFailure();
+
+    // The algorithm behind this pass consists of a single traversal of the
+    // function. This is sufficient because we only support one function per
+    // program at the moment.
+    // TODO(#1048): Find out why .maxIterations = 1 no longer works.
+    // There have been recent refactors to applyPatternsAndFoldGreedily
+    // upstream, and that might be the reason.
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 3;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    populateStablehloRefineShapesPatterns(&patterns, &getContext());
+    patterns.add<RefineDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<RefineDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<RefineDynamicTopKOpPattern>(&getContext());
+    patterns.add<RefineTanOpPattern>(&getContext());
+    patterns.add<RefineTopKOpPattern>(&getContext());
+    if (failed(
+            applyPatternsAndFoldGreedily(func, std::move(patterns), config))) {
+      func.emitError()
+          << "Greedy rewriter in StablehloRefineShapes does not converge after "
+          << config.maxIterations << " iterations.";
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/arange_start_enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.dynamic_iota %3, dim = 0 : (tensor<1xi32>) -> tensor<?xf32>
     %5 = stablehlo.constant dense<0> : tensor<i64>
     %6 = stablehlo.broadcast_in_dim %5, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %8 = stablehlo.constant dense<2> : tensor<i64>
     %9 = stablehlo.multiply %arg0, %8 : tensor<i64>
     %10 = stablehlo.convert %9 : (tensor<i64>) -> tensor<i32>
diff --ruN a/stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir b/stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir
--- stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir
+++ stablehlo/stablehlo/testdata/arange_start_no_dtype_dynamic.mlir
@@ -9,7 +9,7 @@
     %2 = stablehlo.dynamic_iota %1, dim = 0 : (tensor<1xi32>) -> tensor<?xi64>
     %3 = stablehlo.constant dense<0> : tensor<i64>
     %4 = stablehlo.broadcast_in_dim %3, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %5 = "stablehlo.gather"(%arg1, %4) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %5 = "stablehlo.gather"(%arg1, %4) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %6 = stablehlo.convert %2 : (tensor<?xi64>) -> tensor<?xf32>
     %7 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %8 = stablehlo.reshape %7 : (tensor<i32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_even_enable_xla_True_dynamic.mlir
@@ -96,7 +96,7 @@
     %89 = stablehlo.concatenate %86, %88, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %90 = stablehlo.reshape %89 : (tensor<2xi32>) -> tensor<1x2xi32>
     %91 = stablehlo.concatenate %90, dim = 0 : (tensor<1x2xi32>) -> tensor<1x2xi32>
-    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = dense<2> : tensor<1xi64>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
+    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = array<i64: 2, 2>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
     return %92 : tensor<1x?x16xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/conv_general_dilated_1d_stride_2_odd_enable_xla_True_dynamic.mlir
@@ -96,7 +96,7 @@
     %89 = stablehlo.concatenate %86, %88, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %90 = stablehlo.reshape %89 : (tensor<2xi32>) -> tensor<1x2xi32>
     %91 = stablehlo.concatenate %90, dim = 0 : (tensor<1x2xi32>) -> tensor<1x2xi32>
-    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = dense<2> : tensor<1xi64>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
+    %92 = "stablehlo.dynamic_conv"(%arg1, %arg2, %91) {batch_group_count = 1 : i64, dimension_numbers = #stablehlo.conv<[b, 0, f]x[0, i, o]->[b, 0, f]>, feature_group_count = 1 : i64, window_strides = array<i64: 2, 2>} : (tensor<1x?x16xf32>, tensor<4x16x16xf32>, tensor<1x2xi32>) -> tensor<1x?x16xf32>
     return %92 : tensor<1x?x16xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<1x2xf32>, tensor<1x2xi32>)
     %1 = call @expected() : () -> tensor<1xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<1x2xf32>, tensor<1x2xi32>) -> tensor<1xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1>} : (tensor<1x2xf32>, tensor<1x2xi32>) -> tensor<1xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<1xf32>, tensor<1xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<2x3x3xf32>, tensor<2x3xi32>)
     %1 = call @expected() : () -> tensor<2x3x2xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = dense<[1, 3, 2]> : tensor<3xi64>} : (tensor<2x3x3xf32>, tensor<2x3xi32>) -> tensor<2x3x2xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3, 2>} : (tensor<2x3x3xf32>, tensor<2x3xi32>) -> tensor<2x3x2xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2x3x2xf32>, tensor<2x3x2xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__2__6__3__start_indices_shape__2__3__slice_sizes__1__3__3__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<2x6x3xf32>, tensor<2x3xi32>)
     %1 = call @expected() : () -> tensor<2x3x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = dense<[1, 3, 3]> : tensor<3xi64>} : (tensor<2x6x3xf32>, tensor<2x3xi32>) -> tensor<2x3x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3, 3>} : (tensor<2x6x3xf32>, tensor<2x3xi32>) -> tensor<2x3x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2x3x3xf32>, tensor<2x3x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__3__10__start_indices_shape__3__2__slice_sizes__1__5__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<3x10xf32>, tensor<3x2xi32>)
     %1 = call @expected() : () -> tensor<3x5xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 5]> : tensor<2xi64>} : (tensor<3x10xf32>, tensor<3x2xi32>) -> tensor<3x5xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 5>} : (tensor<3x10xf32>, tensor<3x2xi32>) -> tensor<3x5xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<4x6xf32>, tensor<4x2xi32>)
     %1 = call @expected() : () -> tensor<4x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 3]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x2xi32>) -> tensor<4x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3>} : (tensor<4x6xf32>, tensor<4x2xi32>) -> tensor<4x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<4x3xf32>, tensor<4x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_bfloat16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xbf16>, tensor<1xi32>) -> tensor<bf16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xbf16>, tensor<1xi32>) -> tensor<bf16>
     %39 = stablehlo.constant dense<0x7FC0> : tensor<bf16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<bf16>
     return %40 : tensor<bf16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_bool_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi1>, tensor<1xi32>) -> tensor<i1>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi1>, tensor<1xi32>) -> tensor<i1>
     %39 = stablehlo.constant dense<true> : tensor<i1>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i1>
     return %40 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_complex64_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xcomplex<f32>>, tensor<1xi32>) -> tensor<complex<f32>>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xcomplex<f32>>, tensor<1xi32>) -> tensor<complex<f32>>
     %39 = stablehlo.constant dense<(0x7FC00000,0.000000e+00)> : tensor<complex<f32>>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<complex<f32>>
     return %40 : tensor<complex<f32>>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_float16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xf16>, tensor<1xi32>) -> tensor<f16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xf16>, tensor<1xi32>) -> tensor<f16>
     %39 = stablehlo.constant dense<0x7E00> : tensor<f16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<f16>
     return %40 : tensor<f16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_float32_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xf32>, tensor<1xi32>) -> tensor<f32>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xf32>, tensor<1xi32>) -> tensor<f32>
     %39 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<f32>
     return %40 : tensor<f32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi16>, tensor<1xi32>) -> tensor<i16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi16>, tensor<1xi32>) -> tensor<i16>
     %39 = stablehlo.constant dense<-32768> : tensor<i16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i16>
     return %40 : tensor<i16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int32_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi32>, tensor<1xi32>) -> tensor<i32>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi32>, tensor<1xi32>) -> tensor<i32>
     %39 = stablehlo.constant dense<-2147483648> : tensor<i32>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i32>
     return %40 : tensor<i32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_int8_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xi8>, tensor<1xi32>) -> tensor<i8>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xi8>, tensor<1xi32>) -> tensor<i8>
     %39 = stablehlo.constant dense<-128> : tensor<i8>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<i8>
     return %40 : tensor<i8>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint16_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xui16>, tensor<1xi32>) -> tensor<ui16>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xui16>, tensor<1xi32>) -> tensor<ui16>
     %39 = stablehlo.constant dense<65535> : tensor<ui16>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<ui16>
     return %40 : tensor<ui16>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint32_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xui32>, tensor<1xi32>) -> tensor<ui32>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xui32>, tensor<1xi32>) -> tensor<ui32>
     %39 = stablehlo.constant dense<4294967295> : tensor<ui32>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<ui32>
     return %40 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_dtypes_shape_uint8_10__axis_0_enable_xla_True.mlir
@@ -39,7 +39,7 @@
     %15 = stablehlo.add %6, %14 : tensor<1xi32>
     %16 = stablehlo.select %12, %15, %6 : tensor<1xi1>, tensor<1xi32>
     %17 = stablehlo.broadcast_in_dim %16, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %18 = "stablehlo.gather"(%9, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %19 = stablehlo.constant dense<1> : tensor<i32>
     %20 = stablehlo.broadcast_in_dim %19, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %21 = stablehlo.constant dense<0> : tensor<i32>
@@ -50,7 +50,7 @@
     %26 = stablehlo.add %7, %25 : tensor<1xi32>
     %27 = stablehlo.select %23, %26, %7 : tensor<1xi1>, tensor<1xi32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %29 = "stablehlo.gather"(%20, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<1xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %30 = stablehlo.subtract %18, %29 : tensor<1xi32>
     %31 = stablehlo.constant dense<0> : tensor<i32>
     %32 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -63,7 +63,7 @@
       %41 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %41 : tensor<i1>
     }
-    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<10xui8>, tensor<1xi32>) -> tensor<ui8>
+    %38 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<10xui8>, tensor<1xi32>) -> tensor<ui8>
     %39 = stablehlo.constant dense<255> : tensor<ui8>
     %40 = stablehlo.select %37, %38, %39 : tensor<i1>, tensor<ui8>
     return %40 : tensor<ui8>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___-1_-5_-200___enable_xla_True.mlir
@@ -23,7 +23,7 @@
     %15 = stablehlo.broadcast_in_dim %9, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %16 = stablehlo.broadcast_in_dim %13, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %17 = stablehlo.concatenate %14, %15, %16, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
-    %18 = "stablehlo.gather"(%0, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3xi32>) -> tensor<f32>
+    %18 = "stablehlo.gather"(%0, %17) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1, 1>} : (tensor<10x10x10xf32>, tensor<3xi32>) -> tensor<f32>
     %19 = stablehlo.custom_call @check.eq(%18, %1) : (tensor<f32>, tensor<f32>) -> tensor<i1>
     return %19 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___0_1___enable_xla_True.mlir
@@ -12,7 +12,7 @@
     %4 = stablehlo.constant dense<1> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<[1, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<10xf32>
+    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1, 10>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<10xf32>
     %8 = stablehlo.custom_call @check.eq(%7, %1) : (tensor<10xf32>, tensor<10xf32>) -> tensor<i1>
     return %8 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___0___enable_xla_True.mlir
@@ -9,7 +9,7 @@
     %1 = call @expected() : () -> tensor<10x10xf32>
     %2 = stablehlo.constant dense<0> : tensor<i32>
     %3 = stablehlo.broadcast_in_dim %2, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___2_5_5___enable_xla_True.mlir
@@ -12,7 +12,7 @@
     %4 = stablehlo.constant dense<5> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<[3, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
+    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 3, 1, 10>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
     %8 = stablehlo.custom_call @check.eq(%7, %1) : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<i1>
     return %8 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name___5_-2_300___enable_xla_True.mlir
@@ -12,7 +12,7 @@
     %4 = stablehlo.constant dense<300> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<[3, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
+    %7 = "stablehlo.gather"(%0, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 3, 1, 10>} : (tensor<10x10x10xf32>, tensor<2xi32>) -> tensor<3x10xf32>
     %8 = stablehlo.custom_call @check.eq(%7, %1) : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<i1>
     return %8 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir b/stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir
--- stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir
+++ stablehlo/stablehlo/testdata/gather_from_slicing_name______2_____enable_xla_True.mlir
@@ -9,7 +9,7 @@
     %1 = call @expected() : () -> tensor<10x10xf32>
     %2 = stablehlo.constant dense<2> : tensor<i32>
     %3 = stablehlo.broadcast_in_dim %2, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, indices_are_sorted = true, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %4 = "stablehlo.gather"(%0, %3) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, indices_are_sorted = true, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<i32>) -> tensor<10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     return %1 : tensor<10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill.mlir
@@ -44,7 +44,7 @@
     %20 = stablehlo.add %6, %19 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %24 = stablehlo.constant dense<1> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
@@ -60,7 +60,7 @@
     %36 = stablehlo.add %7, %35 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %40 = stablehlo.subtract %23, %39 : tensor<1xi32>
     %41 = stablehlo.constant dense<0> : tensor<i32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -73,7 +73,7 @@
       %53 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %53 : tensor<i1>
     }
-    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %49 = stablehlo.broadcast_in_dim %47, dims = [] : (tensor<i1>) -> tensor<10x10xi1>
     %50 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<f32>) -> tensor<10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<i32>) -> tensor<10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     return %1 : tensor<10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_1_enable_xla_True_mode_fill.mlir
@@ -44,7 +44,7 @@
     %20 = stablehlo.add %6, %19 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %24 = stablehlo.constant dense<10> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
@@ -60,7 +60,7 @@
     %36 = stablehlo.add %7, %35 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %40 = stablehlo.subtract %23, %39 : tensor<1xi32>
     %41 = stablehlo.constant dense<0> : tensor<i32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -73,7 +73,7 @@
       %53 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %53 : tensor<i1>
     }
-    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [1], start_index_map = [1]>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %49 = stablehlo.broadcast_in_dim %47, dims = [] : (tensor<i1>) -> tensor<10x10xi1>
     %50 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<f32>) -> tensor<10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<i32>) -> tensor<10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     return %1 : tensor<10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill.mlir
@@ -44,7 +44,7 @@
     %20 = stablehlo.add %6, %19 : tensor<1xi32>
     %21 = stablehlo.select %17, %20, %6 : tensor<1xi1>, tensor<1xi32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %23 = "stablehlo.gather"(%14, %22) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %24 = stablehlo.constant dense<10> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
@@ -60,7 +60,7 @@
     %36 = stablehlo.add %7, %35 : tensor<1xi32>
     %37 = stablehlo.select %33, %36, %7 : tensor<1xi1>, tensor<1xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %39 = "stablehlo.gather"(%30, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %40 = stablehlo.subtract %23, %39 : tensor<1xi32>
     %41 = stablehlo.constant dense<0> : tensor<i32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i32>) -> tensor<1xi32>
@@ -73,7 +73,7 @@
       %53 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %53 : tensor<i1>
     }
-    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
+    %48 = "stablehlo.gather"(%arg0, %5) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2]>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1xi32>) -> tensor<10x10xf32>
     %49 = stablehlo.broadcast_in_dim %47, dims = [] : (tensor<i1>) -> tensor<10x10xi1>
     %50 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<f32>) -> tensor<10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<1xi32>) -> tensor<1x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
     return %1 : tensor<1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<1x1xi32>
@@ -76,7 +76,7 @@
       %56 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %56 : tensor<i1>
     }
-    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
+    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<1x10x10xf32>
     %52 = stablehlo.broadcast_in_dim %50, dims = [0] : (tensor<1xi1>) -> tensor<1x10x10xi1>
     %53 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %54 = stablehlo.broadcast_in_dim %53, dims = [] : (tensor<f32>) -> tensor<1x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<1xi32>) -> tensor<10x1x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
     return %1 : tensor<10x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<1x1xi32>
@@ -76,7 +76,7 @@
       %56 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %56 : tensor<i1>
     }
-    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
+    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x1x10xf32>
     %52 = stablehlo.broadcast_in_dim %50, dims = [1] : (tensor<1xi1>) -> tensor<10x1x10xi1>
     %53 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %54 = stablehlo.broadcast_in_dim %53, dims = [] : (tensor<f32>) -> tensor<10x1x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<1xi32>) -> tensor<10x10x1xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
     return %1 : tensor<10x10x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__2__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<1x1xi32>
@@ -76,7 +76,7 @@
       %56 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %56 : tensor<i1>
     }
-    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
+    %51 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<1x1xi32>) -> tensor<10x10x1xf32>
     %52 = stablehlo.broadcast_in_dim %50, dims = [2] : (tensor<1xi1>) -> tensor<10x10x1xi1>
     %53 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %54 = stablehlo.broadcast_in_dim %53, dims = [] : (tensor<f32>) -> tensor<10x10x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xi32>) -> tensor<2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
     return %1 : tensor<2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0] : (tensor<2xi1>) -> tensor<2x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xi32>) -> tensor<10x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
     return %1 : tensor<10x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1] : (tensor<2xi1>) -> tensor<10x2x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xi32>) -> tensor<10x10x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
     return %1 : tensor<10x10x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2] : (tensor<2xi1>) -> tensor<10x10x2xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xui32>) -> tensor<2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xui32>) -> tensor<2x1xui32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<2x10x10xf32>
     return %1 : tensor<2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_0_enable_xla_True_mode_fill.mlir
@@ -47,7 +47,7 @@
     %23 = stablehlo.add %8, %22 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %27 = stablehlo.constant dense<1> : tensor<i32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %29 = stablehlo.constant dense<10> : tensor<i32>
@@ -63,7 +63,7 @@
     %39 = stablehlo.add %9, %38 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %43 = stablehlo.subtract %26, %42 : tensor<1xi32>
     %44 = stablehlo.constant dense<0> : tensor<i32>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -78,7 +78,7 @@
       %58 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %58 : tensor<i1>
     }
-    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
+    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<2x10x10xf32>
     %54 = stablehlo.broadcast_in_dim %52, dims = [0] : (tensor<2xi1>) -> tensor<2x10x10xi1>
     %55 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %56 = stablehlo.broadcast_in_dim %55, dims = [] : (tensor<f32>) -> tensor<2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xui32>) -> tensor<10x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xui32>) -> tensor<2x1xui32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x2x10xf32>
     return %1 : tensor<10x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_fill.mlir
@@ -47,7 +47,7 @@
     %23 = stablehlo.add %8, %22 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %27 = stablehlo.constant dense<10> : tensor<i32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %29 = stablehlo.constant dense<1> : tensor<i32>
@@ -63,7 +63,7 @@
     %39 = stablehlo.add %9, %38 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %43 = stablehlo.subtract %26, %42 : tensor<1xi32>
     %44 = stablehlo.constant dense<0> : tensor<i32>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -78,7 +78,7 @@
       %58 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %58 : tensor<i1>
     }
-    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
+    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x2x10xf32>
     %54 = stablehlo.broadcast_in_dim %52, dims = [1] : (tensor<2xi1>) -> tensor<10x2x10xi1>
     %55 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %56 = stablehlo.broadcast_in_dim %55, dims = [] : (tensor<f32>) -> tensor<10x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2xui32>) -> tensor<10x10x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<2xui32>) -> tensor<2x1xui32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x10x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xui32>) -> tensor<10x10x2xf32>
     return %1 : tensor<10x10x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__3_uint32__axis_2_enable_xla_True_mode_fill.mlir
@@ -47,7 +47,7 @@
     %23 = stablehlo.add %8, %22 : tensor<1xi32>
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %26 = "stablehlo.gather"(%16, %25) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %27 = stablehlo.constant dense<10> : tensor<i32>
     %28 = stablehlo.broadcast_in_dim %27, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %29 = stablehlo.constant dense<10> : tensor<i32>
@@ -63,7 +63,7 @@
     %39 = stablehlo.add %9, %38 : tensor<1xi32>
     %40 = stablehlo.select %36, %39, %9 : tensor<1xi1>, tensor<1xi32>
     %41 = stablehlo.broadcast_in_dim %40, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %42 = "stablehlo.gather"(%33, %41) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %43 = stablehlo.subtract %26, %42 : tensor<1xi32>
     %44 = stablehlo.constant dense<0> : tensor<i32>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i32>) -> tensor<2x1xi32>
@@ -78,7 +78,7 @@
       %58 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %58 : tensor<i1>
     }
-    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
+    %53 = "stablehlo.gather"(%arg0, %17) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x1xi32>) -> tensor<10x10x2xf32>
     %54 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<2xi1>) -> tensor<10x10x2xi1>
     %55 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %56 = stablehlo.broadcast_in_dim %55, dims = [] : (tensor<f32>) -> tensor<10x10x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<2x2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     return %1 : tensor<2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1] : (tensor<2x2xi1>) -> tensor<2x2x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x2x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     return %1 : tensor<10x2x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2] : (tensor<2x2xi1>) -> tensor<10x2x2x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x10x2x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     return %1 : tensor<10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3] : (tensor<2x2xi1>) -> tensor<10x10x2x2xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<3x1xi32>) -> tensor<3x1x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<3x1xi32>) -> tensor<3x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
     return %1 : tensor<3x1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<3x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<3x1x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1] : (tensor<3x1xi1>) -> tensor<3x1x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<3x1x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<3x1xi32>) -> tensor<10x3x1x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<3x1xi32>) -> tensor<3x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
     return %1 : tensor<10x3x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<3x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x3x1x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2] : (tensor<3x1xi1>) -> tensor<10x3x1x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x3x1x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<3x1xi32>) -> tensor<10x10x3x1xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<3x1xi32>) -> tensor<3x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
     return %1 : tensor<10x10x3x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__5_oob__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<3x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<3x1x1xi32>) -> tensor<10x10x3x1xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3] : (tensor<3x1xi1>) -> tensor<10x10x3x1xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x3x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<2x2x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     return %1 : tensor<2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<2x2x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1] : (tensor<2x2xi1>) -> tensor<2x2x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x2x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x2x2x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     return %1 : tensor<10x2x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 3], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 2>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x2x2x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2] : (tensor<2x2xi1>) -> tensor<10x2x2x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x2x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2xi32>) -> tensor<10x10x2x2xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<2x2xi32>) -> tensor<2x2x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     return %1 : tensor<10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 2>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1xi32>) -> tensor<10x10x2x2xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3] : (tensor<2x2xi1>) -> tensor<10x10x2x2xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2x2xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<5xi32>) -> tensor<5x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<5xi32>) -> tensor<5x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
     return %1 : tensor<5x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<5x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<5x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0] : (tensor<5xi1>) -> tensor<5x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<5x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<5xi32>) -> tensor<10x5x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<5xi32>) -> tensor<5x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
     return %1 : tensor<10x5x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<5x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 2], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 1>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x5x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1] : (tensor<5xi1>) -> tensor<10x5x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x5x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<5xi32>) -> tensor<10x10x5xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0] : (tensor<5xi32>) -> tensor<5x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
     return %1 : tensor<10x10x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__7_neg__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<5x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 1>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<5x1xi32>) -> tensor<10x10x5xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2] : (tensor<5xi1>) -> tensor<10x10x5xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x5xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2x1xi32>) -> tensor<2x2x1x10x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1, 2] : (tensor<2x2x1xi32>) -> tensor<2x2x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
     return %1 : tensor<2x2x1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<2x2x1x10x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [0, 1, 2] : (tensor<2x2x1xi1>) -> tensor<2x2x1x10x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<2x2x1x10x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2x1xi32>) -> tensor<10x2x2x1x10xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1, 2] : (tensor<2x2x1xi32>) -> tensor<2x2x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
     return %1 : tensor<10x2x2x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_1_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<1> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = dense<[10, 1, 10]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 4], collapsed_slice_dims = [1], start_index_map = [1], index_vector_dim = 3>, slice_sizes = array<i64: 10, 1, 10>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x2x2x1x10xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [1, 2, 3] : (tensor<2x2x1xi1>) -> tensor<10x2x2x1x10xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x2x2x1x10xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip.mlir
@@ -22,7 +22,7 @@
   }
   func.func private @_take(%arg0: tensor<10x10x10xf32>, %arg1: tensor<2x2x1xi32>) -> tensor<10x10x2x2x1xf32> {
     %0 = stablehlo.broadcast_in_dim %arg1, dims = [0, 1, 2] : (tensor<2x2x1xi32>) -> tensor<2x2x1x1xi32>
-    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
+    %1 = "stablehlo.gather"(%arg0, %0) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
     return %1 : tensor<10x10x2x2x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir b/stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
--- stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
+++ stablehlo/stablehlo/testdata/gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_fill.mlir
@@ -46,7 +46,7 @@
     %22 = stablehlo.add %8, %21 : tensor<1xi32>
     %23 = stablehlo.select %19, %22, %8 : tensor<1xi1>, tensor<1xi32>
     %24 = stablehlo.broadcast_in_dim %23, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %25 = "stablehlo.gather"(%16, %24) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %26 = stablehlo.constant dense<10> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<10> : tensor<i32>
@@ -62,7 +62,7 @@
     %38 = stablehlo.add %9, %37 : tensor<1xi32>
     %39 = stablehlo.select %35, %38, %9 : tensor<1xi1>, tensor<1xi32>
     %40 = stablehlo.broadcast_in_dim %39, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
+    %41 = "stablehlo.gather"(%32, %40) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi32>, tensor<1x1xi32>) -> tensor<1xi32>
     %42 = stablehlo.subtract %25, %41 : tensor<1xi32>
     %43 = stablehlo.constant dense<0> : tensor<i32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [] : (tensor<i32>) -> tensor<2x2x1x1xi32>
@@ -77,7 +77,7 @@
       %57 = stablehlo.and %arg2, %arg3 : tensor<i1>
       stablehlo.return %57 : tensor<i1>
     }
-    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = dense<[10, 10, 1]> : tensor<3xi64>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
+    %52 = "stablehlo.gather"(%arg0, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0, 1], collapsed_slice_dims = [2], start_index_map = [2], index_vector_dim = 3>, slice_sizes = array<i64: 10, 10, 1>} : (tensor<10x10x10xf32>, tensor<2x2x1x1xi32>) -> tensor<10x10x2x2x1xf32>
     %53 = stablehlo.broadcast_in_dim %51, dims = [2, 3, 4] : (tensor<2x2x1xi1>) -> tensor<10x10x2x2x1xi1>
     %54 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %55 = stablehlo.broadcast_in_dim %54, dims = [] : (tensor<f32>) -> tensor<10x10x2x2x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir b/stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir
--- stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__10__5__idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic5975718671559903784.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<10x5xf32>, tensor<3x1xi32>)
     %1 = call @expected() : () -> tensor<3x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 3]> : tensor<2xi64>} : (tensor<10x5xf32>, tensor<3x1xi32>) -> tensor<3x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3>} : (tensor<10x5xf32>, tensor<3x1xi32>) -> tensor<3x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir b/stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir
--- stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__10__6__idxs_shape__2__2__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slic-7762956558939593001.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<10x6xf32>, tensor<2x2xi32>)
     %1 = call @expected() : () -> tensor<2x3xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 3]> : tensor<2xi64>} : (tensor<10x6xf32>, tensor<2x2xi32>) -> tensor<2x3xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3>} : (tensor<10x6xf32>, tensor<2x2xi32>) -> tensor<2x3xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir b/stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir
--- stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_slice_-4284776541264791777.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<10xf32>, tensor<3x1xi32>)
     %1 = call @expected() : () -> tensor<3x2xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<2> : tensor<1xi64>} : (tensor<10xf32>, tensor<3x1xi32>) -> tensor<3x2xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<offset_dims = [1], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 2>} : (tensor<10xf32>, tensor<3x1xi32>) -> tensor<3x2xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<3x2xf32>, tensor<3x2xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir b/stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir
--- stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir
+++ stablehlo/stablehlo/testdata/gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slice_dim-7698694031974320657.mlir
@@ -7,7 +7,7 @@
   func.func public @main() -> tensor<i1> {
     %0:2 = call @inputs() : () -> (tensor<5xf32>, tensor<2x1xi32>)
     %1 = call @expected() : () -> tensor<2xf32>
-    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<5xf32>, tensor<2x1xi32>) -> tensor<2xf32>
+    %2 = "stablehlo.gather"(%0#0, %0#1) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<5xf32>, tensor<2x1xi32>) -> tensor<2xf32>
     %3 = stablehlo.custom_call @check.eq(%2, %1) : (tensor<2xf32>, tensor<2xf32>) -> tensor<i1>
     return %3 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_poly_idx_const_enable_xla_True_dynamic.mlir
@@ -6,7 +6,7 @@
   func.func public @main(%arg0: tensor<i64>, %arg1: tensor<?x4xf32> {mhlo.sharding = ""}) -> tensor<4xf32> {
     %0 = stablehlo.constant dense<1> : tensor<i64>
     %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %2 = "stablehlo.gather"(%arg1, %1) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
+    %2 = "stablehlo.gather"(%arg1, %1) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 4>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
     return %2 : tensor<4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_poly_idx_dim_enable_xla_True_dynamic.mlir
@@ -12,7 +12,7 @@
     %5 = stablehlo.add %1, %2 : tensor<i64>
     %6 = stablehlo.select %4, %5, %1 : tensor<i1>, tensor<i64>
     %7 = stablehlo.broadcast_in_dim %6, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %8 = "stablehlo.gather"(%arg1, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
+    %8 = "stablehlo.gather"(%arg1, %7) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 4>} : (tensor<?x4xf32>, tensor<1xi64>) -> tensor<4xf32>
     return %8 : tensor<4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_poly_idx_poly_enable_xla_True_dynamic.mlir
@@ -21,7 +21,7 @@
     %14 = stablehlo.constant dense<1> : tensor<1xi32>
     %15 = stablehlo.concatenate %13, %14, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %16 = stablehlo.dynamic_broadcast_in_dim %11, %15, dims = [0] : (tensor<?xi64>, tensor<2xi32>) -> tensor<?x1xi64>
-    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x4xf32>, tensor<?x1xi64>) -> tensor<?x4xf32>
+    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 4>} : (tensor<?x4xf32>, tensor<?x1xi64>) -> tensor<?x4xf32>
     return %17 : tensor<?x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/getitem_op_static_idx_poly_enable_xla_True_dynamic.mlir
@@ -20,7 +20,7 @@
     %13 = stablehlo.constant dense<1> : tensor<1xi32>
     %14 = stablehlo.concatenate %12, %13, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %15 = stablehlo.dynamic_broadcast_in_dim %10, %14, dims = [0] : (tensor<?xi32>, tensor<2xi32>) -> tensor<?x1xi32>
-    %16 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<3x4xf32>, tensor<?x1xi32>) -> tensor<?x4xf32>
+    %16 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 4>} : (tensor<3x4xf32>, tensor<?x1xi32>) -> tensor<?x4xf32>
     return %16 : tensor<?x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir b/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir
--- stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir
+++ stablehlo/stablehlo/testdata/jnp_pad_mode_constant_bminus1_dynamic.mlir
@@ -39,7 +39,7 @@
     %4 = stablehlo.constant dense<0> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %8 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %9 = stablehlo.reshape %8 : (tensor<i32>) -> tensor<1xi32>
     %10 = stablehlo.constant dense<0> : tensor<1xi32>
@@ -56,21 +56,21 @@
     %21 = stablehlo.constant dense<1> : tensor<i32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %23 = stablehlo.concatenate %20, %22, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %25 = stablehlo.pad %18, %24, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x5xf32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<0> : tensor<i32>
     %29 = stablehlo.broadcast_in_dim %28, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %30 = stablehlo.concatenate %27, %29, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %32 = stablehlo.pad %25, %31, low = [0, 5], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x10xf32>
     %33 = stablehlo.constant dense<1> : tensor<i32>
     %34 = stablehlo.broadcast_in_dim %33, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %35 = stablehlo.constant dense<1> : tensor<i32>
     %36 = stablehlo.broadcast_in_dim %35, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %37 = stablehlo.concatenate %34, %36, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %39 = stablehlo.pad %32, %38, low = [0, 0], high = [0, 1], interior = [0, 0] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?x11xf32>
     return %39 : tensor<?x11xf32>
   }
diff --ruN a/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir b/stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir
--- stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir
+++ stablehlo/stablehlo/testdata/jnp_pad_mode_constant_dynamic.mlir
@@ -16,7 +16,7 @@
     %4 = stablehlo.constant dense<0> : tensor<i32>
     %5 = stablehlo.broadcast_in_dim %4, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %6 = stablehlo.concatenate %3, %5, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %7 = "stablehlo.gather"(%1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %8 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %9 = stablehlo.reshape %8 : (tensor<i32>) -> tensor<1xi32>
     %10 = stablehlo.constant dense<0> : tensor<1xi32>
@@ -33,21 +33,21 @@
     %21 = stablehlo.constant dense<1> : tensor<i32>
     %22 = stablehlo.broadcast_in_dim %21, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %23 = stablehlo.concatenate %20, %22, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %24 = "stablehlo.gather"(%1, %23) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %25 = stablehlo.pad %18, %24, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x5xf32>
     %26 = stablehlo.constant dense<1> : tensor<i32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %28 = stablehlo.constant dense<0> : tensor<i32>
     %29 = stablehlo.broadcast_in_dim %28, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %30 = stablehlo.concatenate %27, %29, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %31 = "stablehlo.gather"(%1, %30) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %32 = stablehlo.pad %25, %31, low = [0, 5], high = [0, 0], interior = [0, 0] : (tensor<?x5xf32>, tensor<f32>) -> tensor<?x10xf32>
     %33 = stablehlo.constant dense<1> : tensor<i32>
     %34 = stablehlo.broadcast_in_dim %33, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %35 = stablehlo.constant dense<1> : tensor<i32>
     %36 = stablehlo.broadcast_in_dim %35, dims = [] : (tensor<i32>) -> tensor<1xi32>
     %37 = stablehlo.concatenate %34, %36, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
-    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
+    %38 = "stablehlo.gather"(%1, %37) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<2x2xf32>, tensor<2xi32>) -> tensor<f32>
     %39 = stablehlo.pad %32, %38, low = [0, 0], high = [0, 1], interior = [0, 0] : (tensor<?x10xf32>, tensor<f32>) -> tensor<?x11xf32>
     return %39 : tensor<?x11xf32>
   }
diff --ruN a/stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir b/stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
--- stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
+++ stablehlo/stablehlo/testdata/nanquantile_axis_None_dynamic.mlir
@@ -90,7 +90,7 @@
     %40 = stablehlo.add %33, %37 : tensor<i64>
     %41 = stablehlo.select %39, %40, %33 : tensor<i1>, tensor<i64>
     %42 = stablehlo.broadcast_in_dim %41, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %43 = "stablehlo.gather"(%8, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %43 = "stablehlo.gather"(%8, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %44 = stablehlo.constant dense<5> : tensor<i64>
     %45 = stablehlo.multiply %arg0, %44 : tensor<i64>
     %46 = stablehlo.convert %45 : tensor<i64>
@@ -99,7 +99,7 @@
     %49 = stablehlo.add %34, %46 : tensor<i64>
     %50 = stablehlo.select %48, %49, %34 : tensor<i1>, tensor<i64>
     %51 = stablehlo.broadcast_in_dim %50, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %52 = "stablehlo.gather"(%8, %51) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %52 = "stablehlo.gather"(%8, %51) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %53 = stablehlo.convert %43 : (tensor<f32>) -> tensor<f64>
     %54 = stablehlo.multiply %53, %22 : tensor<f64>
     %55 = stablehlo.convert %52 : (tensor<f32>) -> tensor<f64>
diff --ruN a/stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir b/stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir
--- stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir
+++ stablehlo/stablehlo/testdata/percentile_axis_None_dynamic.mlir
@@ -88,9 +88,9 @@
     %31 = stablehlo.convert %26 : (tensor<f64>) -> tensor<i64>
     %32 = stablehlo.convert %30 : (tensor<f64>) -> tensor<i64>
     %33 = stablehlo.broadcast_in_dim %31, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %34 = "stablehlo.gather"(%11, %33) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %34 = "stablehlo.gather"(%11, %33) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %35 = stablehlo.broadcast_in_dim %32, dims = [] : (tensor<i64>) -> tensor<1xi64>
-    %36 = "stablehlo.gather"(%11, %35) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
+    %36 = "stablehlo.gather"(%11, %35) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, slice_sizes = array<i64: 1>} : (tensor<?xf32>, tensor<1xi64>) -> tensor<f32>
     %37 = stablehlo.convert %34 : (tensor<f32>) -> tensor<f64>
     %38 = stablehlo.multiply %37, %22 : tensor<f64>
     %39 = stablehlo.convert %36 : (tensor<f32>) -> tensor<f64>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float32.mlir
@@ -323,7 +323,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -641,7 +641,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -758,7 +758,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float32_3.mlir
@@ -326,7 +326,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -644,7 +644,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -761,7 +761,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float64.mlir
@@ -323,7 +323,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -641,7 +641,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -758,7 +758,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir b/stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
--- stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
+++ stablehlo/stablehlo/testdata/random_gamma_shape_float64_3.mlir
@@ -326,7 +326,7 @@
       %105 = stablehlo.concatenate %104#2, %104#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
       %106 = stablehlo.constant dense<0> : tensor<i32>
       %107 = stablehlo.broadcast_in_dim %106, dims = [] : (tensor<i32>) -> tensor<1xi32>
-      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+      %108 = "stablehlo.gather"(%105, %107) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
       %109 = stablehlo.reshape %108 : (tensor<1xui32>) -> tensor<ui32>
       %110 = stablehlo.constant dense<9> : tensor<ui32>
       %111 = stablehlo.shift_right_logical %109, %110 : tensor<ui32>
@@ -644,7 +644,7 @@
           %272 = stablehlo.concatenate %271#2, %271#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
           %273 = stablehlo.constant dense<0> : tensor<i32>
           %274 = stablehlo.broadcast_in_dim %273, dims = [] : (tensor<i32>) -> tensor<1xi32>
-          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+          %275 = "stablehlo.gather"(%272, %274) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
           %276 = stablehlo.reshape %275 : (tensor<1xui32>) -> tensor<ui32>
           %277 = stablehlo.constant dense<9> : tensor<ui32>
           %278 = stablehlo.shift_right_logical %276, %277 : tensor<ui32>
@@ -761,7 +761,7 @@
         %205 = stablehlo.concatenate %204#2, %204#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
         %206 = stablehlo.constant dense<0> : tensor<i32>
         %207 = stablehlo.broadcast_in_dim %206, dims = [] : (tensor<i32>) -> tensor<1xi32>
-        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+        %208 = "stablehlo.gather"(%205, %207) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
         %209 = stablehlo.reshape %208 : (tensor<1xui32>) -> tensor<ui32>
         %210 = stablehlo.constant dense<9> : tensor<ui32>
         %211 = stablehlo.shift_right_logical %209, %210 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int16.mlir
@@ -201,7 +201,7 @@
     %65 = stablehlo.concatenate %64#2, %64#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %66 = stablehlo.constant dense<0> : tensor<i32>
     %67 = stablehlo.broadcast_in_dim %66, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %69 = stablehlo.broadcast_in_dim %68, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %70 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %71 = stablehlo.constant dense<16> : tensor<ui32>
@@ -306,7 +306,7 @@
     %106 = stablehlo.concatenate %105#2, %105#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %107 = stablehlo.constant dense<0> : tensor<i32>
     %108 = stablehlo.broadcast_in_dim %107, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %110 = stablehlo.broadcast_in_dim %109, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %111 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %112 = stablehlo.constant dense<16> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int32.mlir
@@ -200,7 +200,7 @@
     %64 = stablehlo.concatenate %63#2, %63#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %65 = stablehlo.constant dense<0> : tensor<i32>
     %66 = stablehlo.broadcast_in_dim %65, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %67 = "stablehlo.gather"(%64, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %67 = "stablehlo.gather"(%64, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %68 = stablehlo.reshape %67 : (tensor<1xui32>) -> tensor<ui32>
     %69 = stablehlo.constant dense<0> : tensor<1xui32>
     %70 = stablehlo.iota dim = 0 : tensor<1xui32>
@@ -290,7 +290,7 @@
     %90 = stablehlo.concatenate %89#2, %89#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %91 = stablehlo.constant dense<0> : tensor<i32>
     %92 = stablehlo.broadcast_in_dim %91, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %93 = "stablehlo.gather"(%90, %92) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %93 = "stablehlo.gather"(%90, %92) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %94 = stablehlo.reshape %93 : (tensor<1xui32>) -> tensor<ui32>
     %95 = stablehlo.subtract %17, %12 : tensor<i32>
     %96 = stablehlo.convert %95 : (tensor<i32>) -> tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int8.mlir
@@ -201,7 +201,7 @@
     %65 = stablehlo.concatenate %64#2, %64#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %66 = stablehlo.constant dense<0> : tensor<i32>
     %67 = stablehlo.broadcast_in_dim %66, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %68 = "stablehlo.gather"(%65, %67) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %69 = stablehlo.broadcast_in_dim %68, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %70 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %71 = stablehlo.constant dense<8> : tensor<ui32>
@@ -306,7 +306,7 @@
     %106 = stablehlo.concatenate %105#2, %105#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %107 = stablehlo.constant dense<0> : tensor<i32>
     %108 = stablehlo.broadcast_in_dim %107, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %109 = "stablehlo.gather"(%106, %108) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %110 = stablehlo.broadcast_in_dim %109, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %111 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %112 = stablehlo.constant dense<8> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir b/stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir
--- stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir
+++ stablehlo/stablehlo/testdata/random_randint_shape_int8_5_4.mlir
@@ -203,7 +203,7 @@
     %67 = stablehlo.concatenate %66#2, %66#3, dim = 0 : (tensor<3xui32>, tensor<3xui32>) -> tensor<6xui32>
     %68 = stablehlo.constant dense<0> : tensor<i32>
     %69 = stablehlo.broadcast_in_dim %68, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %70 = "stablehlo.gather"(%67, %69) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<5> : tensor<1xi64>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
+    %70 = "stablehlo.gather"(%67, %69) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 5>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
     %71 = stablehlo.broadcast_in_dim %70, dims = [1] : (tensor<5xui32>) -> tensor<1x5xui32>
     %72 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %73 = stablehlo.constant dense<8> : tensor<ui32>
@@ -307,7 +307,7 @@
     %107 = stablehlo.concatenate %106#2, %106#3, dim = 0 : (tensor<3xui32>, tensor<3xui32>) -> tensor<6xui32>
     %108 = stablehlo.constant dense<0> : tensor<i32>
     %109 = stablehlo.broadcast_in_dim %108, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %110 = "stablehlo.gather"(%107, %109) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<5> : tensor<1xi64>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
+    %110 = "stablehlo.gather"(%107, %109) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 5>} : (tensor<6xui32>, tensor<1xi32>) -> tensor<5xui32>
     %111 = stablehlo.broadcast_in_dim %110, dims = [1] : (tensor<5xui32>) -> tensor<1x5xui32>
     %112 = stablehlo.iota dim = 0 : tensor<4x1xui32>
     %113 = stablehlo.constant dense<8> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_bfloat16.mlir
@@ -95,7 +95,7 @@
     %23 = stablehlo.concatenate %22#2, %22#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %24 = stablehlo.constant dense<0> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %28 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %29 = stablehlo.constant dense<16> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_float16.mlir
@@ -95,7 +95,7 @@
     %23 = stablehlo.concatenate %22#2, %22#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %24 = stablehlo.constant dense<0> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %27 = stablehlo.broadcast_in_dim %26, dims = [1] : (tensor<1xui32>) -> tensor<1x1xui32>
     %28 = stablehlo.iota dim = 0 : tensor<2x1xui32>
     %29 = stablehlo.constant dense<16> : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir b/stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
--- stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
+++ stablehlo/stablehlo/testdata/random_uniform_shape_float32.mlir
@@ -95,7 +95,7 @@
     %23 = stablehlo.concatenate %22#2, %22#3, dim = 0 : (tensor<1xui32>, tensor<1xui32>) -> tensor<2xui32>
     %24 = stablehlo.constant dense<0> : tensor<i32>
     %25 = stablehlo.broadcast_in_dim %24, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
+    %26 = "stablehlo.gather"(%23, %25) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1>} : (tensor<2xui32>, tensor<1xi32>) -> tensor<1xui32>
     %27 = stablehlo.reshape %26 : (tensor<1xui32>) -> tensor<ui32>
     %28 = stablehlo.constant dense<9> : tensor<ui32>
     %29 = stablehlo.shift_right_logical %27, %28 : tensor<ui32>
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_0_enable_xla_True_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
     return %2 : tensor<?x7xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_1_enable_xla_True_dynamic.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %11 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %11 : tensor<f32>
-    }) {padding = dense<[[0, 0], [1, 1], [0, 0]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 4, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<1x?x1xf32>, tensor<f32>) -> tensor<1x?x1xf32>
+    }) {padding = dense<[[0, 0], [1, 1], [0, 0]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 4, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<1x?x1xf32>, tensor<f32>) -> tensor<1x?x1xf32>
     return %10 : tensor<1x?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides_7986328860834552647.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {base_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x10xf32>
+    }) {base_dilations = array<i64: 1, 2>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x10xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__-2220972298967194594.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %6 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-1428448145173401924.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_-4698468758011440787.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %6 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_8225502087604216949.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p2993120393378492976.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %6 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p1727597624205542039.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p3586290205775842300.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-4032734852119941554.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad7635044839543803654.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %6 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad6961367239774059619.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-42586455448778120.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %6 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4840663732946877279.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-4938290008184005060.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<i8>
       stablehlo.return %6 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd879372096543558505.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-5953571789298025160.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %6 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8824143900020308206.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa9076128080492083436.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %6 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa408729289442029153.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6324563894280027939.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<ui8>
       stablehlo.return %6 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-7116931799215120108.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.add %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__13_13__windows-691859033508160866.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[5, 6], [3, 4]]> : tensor<2x2xi64>, window_dimensions = dense<13> : tensor<2xi64>, window_strides = dense<[5, 6]> : tensor<2xi64>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<3x2xf32>
+    }) {padding = dense<[[5, 6], [3, 4]]> : tensor<2x2xi64>, window_dimensions = array<i64: 13, 13>, window_strides = array<i64: 5, 6>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<3x2xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x2xf32>, tensor<3x2xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__windowstr6046687465413193911.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<12x12xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 2, 2>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<12x12xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<12x12xf32>, tensor<12x12xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__windowstr-4104859581547667440.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<6x6xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<12x12xf32>, tensor<f32>) -> tensor<6x6xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<6x6xf32>, tensor<6x6xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_padding_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__5080188771172348269.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[1, 2], [0, 3]]> : tensor<2x2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<6x8xf32>
+    }) {padding = dense<[[1, 2], [0, 3]]> : tensor<2x2xi64>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<6x8xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<6x8xf32>, tensor<6x8xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_same_padding_shape_float32_112_112__initvalue_0_0_windowdimensions__3_3__windowstr-3365744398050275565.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<56x56xf32>, tensor<56x56xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstride1490052185320441557.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dilations = dense<[1, 2]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x4xf32>
+    }) {window_dilations = array<i64: 1, 2>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x4xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x4xf32>, tensor<3x4xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windowstri-9144278943768635611.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<1> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<4x6xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<4x6xf32>, tensor<4x6xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir b/stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir
--- stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_add_window_strides_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides-3887130791349734568.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>, window_strides = dense<[1, 2]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x3xf32>
+    }) {window_dimensions = array<i64: 2, 2>, window_strides = array<i64: 1, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x3xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_1__initvalue_-inf_windowdimensions__1_2_1__wi2907722705369048493.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<1x2x1xf32>, tensor<f32>) -> tensor<1x1x1xf32>
+    }) {window_dimensions = array<i64: 1, 2, 1>} : (tensor<1x2x1xf32>, tensor<f32>) -> tensor<1x1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2_2_1-5111631298771835819.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>} : (tensor<1x2x4x1xf32>, tensor<f32>) -> tensor<1x1x3x1xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 1>} : (tensor<1x2x4x1xf32>, tensor<f32>) -> tensor<1x1x3x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1x3x1xf32>, tensor<1x1x3x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__window-1636693082974747423.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2]> : tensor<2xi64>} : (tensor<1x2xf32>, tensor<f32>) -> tensor<1x1xf32>
+    }) {window_dimensions = array<i64: 1, 2>} : (tensor<1x2xf32>, tensor<f32>) -> tensor<1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_1_4_3_2_1__initvalue_-inf_windowdimensions__1_2_2-7793407312160842848.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 2, 1]> : tensor<5xi64>} : (tensor<1x4x3x2x1xf32>, tensor<f32>) -> tensor<1x3x2x1x1xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 2, 1>} : (tensor<1x4x3x2x1xf32>, tensor<f32>) -> tensor<1x3x2x1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x3x2x1x1xf32>, tensor<1x3x2x1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_1__initvalue_-inf_windowdimensions__2_1__window7224370613358112198.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<[2, 1]> : tensor<2xi64>} : (tensor<2x1xf32>, tensor<f32>) -> tensor<1x1xf32>
+    }) {window_dimensions = array<i64: 2, 1>} : (tensor<2x1xf32>, tensor<f32>) -> tensor<1x1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_2__wi2460382255230330798.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x3xf32>, tensor<f32>) -> tensor<1x3x2xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x3xf32>, tensor<f32>) -> tensor<1x3x2xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x3x2xf32>, tensor<1x3x2xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__window3880194203908208615.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<2x4xf32>, tensor<f32>) -> tensor<1x3xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<2x4xf32>, tensor<f32>) -> tensor<1x3xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_batch_channel_dims_shape_float32_2__initvalue_-inf_windowdimensions__2___windowstr-4386488581234318633.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<1xi64>} : (tensor<2xf32>, tensor<f32>) -> tensor<1xf32>
+    }) {window_dimensions = array<i64: 2>} : (tensor<2xf32>, tensor<f32>) -> tensor<1xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<1xf32>, tensor<1xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_-6216933667409612517.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<bf16>
       stablehlo.return %6 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-4994625811150598768.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd5379111250118667329.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i1>
       stablehlo.return %5 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi1>, tensor<3x5xi1>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_bool_4_6__initvalue_False_windowdimensions__2_2__windowstrides__1_1__8072758898254897225.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i1>
       stablehlo.return %6 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi1>, tensor<3x5xi1>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_4291849701066239869.mlir
@@ -20,7 +20,7 @@
       %12 = stablehlo.select %7, %11, %8 : tensor<i1>, tensor<i1>
       %13 = stablehlo.select %12, %arg0, %arg1 : tensor<i1>, tensor<complex<f32>>
       stablehlo.return %13 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_complex64_4_6__initvalue__-inf_0j__windowdimensions__2_2__windowstrid-7363942699456202060.mlir
@@ -21,7 +21,7 @@
       %13 = stablehlo.select %8, %12, %9 : tensor<i1>, tensor<i1>
       %14 = stablehlo.select %13, %arg0, %arg1 : tensor<i1>, tensor<complex<f32>>
       stablehlo.return %14 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_11557672409746689732.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f16>
       stablehlo.return %6 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-7750873517905381577.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_-inf_windowdimensions__2_2__windowstrides__1_15005027447381495146.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-5323860118055166118.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_-32768_windowdimensions__2_2__windowstrides__1_1-3971969563239728636.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i16>
       stablehlo.return %6 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad3937654242282090133.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_-2147483648_windowdimensions__2_2__windowstrides-6104600437082556539.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i32>
       stablehlo.return %6 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-4758189359533826695.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_-128_windowdimensions__2_2__windowstrides__1_1__p-1721685740020350494.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<i8>
       stablehlo.return %6 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd3921525681822021249.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6688282750744868505.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<ui16>
       stablehlo.return %6 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa5793043271262126639.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-6195221688489426534.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<ui32>
       stablehlo.return %6 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-2745576375630020475.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad617319801188093086.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<ui8>
       stablehlo.return %6 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad7930015827732118891.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.maximum %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir b/stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir
--- stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__windowst-4354541867574846756.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.maximum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<56x56xf32>, tensor<56x56xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bfloat16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_13309700146959997041.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<bf16>
       stablehlo.return %6 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_bool_4_6__initvalue_True_windowdimensions__2_2__windowstrides__1_1__p-1453727971620327298.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i1>
       stablehlo.return %6 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi1>, tensor<i1>) -> tensor<3x5xi1>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi1>, tensor<3x5xi1>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_complex64_4_6__initvalue__inf_0j__windowdimensions__2_2__windowstride4144686377792404619.mlir
@@ -21,7 +21,7 @@
       %13 = stablehlo.select %8, %12, %9 : tensor<i1>, tensor<i1>
       %14 = stablehlo.select %13, %arg0, %arg1 : tensor<i1>, tensor<complex<f32>>
       stablehlo.return %14 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float16_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_9069967543444044853.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<f16>
       stablehlo.return %6 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_float32_4_6__initvalue_inf_windowdimensions__2_2__windowstrides__1_1_-3090946471386215849.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int16_4_6__initvalue_32767_windowdimensions__2_2__windowstrides__1_1_-5773481113145828258.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i16>
       stablehlo.return %6 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int32_4_6__initvalue_2147483647_windowdimensions__2_2__windowstrides_7936915878546705934.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i32>
       stablehlo.return %6 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_int8_4_6__initvalue_127_windowdimensions__2_2__windowstrides__1_1__pa-2505694084141416255.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<i8>
       stablehlo.return %6 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint16_4_6__initvalue_65535_windowdimensions__2_2__windowstrides__1_1-4410384324890600071.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<ui16>
       stablehlo.return %6 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint32_4_6__initvalue_4294967295_windowdimensions__2_2__windowstrides3362483014574803263.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<ui32>
       stablehlo.return %6 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_dtypes_shape_uint8_4_6__initvalue_255_windowdimensions__2_2__windowstrides__1_1__p-1965134471500928525.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<ui8>
       stablehlo.return %6 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_enable_xla_True_dynamic.mlir
@@ -9,7 +9,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %2 = stablehlo.minimum %arg2, %arg3 : tensor<f32>
       stablehlo.return %2 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<?x8xf32>, tensor<f32>) -> tensor<?x7xf32>
     return %1 : tensor<?x7xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_init_value_1d_shape_float32_1_16000__initvalue_1_0_windowdimensions__1_401__window2830952238904064054.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.minimum %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<[1, 401]> : tensor<2xi64>, window_strides = dense<[1, 160]> : tensor<2xi64>} : (tensor<1x16000xf32>, tensor<f32>) -> tensor<1x98xf32>
+    }) {window_dimensions = array<i64: 1, 401>, window_strides = array<i64: 1, 160>} : (tensor<1x16000xf32>, tensor<f32>) -> tensor<1x98xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<1x98xf32>, tensor<1x98xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir b/stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir
--- stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__windowstr-2492685243049766723.mlir
@@ -13,7 +13,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %6 = stablehlo.minimum %arg0, %arg1 : tensor<f32>
       stablehlo.return %6 : tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<3> : tensor<2xi64>, window_strides = dense<2> : tensor<2xi64>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 3, 3>, window_strides = array<i64: 2, 2>} : (tensor<112x112xf32>, tensor<f32>) -> tensor<56x56xf32>
     %5 = stablehlo.custom_call @check.eq(%4, %1) : (tensor<56x56xf32>, tensor<56x56xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__3340519855672632656.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__-7707877832553282312.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__-4352332529828279679.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<bf16>
       stablehlo.return %5 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<bf16>) -> tensor<3x5xbf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1_2680457227484159444.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1_-8749826146171525914.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_complex64_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1_-4418219444220371989.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<complex<f32>>, %arg1: tensor<complex<f32>>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<complex<f32>>
       stablehlo.return %5 : tensor<complex<f32>>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xcomplex<f32>>, tensor<complex<f32>>) -> tensor<3x5xcomplex<f32>>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xcomplex<f32>>, tensor<3x5xcomplex<f32>>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p8588115838297523173.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-722024604699842952.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p3251667528106744236.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f16>
       stablehlo.return %5 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<f16>) -> tensor<3x5xf16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__p-9219300395209946880.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__p-1170997397628138029.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_float32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__p5865330874643232007.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<f32>
       stablehlo.return %5 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<f32>) -> tensor<3x5xf32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad6936816701131227392.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-877041750945830425.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-4255861490662115505.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i16>
       stablehlo.return %5 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi16>, tensor<i16>) -> tensor<3x5xi16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi16>, tensor<3x5xi16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad-7315803323352185471.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad-3200229493467114834.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad-6349420869970636753.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i32>
       stablehlo.return %5 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi32>, tensor<i32>) -> tensor<3x5xi32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi32>, tensor<3x5xi32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__padd-3968434153936355982.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__padd2549091231254646447.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_int8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__padd6734116708708925884.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<i8>
       stablehlo.return %5 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xi8>, tensor<i8>) -> tensor<3x5xi8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xi8>, tensor<3x5xi8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa2377140915067044416.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa-9053278640616598724.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint16_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2883905225725849680.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui16>
       stablehlo.return %5 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui16>, tensor<ui16>) -> tensor<3x5xui16>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui16>, tensor<3x5xui16>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pa-8516304739392392826.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pa8716816771292110437.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint32_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pa-2670986806077945892.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui32>
       stablehlo.return %5 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui32>, tensor<ui32>) -> tensor<3x5xui32>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui32>, tensor<3x5xui32>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_0_windowdimensions__2_2__windowstrides__1_1__pad8624066883509500227.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_1_windowdimensions__2_2__windowstrides__1_1__pad5922828822890250003.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir b/stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir
--- stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir
+++ stablehlo/stablehlo/testdata/reduce_window_mul_dtypes_shape_uint8_4_6__initvalue_2_windowdimensions__2_2__windowstrides__1_1__pad5589770953477007003.mlir
@@ -12,7 +12,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %5 = stablehlo.multiply %arg0, %arg1 : tensor<ui8>
       stablehlo.return %5 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xui8>, tensor<ui8>) -> tensor<3x5xui8>
     %4 = stablehlo.custom_call @check.eq(%3, %1) : (tensor<3x5xui8>, tensor<3x5xui8>) -> tensor<i1>
     return %4 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-1234442230378059557.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {base_dilations = dense<[2, 3]> : tensor<2xi64>, window_dilations = dense<[3, 2]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x14xf32>, tensor<4x14xf32>)
+    }) {base_dilations = array<i64: 2, 3>, window_dilations = array<i64: 3, 2>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x14xf32>, tensor<4x14xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<4x14xf32>, tensor<4x14xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid-8339917744334524227.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {base_dilations = dense<[2, 3]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<6x15xf32>, tensor<6x15xf32>)
+    }) {base_dilations = array<i64: 2, 3>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<6x15xf32>, tensor<6x15xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<6x15xf32>, tensor<6x15xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrid4494141771612406334.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dilations = dense<[2, 3]> : tensor<2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x3xf32>, tensor<2x3xf32>)
+    }) {window_dilations = array<i64: 2, 3>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x3xf32>, tensor<2x3xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides4313362319678633333.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<bf16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<bf16>
       stablehlo.return %7, %8 : tensor<bf16>, tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_bfloat16_4_6__selectprim_le_windowdimensions__2__2__windowstrides-1012953229766402479.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<bf16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<bf16>
       stablehlo.return %7, %8 : tensor<bf16>, tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xbf16>, tensor<4x6xbf16>, tensor<bf16>, tensor<bf16>) -> (tensor<3x5xbf16>, tensor<3x5xbf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xbf16>, tensor<3x5xbf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_7468763519813009785.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f16>
       stablehlo.return %7, %8 : tensor<f16>, tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float16_4_6__selectprim_le_windowdimensions__2__2__windowstrides_-5656666469697250749.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f16>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f16>
       stablehlo.return %7, %8 : tensor<f16>, tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf16>, tensor<4x6xf16>, tensor<f16>, tensor<f16>) -> (tensor<3x5xf16>, tensor<3x5xf16>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf16>, tensor<3x5xf16>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstrides_1049736773648920798.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides_8180748863193819158.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstrides7172513329414880838.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x6xf32>, tensor<4x6xf32>)
+    }) {padding = dense<[[0, 1], [0, 1]]> : tensor<2x2xi64>, window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<4x6xf32>, tensor<4x6xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<4x6xf32>, tensor<4x6xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_select_prim_shape_float32_4_6__selectprim_ge_windowdimensions__2__2__windowstr-2717710683935812517.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
+    }) {window_dimensions = array<i64: 2, 2>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x5xf32>, tensor<3x5xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_window_dimensions_shape_float32_4_6__selectprim_le_windowdimensions__2__3__win-2005780783284749355.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<[2, 3]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x4xf32>, tensor<3x4xf32>)
+    }) {window_dimensions = array<i64: 2, 3>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<3x4xf32>, tensor<3x4xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<3x4xf32>, tensor<3x4xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir b/stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir
--- stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir
+++ stablehlo/stablehlo/testdata/select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window1194065983982281707.mlir
@@ -15,7 +15,7 @@
       %7 = stablehlo.select %6, %arg0, %arg2 : tensor<i1>, tensor<f32>
       %8 = stablehlo.select %6, %arg1, %arg3 : tensor<i1>, tensor<f32>
       stablehlo.return %7, %8 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<2xi64>, window_strides = dense<[2, 3]> : tensor<2xi64>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x2xf32>, tensor<2x2xf32>)
+    }) {window_dimensions = array<i64: 2, 2>, window_strides = array<i64: 2, 3>} : (tensor<4x6xf32>, tensor<4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<2x2xf32>, tensor<2x2xf32>)
     %5 = stablehlo.custom_call @check.eq(%4#1, %1) : (tensor<2x2xf32>, tensor<2x2xf32>) -> tensor<i1>
     return %5 : tensor<i1>
   }
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows5498388055904314537.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %8 : tensor<bf16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xbf16>, tensor<1x3x5xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xbf16>, tensor<1x3x5xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xbf16>) -> tensor<2x4x6xbf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xbf16>, tensor<2x4x6xbf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_bool_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid3953935332736217868.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i1>, %arg1: tensor<i1>):
       %8 = stablehlo.or %arg0, %arg1 : tensor<i1>
       stablehlo.return %8 : tensor<i1>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi1>, tensor<1x3x5xi1>, tensor<i1>) -> tensor<2x4x6xi1>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi1>, tensor<1x3x5xi1>, tensor<i1>) -> tensor<2x4x6xi1>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi1>) -> tensor<2x4x6xi1>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi1>, tensor<2x4x6xi1>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst645862737998720881.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %8 : tensor<f16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xf16>, tensor<1x3x5xf16>, tensor<f16>) -> tensor<2x4x6xf16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xf16>, tensor<1x3x5xf16>, tensor<f16>) -> tensor<2x4x6xf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf16>) -> tensor<2x4x6xf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf16>, tensor<2x4x6xf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowst6350781848831556769.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri1180273328702244000.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %8 : tensor<i16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi16>, tensor<1x3x5xi16>, tensor<i16>) -> tensor<2x4x6xi16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi16>, tensor<1x3x5xi16>, tensor<i16>) -> tensor<2x4x6xi16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi16>) -> tensor<2x4x6xi16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi16>, tensor<2x4x6xi16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri-799453658564734982.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %8 : tensor<i32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi32>, tensor<1x3x5xi32>, tensor<i32>) -> tensor<2x4x6xi32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi32>, tensor<1x3x5xi32>, tensor<i32>) -> tensor<2x4x6xi32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi32>) -> tensor<2x4x6xi32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi32>, tensor<2x4x6xi32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_int8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstrid-1158927479391781377.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i8>, %arg1: tensor<i8>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i8>
       stablehlo.return %8 : tensor<i8>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xi8>, tensor<1x3x5xi8>, tensor<i8>) -> tensor<2x4x6xi8>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xi8>, tensor<1x3x5xi8>, tensor<i8>) -> tensor<2x4x6xi8>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi8>) -> tensor<2x4x6xi8>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi8>, tensor<2x4x6xi8>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr2639417885255268371.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %8 : tensor<ui16>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xui16>, tensor<1x3x5xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xui16>, tensor<1x3x5xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui16>) -> tensor<2x4x6xui16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui16>, tensor<2x4x6xui16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstr4996434443108277595.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %8 : tensor<ui32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xui32>, tensor<1x3x5xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xui32>, tensor<1x3x5xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui32>) -> tensor<2x4x6xui32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui32>, tensor<2x4x6xui32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_dtypes_shape_uint8_2_4_6__selectprim_ge_windowdimensions__2__2__2__windowstri3918878843829093576.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui8>, %arg1: tensor<ui8>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui8>
       stablehlo.return %8 : tensor<ui8>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xui8>, tensor<1x3x5xui8>, tensor<ui8>) -> tensor<2x4x6xui8>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xui8>, tensor<1x3x5xui8>, tensor<ui8>) -> tensor<2x4x6xui8>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui8>) -> tensor<2x4x6xui8>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui8>, tensor<2x4x6xui8>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_padding_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__windows7745011263169515186.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<4x6x8xf32>, tensor<3x5x7xf32>, tensor<f32>) -> tensor<4x6x8xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<4x6x8xf32>, tensor<3x5x7xf32>, tensor<f32>) -> tensor<4x6x8xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 3, 5, 7>, start_indices = array<i64: 1, 1, 1>, strides = array<i64: 1, 1, 1>} : (tensor<4x6x8xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_select_prim_shape_float32_2_4_6__selectprim_le_windowdimensions__2__2__2__win3195252304676505603.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>} : (tensor<2x4x6xf32>, tensor<1x3x5xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_bfloat16_2_4_6__selectprim_ge_windowdimensions__1__3__1__win7138983918417685208.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<bf16>, %arg1: tensor<bf16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<bf16>
       stablehlo.return %8 : tensor<bf16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xbf16>, tensor<2x1x6xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xbf16>, tensor<2x1x6xbf16>, tensor<bf16>) -> tensor<2x4x6xbf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xbf16>) -> tensor<2x4x6xbf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xbf16>, tensor<2x4x6xbf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float16_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind7040864175077487301.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f16>, %arg1: tensor<f16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f16>
       stablehlo.return %8 : tensor<f16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xf16>, tensor<2x1x6xf16>, tensor<f16>) -> tensor<2x4x6xf16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xf16>, tensor<2x1x6xf16>, tensor<f16>) -> tensor<2x4x6xf16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf16>) -> tensor<2x4x6xf16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf16>, tensor<2x4x6xf16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1__wind-6867541439412093951.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<2x1x6xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xf32>, tensor<2x1x6xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int16_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-1515521222491375303.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i16>, %arg1: tensor<i16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i16>
       stablehlo.return %8 : tensor<i16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xi16>, tensor<2x1x6xi16>, tensor<i16>) -> tensor<2x4x6xi16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xi16>, tensor<2x1x6xi16>, tensor<i16>) -> tensor<2x4x6xi16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi16>) -> tensor<2x4x6xi16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi16>, tensor<2x4x6xi16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_int32_2_4_6__selectprim_ge_windowdimensions__1__3__1__window-8827730052770528042.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<i32>, %arg1: tensor<i32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<i32>
       stablehlo.return %8 : tensor<i32>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xi32>, tensor<2x1x6xi32>, tensor<i32>) -> tensor<2x4x6xi32>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xi32>, tensor<2x1x6xi32>, tensor<i32>) -> tensor<2x4x6xi32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xi32>) -> tensor<2x4x6xi32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xi32>, tensor<2x4x6xi32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint16_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo2756544409025303514.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui16>, %arg1: tensor<ui16>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui16>
       stablehlo.return %8 : tensor<ui16>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xui16>, tensor<2x1x6xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xui16>, tensor<2x1x6xui16>, tensor<ui16>) -> tensor<2x4x6xui16>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui16>) -> tensor<2x4x6xui16>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui16>, tensor<2x4x6xui16>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_tpu_dtypes_shape_uint32_2_4_6__selectprim_ge_windowdimensions__1__3__1__windo-6304561722695857269.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<ui32>, %arg1: tensor<ui32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<ui32>
       stablehlo.return %8 : tensor<ui32>
-    }) {window_dimensions = dense<[1, 3, 1]> : tensor<3xi64>, window_strides = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<2x4x6xui32>, tensor<2x1x6xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
+    }) {window_dimensions = array<i64: 1, 3, 1>, window_strides = array<i64: 1, 2, 1>} : (tensor<2x4x6xui32>, tensor<2x1x6xui32>, tensor<ui32>) -> tensor<2x4x6xui32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xui32>) -> tensor<2x4x6xui32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xui32>, tensor<2x4x6xui32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__2__2730457018571670567.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 3]> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<2x3x4xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 2, 3>} : (tensor<2x4x6xf32>, tensor<2x3x4xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir b/stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir
--- stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir
+++ stablehlo/stablehlo/testdata/select_and_scatter_add_window_strides_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__-3819633790000483811.mlir
@@ -18,7 +18,7 @@
     ^bb0(%arg0: tensor<f32>, %arg1: tensor<f32>):
       %8 = stablehlo.add %arg0, %arg1 : tensor<f32>
       stablehlo.return %8 : tensor<f32>
-    }) {window_dimensions = dense<2> : tensor<3xi64>, window_strides = dense<[1, 2, 3]> : tensor<3xi64>} : (tensor<2x4x6xf32>, tensor<1x2x2xf32>, tensor<f32>) -> tensor<2x4x6xf32>
+    }) {window_dimensions = array<i64: 2, 2, 2>, window_strides = array<i64: 1, 2, 3>} : (tensor<2x4x6xf32>, tensor<1x2x2xf32>, tensor<f32>) -> tensor<2x4x6xf32>
     %6 = "stablehlo.slice"(%5) {limit_indices = array<i64: 2, 4, 6>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>} : (tensor<2x4x6xf32>) -> tensor<2x4x6xf32>
     %7 = stablehlo.custom_call @check.eq(%6, %1) : (tensor<2x4x6xf32>, tensor<2x4x6xf32>) -> tensor<i1>
     return %7 : tensor<i1>
diff --ruN a/stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take__enable_xla_True_dynamic.mlir
@@ -35,7 +35,7 @@
     %24 = stablehlo.select %20, %23, %8 : tensor<1xi1>, tensor<1xi64>
     %25 = stablehlo.convert %24 : (tensor<1xi64>) -> tensor<1xi32>
     %26 = stablehlo.broadcast_in_dim %25, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %27 = "stablehlo.gather"(%16, %26) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %27 = "stablehlo.gather"(%16, %26) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %28 = stablehlo.convert %arg0 : tensor<i64>
     %29 = stablehlo.broadcast_in_dim %28, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %30 = stablehlo.constant dense<1> : tensor<i64>
@@ -52,7 +52,7 @@
     %41 = stablehlo.select %37, %40, %9 : tensor<1xi1>, tensor<1xi64>
     %42 = stablehlo.convert %41 : (tensor<1xi64>) -> tensor<1xi32>
     %43 = stablehlo.broadcast_in_dim %42, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %44 = "stablehlo.gather"(%34, %43) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %44 = "stablehlo.gather"(%34, %43) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<3xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %45 = stablehlo.subtract %27, %44 : tensor<1xi64>
     %46 = stablehlo.constant dense<0> : tensor<i64>
     %47 = stablehlo.broadcast_in_dim %46, dims = [] : (tensor<i64>) -> tensor<2x1xi64>
diff --ruN a/stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir b/stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
--- stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take_along_axis_0_dynamic.mlir
@@ -40,7 +40,7 @@
     %29 = stablehlo.select %25, %28, %16 : tensor<1xi1>, tensor<1xi64>
     %30 = stablehlo.convert %29 : (tensor<1xi64>) -> tensor<1xi32>
     %31 = stablehlo.broadcast_in_dim %30, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %32 = "stablehlo.gather"(%22, %31) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %32 = "stablehlo.gather"(%22, %31) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %33 = stablehlo.constant dense<1> : tensor<i64>
     %34 = stablehlo.broadcast_in_dim %33, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %35 = stablehlo.constant dense<2> : tensor<i64>
@@ -55,7 +55,7 @@
     %44 = stablehlo.select %40, %43, %17 : tensor<1xi1>, tensor<1xi64>
     %45 = stablehlo.convert %44 : (tensor<1xi64>) -> tensor<1xi32>
     %46 = stablehlo.broadcast_in_dim %45, dims = [0] : (tensor<1xi32>) -> tensor<1x1xi32>
-    %47 = "stablehlo.gather"(%37, %46) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
+    %47 = "stablehlo.gather"(%37, %46) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<1x1xi32>) -> tensor<1xi64>
     %48 = stablehlo.subtract %32, %47 : tensor<1xi64>
     %49 = stablehlo.constant dense<0> : tensor<i64>
     %50 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -78,7 +78,7 @@
       %79 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %79 : tensor<i1>
     }
-    %66 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<[1, 2]> : tensor<2xi64>} : (tensor<?x2xf32>, tensor<?x1xi64>) -> tensor<?x2xf32>
+    %66 = "stablehlo.gather"(%arg1, %15) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 2>} : (tensor<?x2xf32>, tensor<?x1xi64>) -> tensor<?x2xf32>
     %67 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %68 = stablehlo.reshape %67 : (tensor<i32>) -> tensor<1xi32>
     %69 = stablehlo.constant dense<2> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir b/stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
--- stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
+++ stablehlo/stablehlo/testdata/take_along_axis_1_dynamic.mlir
@@ -53,7 +53,7 @@
     %42 = stablehlo.select %38, %41, %29 : tensor<2xi1>, tensor<2xi64>
     %43 = stablehlo.convert %42 : (tensor<2xi64>) -> tensor<2xi32>
     %44 = stablehlo.broadcast_in_dim %43, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %45 = "stablehlo.gather"(%35, %44) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %45 = "stablehlo.gather"(%35, %44) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %46 = stablehlo.constant dense<1> : tensor<i64>
     %47 = stablehlo.broadcast_in_dim %46, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %48 = stablehlo.constant dense<1> : tensor<i64>
@@ -68,7 +68,7 @@
     %57 = stablehlo.select %53, %56, %30 : tensor<2xi1>, tensor<2xi64>
     %58 = stablehlo.convert %57 : (tensor<2xi64>) -> tensor<2xi32>
     %59 = stablehlo.broadcast_in_dim %58, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %60 = "stablehlo.gather"(%50, %59) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %60 = "stablehlo.gather"(%50, %59) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %61 = stablehlo.subtract %45, %60 : tensor<2xi64>
     %62 = stablehlo.constant dense<0> : tensor<i64>
     %63 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -93,7 +93,7 @@
       %89 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %89 : tensor<i1>
     }
-    %81 = "stablehlo.gather"(%arg1, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x2xf32>, tensor<?x1x2xi64>) -> tensor<?x1xf32>
+    %81 = "stablehlo.gather"(%arg1, %28) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1>} : (tensor<?x2xf32>, tensor<?x1x2xi64>) -> tensor<?x1xf32>
     %82 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %83 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %84 = stablehlo.reshape %83 : (tensor<i32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir b/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir
--- stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir
+++ stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float32_3.mlir
@@ -80,7 +80,7 @@
     %72 = stablehlo.broadcast_in_dim %71, dims = [] : (tensor<f32>) -> tensor<1xf32>
     %73 = stablehlo.constant dense<0> : tensor<i32>
     %74 = stablehlo.broadcast_in_dim %73, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<2> : tensor<1xi64>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
+    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 2>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
     %76 = call @append(%72, %75) : (tensor<1xf32>, tensor<2xf32>) -> tensor<3xf32>
     %77 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
     %78 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 3, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<3x1xf32>
@@ -173,7 +173,7 @@
     %165 = stablehlo.add %163, %164 : tensor<i32>
     %166 = stablehlo.convert %165 : tensor<i32>
     %167 = stablehlo.broadcast_in_dim %166, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
+    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
     %169 = stablehlo.reverse %162, dims = [0] : tensor<3x1xf32>
     %170 = stablehlo.reverse %61, dims = [0] : tensor<3xf32>
     %171 = "stablehlo.slice"(%169) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir b/stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir
--- stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir
+++ stablehlo/stablehlo/testdata/tridiagonal_solve_shape_float64_3.mlir
@@ -80,7 +80,7 @@
     %72 = stablehlo.broadcast_in_dim %71, dims = [] : (tensor<f32>) -> tensor<1xf32>
     %73 = stablehlo.constant dense<0> : tensor<i32>
     %74 = stablehlo.broadcast_in_dim %73, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<2> : tensor<1xi64>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
+    %75 = "stablehlo.gather"(%61, %74) {dimension_numbers = #stablehlo.gather<offset_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 2>} : (tensor<3xf32>, tensor<1xi32>) -> tensor<2xf32>
     %76 = call @append(%72, %75) : (tensor<1xf32>, tensor<2xf32>) -> tensor<3xf32>
     %77 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
     %78 = "stablehlo.slice"(%0#3) {limit_indices = array<i64: 3, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<3x1xf32>
@@ -173,7 +173,7 @@
     %165 = stablehlo.add %163, %164 : tensor<i32>
     %166 = stablehlo.convert %165 : tensor<i32>
     %167 = stablehlo.broadcast_in_dim %166, dims = [] : (tensor<i32>) -> tensor<1xi32>
-    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
+    %168 = "stablehlo.gather"(%162, %167) {dimension_numbers = #stablehlo.gather<offset_dims = [0], collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<3x1xf32>, tensor<1xi32>) -> tensor<1xf32>
     %169 = stablehlo.reverse %162, dims = [0] : tensor<3x1xf32>
     %170 = stablehlo.reverse %61, dims = [0] : tensor<3xf32>
     %171 = "stablehlo.slice"(%169) {limit_indices = array<i64: 0, 1>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>} : (tensor<3x1xf32>) -> tensor<0x1xf32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_dtypes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_3__start_indices__1___limit_indices__2___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x3xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__2__1__enablexla_True_dynamic.mlir
@@ -71,7 +71,7 @@
     %64 = stablehlo.concatenate %62, %63, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %65 = stablehlo.dynamic_iota %64, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %66 = stablehlo.concatenate %65, %60, dim = 1 : (tensor<?x1xi64>, tensor<?x2xi64>) -> tensor<?x3xi64>
-    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 1, 0]> : tensor<3xi64>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x1x0xf32>
+    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1, 0>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x1x0xf32>
     return %67 : tensor<?x1x0xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5_3__start_indices__1__1__limit_indices__3__2__enablexla_True_dynamic.mlir
@@ -71,7 +71,7 @@
     %64 = stablehlo.concatenate %62, %63, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %65 = stablehlo.dynamic_iota %64, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %66 = stablehlo.concatenate %65, %60, dim = 1 : (tensor<?x1xi64>, tensor<?x2xi64>) -> tensor<?x3xi64>
-    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 2, 1]> : tensor<3xi64>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x2x1xf32>
+    %67 = "stablehlo.gather"(%arg1, %66) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0], start_index_map = [0, 1, 2], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 2, 1>} : (tensor<?x5x3xf32>, tensor<?x3xi64>) -> tensor<?x2x1xf32>
     return %67 : tensor<?x2x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-100___limit_indices__-99___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-10___limit_indices__-9___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__-6___limit_indices__-5___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__1___limit_indices__5___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 4]> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x4xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 4>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x4xf32>
     return %36 : tensor<?x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_5__start_indices__5___limit_indices__6___enablexla_True_dynamic.mlir
@@ -40,7 +40,7 @@
     %33 = stablehlo.concatenate %31, %32, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %34 = stablehlo.dynamic_iota %33, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %35 = stablehlo.concatenate %34, %29, dim = 1 : (tensor<?x1xi64>, tensor<?x1xi64>) -> tensor<?x2xi64>
-    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
+    %36 = "stablehlo.gather"(%arg1, %35) {dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2xi64>) -> tensor<?x1xf32>
     return %36 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_dynamic_slice_shapes_a_float32_7_5_3__start_indices__4__0__1__limit_indices__7__1__3__enablexla_True_dynamic.mlir
@@ -101,7 +101,7 @@
     %94 = stablehlo.concatenate %92, %93, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %95 = stablehlo.dynamic_iota %94, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi64>
     %96 = stablehlo.concatenate %95, %90, dim = 1 : (tensor<?x1xi64>, tensor<?x3xi64>) -> tensor<?x4xi64>
-    %97 = "stablehlo.gather"(%arg1, %96) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2, 3], collapsed_slice_dims = [0], start_index_map = [0, 1, 2, 3], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 3, 1, 2]> : tensor<4xi64>} : (tensor<?x7x5x3xf32>, tensor<?x4xi64>) -> tensor<?x3x1x2xf32>
+    %97 = "stablehlo.gather"(%arg1, %96) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2, 3], collapsed_slice_dims = [0], start_index_map = [0, 1, 2, 3], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = array<i64: 1, 3, 1, 2>} : (tensor<?x7x5x3xf32>, tensor<?x4xi64>) -> tensor<?x3x1x2xf32>
     return %97 : tensor<?x3x1x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__1__2__start_indices_shape__1__2__slice_sizes__1__1__enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x1x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x1x1xi64>, tensor<?x1x2xi64>) -> tensor<?x1x3xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = dense<1> : tensor<3xi64>} : (tensor<?x1x2xf32>, tensor<?x1x3xi64>) -> tensor<?x1xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1, 2], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 1>} : (tensor<?x1x2xf32>, tensor<?x1x3xi64>) -> tensor<?x1xf32>
     return %7 : tensor<?x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__2__3__3__start_indices_shape__2__3__slice_sizes__1__3__2__enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x2x1xi64>, tensor<?x2x3xi64>) -> tensor<?x2x4xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2, 3], index_vector_dim = 2>, slice_sizes = dense<[1, 1, 3, 2]> : tensor<4xi64>} : (tensor<?x2x3x3xf32>, tensor<?x2x4xi64>) -> tensor<?x2x3x2xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2, 3], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 3, 2>} : (tensor<?x2x3x3xf32>, tensor<?x2x4xi64>) -> tensor<?x2x3x2xf32>
     return %7 : tensor<?x2x3x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_batchdims_shape__4__6__start_indices_shape__4__2__slice_sizes__1__3__enable_xla_True_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x4x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x4x1xi64>, tensor<?x4x2xi64>) -> tensor<?x4x3xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 1, 3]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x3xi64>) -> tensor<?x4x3xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 3>} : (tensor<?x4x6xf32>, tensor<?x4x3xi64>) -> tensor<?x4x3xf32>
     return %7 : tensor<?x4x3xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_dtypes_shape_float32_10__axis_0_enable_xla_True_dynamic.mlir
@@ -47,7 +47,7 @@
     %36 = stablehlo.select %32, %35, %22 : tensor<2xi1>, tensor<2xi64>
     %37 = stablehlo.convert %36 : (tensor<2xi64>) -> tensor<2xi32>
     %38 = stablehlo.broadcast_in_dim %37, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %39 = "stablehlo.gather"(%28, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %39 = "stablehlo.gather"(%28, %38) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %40 = stablehlo.constant dense<1> : tensor<i64>
     %41 = stablehlo.broadcast_in_dim %40, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %42 = stablehlo.constant dense<1> : tensor<i64>
@@ -62,7 +62,7 @@
     %51 = stablehlo.select %47, %50, %23 : tensor<2xi1>, tensor<2xi64>
     %52 = stablehlo.convert %51 : (tensor<2xi64>) -> tensor<2xi32>
     %53 = stablehlo.broadcast_in_dim %52, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %54 = "stablehlo.gather"(%44, %53) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %54 = "stablehlo.gather"(%44, %53) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<2xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %55 = stablehlo.subtract %39, %54 : tensor<2xi64>
     %56 = stablehlo.constant dense<0> : tensor<i64>
     %57 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -85,7 +85,7 @@
       %79 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %79 : tensor<i1>
     }
-    %73 = "stablehlo.gather"(%arg1, %29) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x10xf32>, tensor<?x2xi64>) -> tensor<?xf32>
+    %73 = "stablehlo.gather"(%arg1, %29) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1>} : (tensor<?x10xf32>, tensor<?x2xi64>) -> tensor<?xf32>
     %74 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
     %75 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %76 = stablehlo.reshape %75 : (tensor<i32>) -> tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -19,7 +19,7 @@
     %8 = stablehlo.concatenate %6, %7, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %9 = stablehlo.dynamic_iota %8, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi32>
     %10 = stablehlo.concatenate %9, %4, dim = 1 : (tensor<?x1xi32>, tensor<?x1xi32>) -> tensor<?x2xi32>
-    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
+    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
     return %11 : tensor<?x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_0_enable_xla_True_mode_fill_dynamic.mlir
@@ -51,7 +51,7 @@
     %40 = stablehlo.select %36, %39, %22 : tensor<2xi1>, tensor<2xi64>
     %41 = stablehlo.convert %40 : (tensor<2xi64>) -> tensor<2xi32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %44 = stablehlo.constant dense<1> : tensor<i64>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %46 = stablehlo.constant dense<1> : tensor<i64>
@@ -70,7 +70,7 @@
     %59 = stablehlo.select %55, %58, %23 : tensor<2xi1>, tensor<2xi64>
     %60 = stablehlo.convert %59 : (tensor<2xi64>) -> tensor<2xi32>
     %61 = stablehlo.broadcast_in_dim %60, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %63 = stablehlo.subtract %43, %62 : tensor<2xi64>
     %64 = stablehlo.constant dense<0> : tensor<i64>
     %65 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -93,7 +93,7 @@
       %96 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %96 : tensor<i1>
     }
-    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
+    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
     %82 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %83 = stablehlo.reshape %82 : (tensor<i32>) -> tensor<1xi32>
     %84 = stablehlo.constant dense<10> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -19,7 +19,7 @@
     %8 = stablehlo.concatenate %6, %7, dim = 0 : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
     %9 = stablehlo.dynamic_iota %8, dim = 0 : (tensor<2xi32>) -> tensor<?x1xi32>
     %10 = stablehlo.concatenate %9, %4, dim = 1 : (tensor<?x1xi32>, tensor<?x1xi32>) -> tensor<?x2xi32>
-    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
+    %11 = "stablehlo.gather"(%arg1, %10) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2xi32>) -> tensor<?x10x10xf32>
     return %11 : tensor<?x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__1__axis_2_enable_xla_True_mode_fill_dynamic.mlir
@@ -51,7 +51,7 @@
     %40 = stablehlo.select %36, %39, %22 : tensor<2xi1>, tensor<2xi64>
     %41 = stablehlo.convert %40 : (tensor<2xi64>) -> tensor<2xi32>
     %42 = stablehlo.broadcast_in_dim %41, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %43 = "stablehlo.gather"(%32, %42) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %44 = stablehlo.constant dense<1> : tensor<i64>
     %45 = stablehlo.broadcast_in_dim %44, dims = [] : (tensor<i64>) -> tensor<1xi64>
     %46 = stablehlo.constant dense<10> : tensor<i64>
@@ -70,7 +70,7 @@
     %59 = stablehlo.select %55, %58, %23 : tensor<2xi1>, tensor<2xi64>
     %60 = stablehlo.convert %59 : (tensor<2xi64>) -> tensor<2xi32>
     %61 = stablehlo.broadcast_in_dim %60, dims = [0] : (tensor<2xi32>) -> tensor<2x1xi32>
-    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
+    %62 = "stablehlo.gather"(%52, %61) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1>} : (tensor<4xi64>, tensor<2x1xi32>) -> tensor<2xi64>
     %63 = stablehlo.subtract %43, %62 : tensor<2xi64>
     %64 = stablehlo.constant dense<0> : tensor<i64>
     %65 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
@@ -93,7 +93,7 @@
       %96 = stablehlo.and %arg3, %arg4 : tensor<i1>
       stablehlo.return %96 : tensor<i1>
     }
-    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
+    %81 = "stablehlo.gather"(%arg1, %33) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2xi64>) -> tensor<?x10x10xf32>
     %82 = stablehlo.convert %arg0 : (tensor<i64>) -> tensor<i32>
     %83 = stablehlo.reshape %82 : (tensor<i32>) -> tensor<1xi32>
     %84 = stablehlo.constant dense<10> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__2__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x1x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x1x1xi32>, tensor<?x1x1xi32>) -> tensor<?x1x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x1x2xi32>) -> tensor<?x10x1x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x1x2xi32>) -> tensor<?x10x1x10xf32>
     return %13 : tensor<?x10x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x2x1xi32>, tensor<?x2x1xi32>) -> tensor<?x2x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x2x10x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [2, 3], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x2x10x10xf32>
     return %13 : tensor<?x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x2x1xi32>, tensor<?x2x1xi32>) -> tensor<?x2x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x10x10x2xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xi32>) -> tensor<?x10x10x2xf32>
     return %13 : tensor<?x10x10x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__3_uint32__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xui32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x2x1xui32>, tensor<?x2x1xui32>) -> tensor<?x2x2xui32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xui32>) -> tensor<?x10x2x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2xui32>) -> tensor<?x10x2x10xf32>
     return %13 : tensor<?x10x2x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
     return %15 : tensor<?x2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__4__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
     return %15 : tensor<?x10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__5_oob__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x3x1x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x3x1x1xi32>, tensor<?x3x1x1xi32>) -> tensor<?x3x1x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 4], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x3x1x2xi32>) -> tensor<?x10x3x1x10xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 4], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x3x1x2xi32>) -> tensor<?x10x3x1x10xf32>
     return %15 : tensor<?x10x3x1x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [3, 4], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 3>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x2x2x10x10xf32>
     return %15 : tensor<?x2x2x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__6_neg__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -23,7 +23,7 @@
     %12 = stablehlo.concatenate %8, %9, %10, %11, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<4xi32>
     %13 = stablehlo.dynamic_iota %12, dim = 0 : (tensor<4xi32>) -> tensor<?x2x2x1xi32>
     %14 = stablehlo.concatenate %13, %6, dim = 3 : (tensor<?x2x2x1xi32>, tensor<?x2x2x1xi32>) -> tensor<?x2x2x2xi32>
-    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
+    %15 = "stablehlo.gather"(%arg1, %14) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 3>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x2xi32>) -> tensor<?x10x10x2x2xf32>
     return %15 : tensor<?x10x10x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__7_neg__axis_1_enable_xla_True_mode_clip_dynamic.mlir
@@ -21,7 +21,7 @@
     %10 = stablehlo.concatenate %7, %8, %9, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %11 = stablehlo.dynamic_iota %10, dim = 0 : (tensor<3xi32>) -> tensor<?x5x1xi32>
     %12 = stablehlo.concatenate %11, %5, dim = 2 : (tensor<?x5x1xi32>, tensor<?x5x1xi32>) -> tensor<?x5x2xi32>
-    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = dense<[1, 10, 1, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x5x2xi32>) -> tensor<?x10x5x10xf32>
+    %13 = "stablehlo.gather"(%arg1, %12) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 3], collapsed_slice_dims = [0, 2], start_index_map = [0, 2], index_vector_dim = 2>, slice_sizes = array<i64: 1, 10, 1, 10>} : (tensor<?x10x10x10xf32>, tensor<?x5x2xi32>) -> tensor<?x10x5x10xf32>
     return %13 : tensor<?x10x5x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_0_enable_xla_True_mode_clip_dynamic.mlir
@@ -25,7 +25,7 @@
     %14 = stablehlo.concatenate %9, %10, %11, %12, %13, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<5xi32>
     %15 = stablehlo.dynamic_iota %14, dim = 0 : (tensor<5xi32>) -> tensor<?x2x2x1x1xi32>
     %16 = stablehlo.concatenate %15, %7, dim = 4 : (tensor<?x2x2x1x1xi32>, tensor<?x2x2x1x1xi32>) -> tensor<?x2x2x1x2xi32>
-    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [4, 5], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 4>, slice_sizes = dense<[1, 1, 10, 10]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x2x2x1x10x10xf32>
+    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [4, 5], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 4>, slice_sizes = array<i64: 1, 1, 10, 10>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x2x2x1x10x10xf32>
     return %17 : tensor<?x2x2x1x10x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_from_take_indices_name__8_neg_oob__axis_2_enable_xla_True_mode_clip_dynamic.mlir
@@ -25,7 +25,7 @@
     %14 = stablehlo.concatenate %9, %10, %11, %12, %13, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<5xi32>
     %15 = stablehlo.dynamic_iota %14, dim = 0 : (tensor<5xi32>) -> tensor<?x2x2x1x1xi32>
     %16 = stablehlo.concatenate %15, %7, dim = 4 : (tensor<?x2x2x1x1xi32>, tensor<?x2x2x1x1xi32>) -> tensor<?x2x2x1x2xi32>
-    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 4>, slice_sizes = dense<[1, 10, 10, 1]> : tensor<4xi64>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x10x10x2x2x1xf32>
+    %17 = "stablehlo.gather"(%arg1, %16) {dimension_numbers = #stablehlo.gather<offset_dims = [1, 2], collapsed_slice_dims = [0, 3], start_index_map = [0, 3], index_vector_dim = 4>, slice_sizes = array<i64: 1, 10, 10, 1>} : (tensor<?x10x10x10xf32>, tensor<?x2x2x1x2xi32>) -> tensor<?x10x10x2x2x1xf32>
     return %17 : tensor<?x10x10x2x2x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_shape__10___idxs_shape__3__1__dnums_GatherDimensionNumbers_offset_dims__1____collapsed_s-3138458249020037857_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x3x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x3x1xi64>, tensor<?x3x1xi64>) -> tensor<?x3x2xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<[1, 2]> : tensor<2xi64>} : (tensor<?x10xf32>, tensor<?x3x2xi64>) -> tensor<?x3x2xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<offset_dims = [2], collapsed_slice_dims = [0], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 2>} : (tensor<?x10xf32>, tensor<?x3x2xi64>) -> tensor<?x3x2xf32>
     return %7 : tensor<?x3x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_gather_shape__5___idxs_shape__2__1__dnums_GatherDimensionNumbers_offset_dims_____collapsed_slic-4256662786843852688_dynamic.mlir
@@ -11,7 +11,7 @@
     %4 = stablehlo.concatenate %1, %2, %3, dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
     %5 = stablehlo.dynamic_iota %4, dim = 0 : (tensor<3xi32>) -> tensor<?x2x1xi64>
     %6 = stablehlo.concatenate %5, %arg2, dim = 2 : (tensor<?x2x1xi64>, tensor<?x2x1xi64>) -> tensor<?x2x2xi64>
-    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = dense<1> : tensor<2xi64>} : (tensor<?x5xf32>, tensor<?x2x2xi64>) -> tensor<?x2xf32>
+    %7 = "stablehlo.gather"(%arg1, %6) {dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 2>, slice_sizes = array<i64: 1, 1>} : (tensor<?x5xf32>, tensor<?x2x2xi64>) -> tensor<?x2xf32>
     return %7 : tensor<?x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_base_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstr-763947363108393640_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {base_dilations = dense<[1, 1, 2]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x10xf32>
+    }) {base_dilations = array<i64: 1, 1, 2>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x10xf32>
     return %2 : tensor<?x3x10xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_dtypes_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windowstrides__13341440039283118536_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
     return %2 : tensor<?x3x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__2_2__wind3919194238712771376_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x12x12xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x12x12xf32>
     return %2 : tensor<?x12x12xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_manual_padding_shape_float32_12_12__initvalue_0_0_windowdimensions__3_3__wind-1621514835767157889_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 3, 3]> : tensor<3xi64>, window_strides = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x6x6xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 3, 3>, window_strides = array<i64: 1, 2, 2>} : (tensor<?x12x12xf32>, tensor<f32>) -> tensor<?x6x6xf32>
     return %2 : tensor<?x6x6xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dilation_shape_float32_4_6__initvalue_0_windowdimensions__2_2__windows-6742770852540702874_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dilations = dense<[1, 1, 2]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x4xf32>
+    }) {window_dilations = array<i64: 1, 1, 2>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x4xf32>
     return %2 : tensor<?x3x4xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_add_window_dimensions_shape_float32_4_6__initvalue_0_windowdimensions__1_1__windo3189028916486005793_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.add %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<1> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1, 1>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x4x6xf32>
     return %2 : tensor<?x4x6xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2_4_1__initvalue_-inf_windowdimensions__1_2184160813763330522_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 2, 2, 1]> : tensor<5xi64>} : (tensor<?x1x2x4x1xf32>, tensor<f32>) -> tensor<?x1x1x3x1xf32>
+    }) {window_dimensions = array<i64: 1, 1, 2, 2, 1>} : (tensor<?x1x2x4x1xf32>, tensor<f32>) -> tensor<?x1x1x3x1xf32>
     return %2 : tensor<?x1x1x3x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_1_2__initvalue_-inf_windowdimensions__1_2__w3223090226801945110_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 2]> : tensor<3xi64>} : (tensor<?x1x2xf32>, tensor<f32>) -> tensor<?x1x1xf32>
+    }) {window_dimensions = array<i64: 1, 1, 2>} : (tensor<?x1x2xf32>, tensor<f32>) -> tensor<?x1x1xf32>
     return %2 : tensor<?x1x1xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4_3__initvalue_-inf_windowdimensions__2_2_5486644682491296121_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 2]> : tensor<4xi64>} : (tensor<?x2x4x3xf32>, tensor<f32>) -> tensor<?x1x3x2xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 2>} : (tensor<?x2x4x3xf32>, tensor<f32>) -> tensor<?x1x3x2xf32>
     return %2 : tensor<?x1x3x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_batch_channel_dims_shape_float32_2_4__initvalue_-inf_windowdimensions__2_2__w-6307575688664696782_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x2x4xf32>, tensor<f32>) -> tensor<?x1x3xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x2x4xf32>, tensor<f32>) -> tensor<?x1x3xf32>
     return %2 : tensor<?x1x3xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_dtypes_shape_float32_4_6__initvalue_1_windowdimensions__2_2__windowstrides__11940131758519929992_dynamic.mlir
@@ -9,7 +9,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %2 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %2 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<f32>) -> tensor<?x3x5xf32>
     return %1 : tensor<?x3x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_max_same_padding_shape_float32_112_112__initvalue_-inf_windowdimensions__3_3__win2956379201356537115_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.maximum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 3, 3]> : tensor<3xi64>, window_strides = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 3, 3>, window_strides = array<i64: 1, 2, 2>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
     return %2 : tensor<?x56x56xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_min_same_padding_shape_float32_112_112__initvalue_inf_windowdimensions__3_3__wind-2185529473609255265_dynamic.mlir
@@ -10,7 +10,7 @@
     ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
       %3 = stablehlo.minimum %arg2, %arg3 : tensor<f32>
       stablehlo.return %3 : tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 3, 3]> : tensor<3xi64>, window_strides = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 3, 3>, window_strides = array<i64: 1, 2, 2>} : (tensor<?x112x112xf32>, tensor<f32>) -> tensor<?x56x56xf32>
     return %2 : tensor<?x56x56xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_reduce_window_mul_dtypes_shape_bfloat16_4_6__initvalue_0_windowdimensions__2_2__windowstrides__-7573250183158637827_dynamic.mlir
@@ -9,7 +9,7 @@
     ^bb0(%arg2: tensor<bf16>, %arg3: tensor<bf16>):
       %2 = stablehlo.multiply %arg2, %arg3 : tensor<bf16>
       stablehlo.return %2 : tensor<bf16>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xbf16>, tensor<bf16>) -> tensor<?x3x5xbf16>
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xbf16>, tensor<bf16>) -> tensor<?x3x5xbf16>
     return %1 : tensor<?x3x5xbf16>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window-3668408315562044057_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {base_dilations = dense<[1, 2, 3]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x6x15xf32>, tensor<?x6x15xf32>)
+    }) {base_dilations = array<i64: 1, 2, 3>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x6x15xf32>, tensor<?x6x15xf32>)
     return %2#1 : tensor<?x6x15xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dilations_shape_float32_4_6__selectprim_le_windowdimensions__2__2__window5641367141920285965_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {window_dilations = dense<[1, 2, 3]> : tensor<3xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x3xf32>, tensor<?x2x3xf32>)
+    }) {window_dilations = array<i64: 1, 2, 3>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x3xf32>, tensor<?x2x3xf32>)
     return %2#1 : tensor<?x2x3xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_dtypes_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowstr9035867587788163605_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x3x5xf32>, tensor<?x3x5xf32>)
+    }) {window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x3x5xf32>, tensor<?x3x5xf32>)
     return %2#1 : tensor<?x3x5xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_padding_shape_float32_4_6__selectprim_le_windowdimensions__2__2__windowst-1651705789938795959_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x4x6xf32>, tensor<?x4x6xf32>)
+    }) {padding = dense<[[0, 0], [0, 1], [0, 1]]> : tensor<3x2xi64>, window_dimensions = array<i64: 1, 2, 2>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x4x6xf32>, tensor<?x4x6xf32>)
     return %2#1 : tensor<?x4x6xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_gather_add_window_strides_shape_float32_4_6__selectprim_le_windowdimensions__2__2__w-7130227668472867832_dynamic.mlir
@@ -12,7 +12,7 @@
       %4 = stablehlo.select %3, %arg3, %arg5 : tensor<i1>, tensor<f32>
       %5 = stablehlo.select %3, %arg4, %arg6 : tensor<i1>, tensor<f32>
       stablehlo.return %4, %5 : tensor<f32>, tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2]> : tensor<3xi64>, window_strides = dense<[1, 2, 3]> : tensor<3xi64>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x2xf32>, tensor<?x2x2xf32>)
+    }) {window_dimensions = array<i64: 1, 2, 2>, window_strides = array<i64: 1, 2, 3>} : (tensor<?x4x6xf32>, tensor<?x4x6xf32>, tensor<f32>, tensor<f32>) -> (tensor<?x2x2xf32>, tensor<?x2x2xf32>)
     return %2#1 : tensor<?x2x2xf32>
   }
 }
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__2__2__2__win-6232315144219558965_dynamic.mlir
@@ -15,7 +15,7 @@
     ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
       %21 = stablehlo.add %arg3, %arg4 : tensor<f32>
       stablehlo.return %21 : tensor<f32>
-    }) {window_dimensions = dense<[1, 2, 2, 2]> : tensor<4xi64>} : (tensor<?x2x4x6xf32>, tensor<?x1x3x5xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 2, 2, 2>} : (tensor<?x2x4x6xf32>, tensor<?x1x3x5xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
     %5 = stablehlo.constant dense<0> : tensor<1xi32>
     %6 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_tpu_dtypes_shape_float32_2_4_6__selectprim_ge_windowdimensions__1__3__1_7476758622011829286_dynamic.mlir
@@ -15,7 +15,7 @@
     ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
       %21 = stablehlo.add %arg3, %arg4 : tensor<f32>
       stablehlo.return %21 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 3, 1]> : tensor<4xi64>, window_strides = dense<[1, 1, 2, 1]> : tensor<4xi64>} : (tensor<?x2x4x6xf32>, tensor<?x2x1x6xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1, 3, 1>, window_strides = array<i64: 1, 1, 2, 1>} : (tensor<?x2x4x6xf32>, tensor<?x2x1x6xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
     %5 = stablehlo.constant dense<0> : tensor<1xi32>
     %6 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir b/stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir
--- stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir
+++ stablehlo/stablehlo/testdata/vmap_select_and_scatter_add_window_dimensions_shape_float32_2_4_6__selectprim_ge_windowdimensions__13419466118969880708_dynamic.mlir
@@ -15,7 +15,7 @@
     ^bb0(%arg3: tensor<f32>, %arg4: tensor<f32>):
       %21 = stablehlo.add %arg3, %arg4 : tensor<f32>
       stablehlo.return %21 : tensor<f32>
-    }) {window_dimensions = dense<[1, 1, 2, 3]> : tensor<4xi64>} : (tensor<?x2x4x6xf32>, tensor<?x2x3x4xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
+    }) {window_dimensions = array<i64: 1, 1, 2, 3>} : (tensor<?x2x4x6xf32>, tensor<?x2x3x4xf32>, tensor<f32>) -> tensor<?x2x4x6xf32>
     %4 = stablehlo.constant dense<0> : tensor<1xi32>
     %5 = stablehlo.constant dense<0> : tensor<1xi32>
     %6 = stablehlo.constant dense<0> : tensor<1xi32>
diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo.mlir b/stablehlo/stablehlo/tests/ops_stablehlo.mlir
--- stablehlo/stablehlo/tests/ops_stablehlo.mlir
+++ stablehlo/stablehlo/tests/ops_stablehlo.mlir
@@ -1122,14 +1122,6 @@
 func.func @broadcast_in_dim_c5(%arg0: tensor<3xi32>) -> tensor<1x2x3xi32> {
   // expected-error@+1 {{size of operand dimension 0 (3) is not equal to 1 or size of result dimension 1 (2)}}
   %0 = "stablehlo.broadcast_in_dim"(%arg0) {broadcast_dimensions = array<i64: 1>} : (tensor<3xi32>) -> tensor<1x2x3xi32>
-  func.return %0 : tensor<1x2x3xi32>
-}
-
-// -----
-
-func.func @broadcast_in_dim_i2(%arg0: tensor<1x2xi32>) -> tensor<1x2x3xi32> {
-  // expected-error@+1 {{failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr}}
-  %0 = "stablehlo.broadcast_in_dim"(%arg0) {broadcast_dimensions = dense<[[1,1],[1,1]]> : tensor<2x2xi64>} : (tensor<1x2xi32>) -> tensor<1x2x3xi32>
   func.return %0 : tensor<1x2x3xi32>
 }
 
@@ -1959,18 +1951,6 @@
 
 // -----
 
-func.func @map_i2(%arg0: tensor<4x5xf32>, %arg1: tensor<4x5xf32>) -> tensor<4x5xf32> {
-  // expected-error@+1 {{attribute 'dimensions' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0 = "stablehlo.map"(%arg0, %arg1) ({
-    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):
-    %1 = stablehlo.constant dense<2.0> : tensor<f32>
-    "stablehlo.return"(%1) : (tensor<f32>) -> ()
-  }) {dimensions = dense<[[0, 1]]> : tensor<1x2xi64>} : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<4x5xf32>
-  func.return %0 : tensor<4x5xf32>
-}
-
-// -----
-
 // CHECK-LABEL: func @map_scalar_operands
 func.func @map_scalar_operands(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
   %0 = "stablehlo.map"(%arg0, %arg1) ({
@@ -4030,23 +4010,6 @@
     slice_sizes = array<i64: 1, 1, 8, 1, 7, 1, 6, 1>
   } : (tensor<*xi32>, tensor<?x?x?xi32>) -> tensor<3xi32>
   func.return %res : tensor<3xi32>
-}
-
-// -----
-
-func.func @gather_i7(%operand : tensor<2x4x9xi32>, %start_indices : tensor<1x5x2xi32>) -> tensor<1x5x8xi32> {
-  // expected-error@+1 {{attribute 'slice_sizes' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %res = "stablehlo.gather"(%operand, %start_indices) {
-    dimension_numbers = #stablehlo.gather<
-      offset_dims = [2],
-      collapsed_slice_dims = [0, 1],
-      start_index_map = [0, 1],
-      index_vector_dim = 2
-    >,
-    slice_sizes = dense<[[1, 1, 8]]> : tensor<1x3xi64>,
-    indices_are_sorted = false
-  } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
-  func.return %res : tensor<1x5x8xi32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
@@ -279,7 +279,7 @@
   // CHECK-SAME:     start_index_map = [0, 1],
   // CHECK-SAME:     index_vector_dim = 2
   // CHECK-SAME:   >,
-  // CHECK-SAME:   slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+  // CHECK-SAME:   slice_sizes = array<i64: 1, 1, 8>
   // CHECK-SAME: } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   %0 = stablehlo.constant dense<[1, 1, 8]> : tensor<3xi32>
   %1 = "stablehlo.dynamic_gather"(%arg0, %arg1, %0) {
@@ -305,7 +305,7 @@
   // CHECK-SAME:     start_index_map = [0, 1],
   // CHECK-SAME:     index_vector_dim = 2
   // CHECK-SAME:   >,
-  // CHECK-SAME:   slice_sizes = dense<[1, 1, 8]> : tensor<3xi64>
+  // CHECK-SAME:   slice_sizes = array<i64: 1, 1, 8>
   // CHECK-SAME: } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x?xi32>
   %0 = stablehlo.constant dense<[1, 1, 8]> : tensor<3xi32>
   %1 = "stablehlo.dynamic_gather"(%arg0, %arg1, %0) {
diff --ruN a/stablehlo/stablehlo/tests/verify_reduce.mlir b/stablehlo/stablehlo/tests/verify_reduce.mlir
--- stablehlo/stablehlo/tests/verify_reduce.mlir
+++ stablehlo/stablehlo/tests/verify_reduce.mlir
@@ -486,19 +486,6 @@
   return %0 : tensor<4x!quant.uniform<i32:f32, 2.000000e+00:15>>
 }
 
-// -----
-
-func.func @reduce_i3(%input: tensor<1x6xi64>, %init_value: tensor<i64>) -> tensor<1xi64> {
-  // expected-error@+1 {{attribute 'dimensions' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0 = "stablehlo.reduce"(%input, %init_value) ({
-    ^bb0(%arg0: tensor<i64>, %arg1: tensor<i64>):
-      stablehlo.return %arg0 : tensor<i64>
-  }) {
-    dimensions = dense<1> : tensor<1x1xi64>
-  } : (tensor<1x6xi64>, tensor<i64>) -> tensor<1xi64>
-  func.return %0 : tensor<1xi64>
-}
-
 // The following invalid cases arises while parsing a pretty-printed version of reduce-op will "non-eligible" inner-op.
 // -----
 
diff --ruN a/stablehlo/stablehlo/tests/verify_reduce_window.mlir b/stablehlo/stablehlo/tests/verify_reduce_window.mlir
--- stablehlo/stablehlo/tests/verify_reduce_window.mlir
+++ stablehlo/stablehlo/tests/verify_reduce_window.mlir
@@ -95,7 +95,7 @@
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
            window_dimensions = array<i64: 5, 1>,
-           window_strides = array<i64: 3, 1>}
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -681,94 +681,6 @@
 
 // -----
 
-func.func @reduce_window_i3(%arg0: tensor<4x2xf32>, %arg1: tensor<4x2xi32>,
-                    %init0: tensor<f32>, %init1: tensor<i32>) ->
-                      (tensor<2x2xf32>, tensor<2x2xi32>) {
-  // expected-error@+1 {{attribute 'window_dimensions' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[[5, 1]]> : tensor<1x2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
-         : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<2x2xf32>, tensor<2x2xi32>)
-  func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
-}
-
-// -----
-
-func.func @reduce_window_i4(%arg0: tensor<4x2xf32>, %arg1: tensor<4x2xi32>,
-                    %init0: tensor<f32>, %init1: tensor<i32>) ->
-                      (tensor<2x2xf32>, tensor<2x2xi32>) {
-  // expected-error@+1 {{attribute 'window_strides' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[[3, 1]]> : tensor<1x2xi64> }
-         : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<2x2xf32>, tensor<2x2xi32>)
-  func.return %0#0, %0#1 : tensor<2x2xf32>, tensor<2x2xi32>
-}
-
-// -----
-
-func.func @reduce_window_i5(%arg0: tensor<*xf32>,
-    %arg1: tensor<4x?xi32>, %init0: tensor<f32>, %init1: tensor<i32>) ->
-        (tensor<?x?xf32>, tensor<*xi32>) {
-  // expected-error@+1 {{attribute 'base_dilations' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[[1, 1]]> : tensor<1x2xi64>,
-           window_dilations = dense<[1, 1]> : tensor<2xi64> }
-         : (tensor<*xf32>, tensor<4x?xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<?x?xf32>, tensor<*xi32>)
-  func.return %0#0, %0#1 : tensor<?x?xf32>, tensor<*xi32>
-}
-
-// -----
-
-func.func @reduce_window_i6(%arg0: tensor<*xf32>,
-    %arg1: tensor<4x?xi32>, %init0: tensor<f32>, %init1: tensor<i32>) ->
-        (tensor<?x?xf32>, tensor<*xi32>) {
-  // expected-error@+1 {{attribute 'window_dilations' failed to satisfy constraint: either a DenseI64ArrayAttr or a 1-dimensional I64ElementsAttr.}}
-  %0:2 = "stablehlo.reduce_window"(%arg0, %arg1, %init0, %init1) ({
-         ^bb0(%a0: tensor<f32>, %a1: tensor<i32>,
-                %b0: tensor<f32>, %b1: tensor<i32>):
-              %2 = stablehlo.add %a0, %b0 : tensor<f32>
-              %3 = stablehlo.add %a1, %b1 : tensor<i32>
-              "stablehlo.return"(%2, %3) : (tensor<f32>, tensor<i32>) -> ()
-            })
-         { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64>,
-           base_dilations = dense<[1, 1]> : tensor<2xi64>,
-           window_dilations = dense<[[1, 1]]> : tensor<1x2xi64> }
-         : (tensor<*xf32>, tensor<4x?xi32>, tensor<f32>, tensor<i32>) ->
-              (tensor<?x?xf32>, tensor<*xi32>)
-  func.return %0#0, %0#1 : tensor<?x?xf32>, tensor<*xi32>
-}
-
-// -----
-
 func.func @reduce_window_i7_c12(%arg0: tensor<4x2xf32>, %arg1: tensor<4x2xi32>,
                     %init0: tensor<f32>, %init1: tensor<i32>) ->
                       (tensor<2x2xf32>, tensor<2x2xi32>) {
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_10_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1610,7 +1610,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1671,10 +1671,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1828,8 +1828,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_11_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1610,7 +1610,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1671,10 +1671,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1828,8 +1828,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_12_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1610,7 +1610,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1671,10 +1671,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1828,8 +1828,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_13_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1610,7 +1610,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1671,10 +1671,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1828,8 +1828,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_14_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1610,7 +1610,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1671,10 +1671,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1828,8 +1828,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_15_0.mlir
@@ -585,7 +585,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -676,7 +676,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -734,7 +734,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1062,11 +1062,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1215,11 +1215,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1369,7 +1369,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1485,7 +1485,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1618,7 +1618,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1679,10 +1679,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1836,8 +1836,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_16_0.mlir
@@ -596,7 +596,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -687,7 +687,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -745,7 +745,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1073,11 +1073,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1226,11 +1226,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1380,7 +1380,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1496,7 +1496,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1629,7 +1629,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1690,10 +1690,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1847,8 +1847,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_17_0.mlir
@@ -594,7 +594,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -685,7 +685,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -743,7 +743,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1090,11 +1090,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1243,11 +1243,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1397,7 +1397,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1513,7 +1513,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1646,7 +1646,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1676,7 +1676,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
 
   func.return %0: tensor<4xf64>
 }
@@ -1742,10 +1742,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1767,8 +1767,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+         window_dimensions = array<i64: 5, 1>,
+         window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -1948,8 +1948,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
@@ -1971,8 +1971,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f64>, tensor<f64>) -> tensor<f64>
       "stablehlo.return"(%1) : (tensor<f64>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf64>
   func.return %0 : tensor<10x24x24x64xf64>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.0_9_0.mlir
@@ -577,7 +577,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -668,7 +668,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -726,7 +726,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1054,11 +1054,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1207,11 +1207,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1361,7 +1361,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1477,7 +1477,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1610,7 +1610,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1671,10 +1671,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1828,8 +1828,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
@@ -599,7 +599,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>
+    slice_sizes = array<i64: 1, 1, 1>
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
 }
@@ -690,7 +690,7 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
   func.return %0 : tensor<2x16x30x7xf32>
 }
@@ -748,7 +748,7 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>
+    window_dimensions = array<i64: 1, 2, 2, 1>
   } : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
 }
@@ -1095,11 +1095,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x7x16x!vhlo.f32_v1>
   %0 = "stablehlo.convolution"(%arg0, %arg1) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1248,11 +1248,11 @@
   // CHECK-SAME:   window_strides = #vhlo.tensor_v1<dense<2> : tensor<2xi64>>
   // CHECK-SAME: }> : (!vhlo.tensor_v1<1x8x8x207x!vhlo.f32_v1>, !vhlo.tensor_v1<3x3x207x16x!vhlo.f32_v1>, !vhlo.tensor_v1<4x!vhlo.i32_v1>) -> !vhlo.tensor_v1<1x?x?x16x!vhlo.f32_v1>
   %0 = "stablehlo.dynamic_conv"(%arg0, %arg1, %arg2) {
-    window_strides = dense<2> : tensor<2xi64>,
+    window_strides = array<i64: 2, 2>,
     padding = dense<1> : tensor<2x2xi64>,
-    lhs_dilation = dense<2> : tensor<2xi64>,
-    rhs_dilation = dense<2> : tensor<2xi64>,
-    window_reversal = dense<true> : tensor<2xi1>,
+    lhs_dilation = array<i64: 2, 2>,
+    rhs_dilation = array<i64: 2, 2>,
+    window_reversal = array<i1: true, true>,
     dimension_numbers = #stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>,
     feature_group_count = 1 : i64,
     batch_group_count = 1 : i64,
@@ -1402,7 +1402,7 @@
       start_index_map = [0, 1],
       index_vector_dim = 2
     >,
-    slice_sizes = dense<1> : tensor<3xi64>,
+    slice_sizes = array<i64: 1, 1, 1>,
     indices_are_sorted = true
   } : (tensor<2x4x9xf32>, tensor<1x5x2xi32>) -> tensor<1x5x1xf32>
   func.return %0 : tensor<1x5x1xf32>
@@ -1518,7 +1518,7 @@
       %1 = "stablehlo.abs"(%arg1) : (tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>) -> tensor<16xf32>
   func.return %0 : tensor<16xf32>
 }
@@ -1651,7 +1651,7 @@
       %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    dimensions = dense<0> : tensor<1xi64>
+    dimensions = array<i64: 0>
   } : (tensor<16xf32>, tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
@@ -1681,7 +1681,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
 
   func.return %0: tensor<4xf64>
 }
@@ -1747,10 +1747,10 @@
       %1 = "stablehlo.maximum"(%arg2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 4, 4, 1]> : tensor<4xi64>,
-    base_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_dilations = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 4, 4, 1>,
+    base_dilations = array<i64: 1, 2, 2, 1>,
+    window_dilations = array<i64: 1, 2, 2, 1>,
     padding = dense<[[0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>
   } : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x9x16x7xf32>
   func.return %0 : tensor<2x9x16x7xf32>
@@ -1772,8 +1772,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+         window_dimensions = array<i64: 5, 1>,
+         window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -1953,8 +1953,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f32>, tensor<f32>) -> tensor<f32>
       "stablehlo.return"(%1) : (tensor<f32>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
   func.return %0 : tensor<10x24x24x64xf32>
@@ -1976,8 +1976,8 @@
       %1 = "stablehlo.add"(%arg3, %arg4) : (tensor<f64>, tensor<f64>) -> tensor<f64>
       "stablehlo.return"(%1) : (tensor<f64>) -> ()
   }) {
-    window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-    window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+    window_dimensions = array<i64: 1, 2, 2, 1>,
+    window_strides = array<i64: 1, 2, 2, 1>,
     padding = dense<1> : tensor<4x2xi64>
   } : (tensor<10x24x24x64xf32>, tensor<12x13x13x66xf32>, tensor<f32>) -> tensor<10x24x24x64xf64>
   func.return %0 : tensor<10x24x24x64xf64>
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
@@ -10,7 +10,7 @@
     %1 = "stablehlo.add"(%arg2, %arg3) : (tensor<f64>, tensor<f64>) -> tensor<f64>
     "stablehlo.return"(%1) : (tensor<f64>) -> ()
 
-  }) {dimensions = dense<[0]> : tensor<1xi64>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
+  }) {dimensions = array<i64: 0>} : (tensor<4x4xf32>, tensor<f32>) -> tensor<4xf64>
 
   func.return %0: tensor<4xf64>
 }
@@ -63,8 +63,8 @@
               "stablehlo.return"(%2,%3) : (tensor<f64>, tensor<f32>) -> ()
             })
          { padding = dense<[[2, 2], [0, 0]]> : tensor<2x2xi64>,
-           window_dimensions = dense<[5, 1]> : tensor<2xi64>,
-           window_strides = dense<[3, 1]> : tensor<2xi64> }
+           window_dimensions = array<i64: 5, 1>,
+           window_strides = array<i64: 3, 1> }
          : (tensor<4x2xf32>, tensor<4x2xf32>, tensor<f32>, tensor<f32>) ->
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
@@ -114,8 +114,8 @@
       %2 = stablehlo.add %arg3, %arg4 : tensor<f64>
       "stablehlo.return"(%2) : (tensor<f64>) -> ()
     }) {
-      window_dimensions = dense<[1, 2, 2, 1]> : tensor<4xi64>,
-      window_strides = dense<[1, 2, 2, 1]> : tensor<4xi64>,
+      window_dimensions = array<i64: 1, 2, 2, 1>,
+      window_strides = array<i64: 1, 2, 2, 1>,
       padding = dense<0> : tensor<4x2xi64>
     } : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) ->
           tensor<10x24x24x64xf64>
diff --ruN a/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
@@ -153,7 +153,7 @@
       return rewriter.notifyMatchFailure(op, "expected static slice_sizes");
     rewriter.replaceOpWithNewOp<GatherOp>(
         op, op.getType(), op.getOperand(), op.getStartIndices(),
-        op.getDimensionNumbersAttr(), rewriter.getI64TensorAttr(sliceSizes),
+        op.getDimensionNumbersAttr(), rewriter.getDenseI64ArrayAttr(sliceSizes),
         op.getIndicesAreSortedAttr());
     return success();
   }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp b/stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
--- stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
+++ stablehlo/stablehlo/transforms/StablehloLegalizeToVhlo.cpp
@@ -625,10 +625,11 @@
                 std::is_same<StablehloOpTy, stablehlo::DynamicConvOp>::value) {
     auto numSpatialDimensions = static_cast<int64_t>(
         stablehloOp.getDimensionNumbers().getInputSpatialDimensions().size());
-    if (!stablehloOp.getWindowStridesAttr())
+    if (!stablehloOp.getWindowStridesAttr()) {
       addDefaultAttr("window_strides",
-                     builder.getI64TensorAttr(
+                     builder.getDenseI64ArrayAttr(
                          SmallVector<int64_t>(numSpatialDimensions, 1ll)));
+    }
     if (!stablehloOp.getPaddingAttr())
       addDefaultAttr("padding",
                      DenseIntElementsAttr::get(
@@ -637,11 +638,11 @@
                          SmallVector<int64_t>(numSpatialDimensions * 2, 0ll)));
     if (!stablehloOp.getLhsDilationAttr())
       addDefaultAttr("lhs_dilation",
-                     builder.getI64TensorAttr(
+                     builder.getDenseI64ArrayAttr(
                          SmallVector<int64_t>(numSpatialDimensions, 1ll)));
     if (!stablehloOp.getRhsDilationAttr())
       addDefaultAttr("rhs_dilation",
-                     builder.getI64TensorAttr(
+                     builder.getDenseI64ArrayAttr(
                          SmallVector<int64_t>(numSpatialDimensions, 1ll)));
     if (!stablehloOp.getWindowReversalAttr())
       addDefaultAttr("window_reversal",
diff --ruN a/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/VhloLegalizeToStablehlo.cpp
@@ -426,19 +426,21 @@
   return success();
 }
 
-SpecialResult convertDenseI64Array(
-    StringAttr vhloName, Attribute vhloAttr,
-    SmallVector<NamedAttribute>& stablehloAttrs) {
+template <typename T, typename Attr>
+SpecialResult convertDenseArray(StringAttr vhloName, Attribute vhloAttr,
+                                SmallVector<NamedAttribute>& stablehloAttrs) {
   auto tensorAttr = dyn_cast<vhlo::TensorV1Attr>(vhloAttr);
   if (!tensorAttr) return specialFailure();
 
-  if (tensorAttr.getData().size() % sizeof(int64_t) != 0)
-    return specialFailure();
-
-  auto data = ArrayRef<int64_t>(
-                  reinterpret_cast<const int64_t*>(tensorAttr.getData().data()),
-                  tensorAttr.getData().size() / sizeof(int64_t))
-                  .vec();
+  if (tensorAttr.getData().size() % sizeof(T) != 0) return specialFailure();
+
+  // Extracting the data in this way prevents misaligned memory issues.
+  auto vec =
+      ArrayRef<T>(reinterpret_cast<const T*>(tensorAttr.getData().data()),
+                  tensorAttr.getData().size() / sizeof(T))
+          .vec();
+  SmallVector<T> data;
+  for (T b : vec) data.push_back(b);
 
   // Handle splats
   if (data.size() == 1) {
@@ -449,9 +451,22 @@
     data.resize(size, data[0]);
   }
 
-  stablehloAttrs.emplace_back(
-      vhloName, DenseI64ArrayAttr::get(vhloAttr.getContext(), data));
+  stablehloAttrs.emplace_back(vhloName, Attr::get(vhloAttr.getContext(), data));
   return specialSuccess();
+}
+
+SpecialResult convertDenseI64Array(
+    StringAttr vhloName, Attribute vhloAttr,
+    SmallVector<NamedAttribute>& stablehloAttrs) {
+  return convertDenseArray<int64_t, DenseI64ArrayAttr>(vhloName, vhloAttr,
+                                                       stablehloAttrs);
+}
+
+SpecialResult convertDenseBoolArray(
+    StringAttr vhloName, Attribute vhloAttr,
+    SmallVector<NamedAttribute>& stablehloAttrs) {
+  return convertDenseArray<bool, DenseBoolArrayAttr>(vhloName, vhloAttr,
+                                                     stablehloAttrs);
 }
 
 template <typename VhloOpTy>
@@ -535,6 +550,32 @@
         vhloName == "known_expanding_dimensions" ||
         vhloName == "known_nonexpanding_dimensions")
       return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::SelectAndScatterOpV1>::value) {
+    if (vhloName == "window_dimensions" || vhloName == "window_strides")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ReduceWindowOpV1>::value) {
+    if (vhloName == "window_dimensions" || vhloName == "window_strides" ||
+        vhloName == "base_dilations" || vhloName == "window_dilations")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::MapOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::ReduceOpV1>::value) {
+    if (vhloName == "dimensions")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::GatherOpV1>::value) {
+    if (vhloName == "slice_sizes")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+  }
+  if constexpr (std::is_same<VhloOpTy, vhlo::ConvolutionOpV1>::value ||
+                std::is_same<VhloOpTy, vhlo::DynamicConvOpV1>::value) {
+    if (vhloName == "lhs_dilation" || vhloName == "rhs_dilation" ||
+        vhloName == "window_strides")
+      return convertDenseI64Array(vhloName, vhloAttr, stablehloAttrs);
+    if (vhloName == "window_reversal")
+      return convertDenseBoolArray(vhloName, vhloAttr, stablehloAttrs);
   }
   return notSpecial();
 }

