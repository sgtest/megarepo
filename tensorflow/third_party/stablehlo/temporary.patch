diff --ruN a/stablehlo/CMakeLists.txt b/stablehlo/CMakeLists.txt
--- stablehlo/CMakeLists.txt
+++ stablehlo/CMakeLists.txt
@@ -13,153 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-cmake_minimum_required(VERSION 3.15.0)
 
-if(POLICY CMP0068)
-  cmake_policy(SET CMP0068 NEW)
-  set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)
-endif()
-
-if(POLICY CMP0075)
-  cmake_policy(SET CMP0075 NEW)
-endif()
-
-if(POLICY CMP0077)
-  cmake_policy(SET CMP0077 NEW)
-endif()
-
-# CMP0116: Ninja generators transform `DEPFILE`s from `add_custom_command()`
-# New in CMake 3.20. https://cmake.org/cmake/help/latest/policy/CMP0116.html
-if(POLICY CMP0116)
-  cmake_policy(SET CMP0116 OLD)
-endif()
-
-# Support for return(PROPAGATE ...) in functions.
-if (POLICY CMP0140)
-  cmake_policy(SET CMP0140 NEW)
-endif()
+# This build of StableHLO is meant to be embedded in MLIR-HLO.
+# As a result, its root CMakeLists.txt is different from the original
+# CMakeLists.txt from https://github.com/openxla/stablehlo.
+# All other files of this build of StableHLO except for this one are the same
+# as the original files.
+# To get access to a standalone build of StableHLO, check out the
+# openxla/stablehlo repository.
 
 #-------------------------------------------------------------------------------
 # Options and settings
 #-------------------------------------------------------------------------------
-option(STABLEHLO_BUILD_EMBEDDED "Build StableHLO as part of another project" OFF)
-option(STABLEHLO_ENABLE_BINDINGS_PYTHON "Enables StableHLO Python bindings" OFF)
-option(STABLEHLO_ENABLE_STRICT_BUILD "Build StableHLO with strict warnings and warnings as errors" OFF)
-option(STABLEHLO_ENABLE_SANITIZER "Enable a sanitizer [OFF, address]" OFF)
-option(STABLEHLO_ENABLE_SPLIT_DWARF "Enable split DWARF if the platform supports it" OFF)
-option(STABLEHLO_ENABLE_LLD "Use LLD as the linker if available" OFF)
 
-#-------------------------------------------------------------------------------
-# Project setup and globals
-#-------------------------------------------------------------------------------
-set(STABLEHLO_EXTERNAL_PROJECT_BUILD OFF)
-
-if(NOT (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR) AND NOT MLIR_BINARY_DIR)
-  # Building as part of LLVM via the external project mechanism.
-  set(STABLEHLO_EXTERNAL_PROJECT_BUILD ON)
-else()
-  # Building standalone.
-  project(stablehlo LANGUAGES CXX C)
-  set(CMAKE_C_STANDARD 11)
-  set(CMAKE_CXX_STANDARD 17)
-endif()
-
-#-------------------------------------------------------------------------------
-# MLIR/LLVM Configuration
-#-------------------------------------------------------------------------------
-if (STABLEHLO_ENABLE_STRICT_BUILD)
-  set(LLVM_ENABLE_WARNINGS ON)
-  set(LLVM_ENABLE_WERROR ON)
-  set(LLVM_ENABLE_PEDANTIC ON)
-endif()
-
-# Find MLIR to install if we are building standalone. If building as part of
-# another project, let it handle the MLIR dependency. The dependent project
-# might use a bundled version of MLIR instead of installing, for instance.
-if(STABLEHLO_EXTERNAL_PROJECT_BUILD)
-  message(STATUS "Building StableHLO as an external LLVM project")
-  set(MLIR_MAIN_SRC_DIR ${LLVM_MAIN_SRC_DIR}/../mlir ) # --src-root
-  set(MLIR_INCLUDE_DIR ${MLIR_MAIN_SRC_DIR}/include ) # --includedir
-  set(MLIR_GENERATED_INCLUDE_DIR ${LLVM_BINARY_DIR}/tools/mlir/include)
-  include_directories(SYSTEM ${MLIR_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_GENERATED_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_TABLEGEN_OUTPUT_DIR})
-
-  set(BACKEND_PACKAGE_STRING "${PACKAGE_STRING}")
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_MAIN_SRC_DIR}/cmake/modules")
-elseif(NOT STABLEHLO_BUILD_EMBEDDED)
-  message(STATUS "Building StableHLO with an installed MLIR")
-  find_package(MLIR REQUIRED CONFIG)
-  message(STATUS "Using MLIRConfig.cmake in: ${MLIR_DIR}")
-  message(STATUS "Using LLVMConfig.cmake in: ${LLVM_DIR}")
-  set(LLVM_RUNTIME_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/bin)
-  set(LLVM_LIBRARY_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/lib)
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_CMAKE_DIR}")
-  list(APPEND CMAKE_MODULE_PATH "${LLVM_CMAKE_DIR}")
-else()
-  message(STATUS "Building StableHLO embedded in another project")
-endif()
-
-# Add the CMake modules specific to StableHLO
-list(APPEND CMAKE_MODULE_PATH "${CMAKE_CURRENT_LIST_DIR}/cmake")
-
-if(LLVM_ENABLE_ZLIB)
-  find_package(ZLIB)
-endif()
-
-#-------------------------------------------------------------------------------
-# Performance configuration
-#-------------------------------------------------------------------------------
-
-include(CheckCXXCompilerFlag)
-include(CheckLinkerFlag)
-if (STABLEHLO_ENABLE_LLD)
-  message(STATUS "Enabling LLD as the linker")
-  add_link_options("-fuse-ld=lld")
-endif()
-
-if(STABLEHLO_ENABLE_SPLIT_DWARF)
-    check_cxx_compiler_flag(-gsplit-dwarf STABLEHLO_SUPPORTS_SPLIT_DWARF)
-    if (STABLEHLO_SUPPORTS_SPLIT_DWARF)
-      message(STATUS "Enabling split-dwarf build")
-      add_compile_options(-gsplit-dwarf -ggnu-pubnames)
-    endif()
-    check_linker_flag(CXX "-Wl,--gdb-index" STABLEHLO_SUPPORTS_GDB_INDEX)
-    # If we set LLD it doesn't seem to affect the check_linker_flag above.
-    # Account for it with the generator expression OR
-    if (STABLEHLO_SUPPORTS_GDB_INDEX OR STABLEHLO_ENABLE_LLD)
-      message(STATUS "Enabling GDB index in binary")
-      add_link_options("-Wl,--gdb-index")
-    endif()
-endif()
-
-include(TableGen)
-include(AddLLVM)
-include(AddMLIR)
-include(HandleLLVMOptions)
-include_directories(${LLVM_INCLUDE_DIRS})
-include_directories(${MLIR_INCLUDE_DIRS})
-include_directories(${CMAKE_CURRENT_SOURCE_DIR})
-include_directories(${CMAKE_CURRENT_BINARY_DIR})
-link_directories(${LLVM_BUILD_LIBRARY_DIR})
-add_definitions(${LLVM_DEFINITIONS})
-
-
-#-------------------------------------------------------------------------------
-# Sanitizer configuration
-#-------------------------------------------------------------------------------
-
-include(SetupSanitizers)
-setup_sanitizers()
-
-#-------------------------------------------------------------------------------
-# Python configuration
-#-------------------------------------------------------------------------------
-
-if(STABLEHLO_ENABLE_BINDINGS_PYTHON)
-  include(MLIRDetectPythonEnv)
-  mlir_configure_python_dev_packages()
-endif()
+set(STABLEHLO_ENABLE_BINDINGS_PYTHON ${MHLO_ENABLE_BINDINGS_PYTHON})
 
 #-------------------------------------------------------------------------------
 # Directory setup
diff --ruN a/stablehlo/stablehlo/CMakeLists.txt b/stablehlo/stablehlo/CMakeLists.txt
--- stablehlo/stablehlo/CMakeLists.txt
+++ stablehlo/stablehlo/CMakeLists.txt
@@ -15,6 +15,7 @@
 add_subdirectory(api)
 add_subdirectory(conversions)
 add_subdirectory(dialect)
+add_subdirectory(experimental)
 add_subdirectory(integrations)
 add_subdirectory(reference)
 add_subdirectory(tests)
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
@@ -826,8 +826,10 @@
     Value emptyTensor =
         getEmptyTensorFor(rewriter, loc, resultTy, op, adaptor.getOperands());
 
+    // TODO(#2216) Cleanup Attribute -> DenseArrayAttr
     rewriter.replaceOpWithNewOp<linalg::TransposeOp>(
-        op, adaptor.getOperand(), emptyTensor, op.getPermutationAttr(),
+        op, adaptor.getOperand(), emptyTensor,
+        op.getPermutationAttr().dyn_cast_or_null<DenseI64ArrayAttr>(),
         linalg::getPrunedAttributeList(op));
     return success();
   }
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
@@ -16,7 +16,9 @@
 
 // Implements logic for lowering StableHLO convolution ops to Linalg dialect.
 
+#include <cstdint>
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
+#include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinTypes.h"
 #include "mlir/Support/LogicalResult.h"
 #include "mlir/Transforms/DialectConversion.h"
@@ -30,12 +32,12 @@
 /// Apply dilation and padding to the input of a convolution.
 Value applyConvolutionPadding(Location loc, Value input,
                               DenseIntElementsAttr padding,
-                              DenseI64ArrayAttr lhsDilation,
+                              std::optional<ArrayRef<int64_t>> lhsDilation,
                               llvm::ArrayRef<int64_t> dimMappings,
                               OpBuilder &rewriter) {
   SmallVector<int64_t> lhsDilationValues;
-  if (lhsDilation)
-    lhsDilationValues = llvm::to_vector(lhsDilation.asArrayRef());
+  if (lhsDilation.has_value())
+    lhsDilationValues = llvm::to_vector(lhsDilation.value());
   bool noPadding = !padding || isSplatValue(padding, 0);
   bool noDilation = !lhsDilation || hlo::isSplatArray(lhsDilationValues, 1);
   if (noPadding && noDilation) return input;
@@ -230,7 +232,7 @@
     llvm::SmallVector<int64_t> spatialDimMapping(rank - 2);
     std::iota(spatialDimMapping.begin(), spatialDimMapping.end(), 1);
     input = applyConvolutionPadding(loc, input, op.getPaddingAttr(),
-                                    op.getLhsDilationAttr(), spatialDimMapping,
+                                    op.getLhsDilation(), spatialDimMapping,
                                     rewriter);
 
     switch (rank) {
@@ -350,10 +352,10 @@
     // Decompose the convolution into an initial padding
     Value modifiedLhs = applyConvolutionPadding(
         op.getLoc(), adaptor.getLhs(), adaptor.getPaddingAttr(),
-        adaptor.getLhsDilationAttr(),
+        adaptor.getLhsDilation(),
         op.getDimensionNumbers().getInputSpatialDimensions(), rewriter);
     Value modifiedRhs = applyConvolutionPadding(
-        op.getLoc(), adaptor.getRhs(), nullptr, adaptor.getRhsDilationAttr(),
+        op.getLoc(), adaptor.getRhs(), nullptr, adaptor.getRhsDilation(),
         op.getDimensionNumbers().getKernelSpatialDimensions(), rewriter);
     modifiedRhs = applyConvolutionReversal(loc, rewriter, op, modifiedRhs);
 
@@ -643,7 +645,7 @@
     llvm::SmallVector<int64_t> spatialDimMapping(spatialRank);
     std::iota(spatialDimMapping.begin(), spatialDimMapping.end(), 1);
     input = applyConvolutionPadding(loc, input, op.getPaddingAttr(),
-                                    op.getLhsDilationAttr(), spatialDimMapping,
+                                    op.getLhsDilation(), spatialDimMapping,
                                     rewriter);
 
     auto filterDims = llvm::to_vector(op.getRhs().getType().getShape());
diff --ruN a/stablehlo/stablehlo/dialect/AssemblyFormat.cpp b/stablehlo/stablehlo/dialect/AssemblyFormat.cpp
--- stablehlo/stablehlo/dialect/AssemblyFormat.cpp
+++ stablehlo/stablehlo/dialect/AssemblyFormat.cpp
@@ -150,7 +150,16 @@
   // Parse the generic form.
   if (succeeded(parser.parseOptionalLParen())) {
     if (parser.parseRParen()) return failure();
+    // Parse optional properties
+    if (succeeded(parser.parseOptionalLess()) &&
+        (failed(parser.parseAttribute(result.propertiesAttr)) ||
+         failed(parser.parseGreater())))
+      return failure();
+
+    // Parse optional attributes
     if (parser.parseOptionalAttrDict(result.attributes)) return failure();
+
+    // Parse type signature
     if (parser.parseColon() || parser.parseLParen() || parser.parseRParen() ||
         parser.parseArrow())
       return failure();
@@ -685,39 +694,41 @@
 //===----------------------------------------------------------------------===//
 
 void printSliceRanges(OpAsmPrinter& p, Operation* op,
-                      ArrayRef<int64_t> startIndices,
-                      ArrayRef<int64_t> limitIndices,
-                      ArrayRef<int64_t> strides) {
+                      Attribute startIndicesAttr, Attribute limitIndicesAttr,
+                      Attribute stridesAttr) {
+  auto startIndices = cast<DenseI64ArrayAttr>(startIndicesAttr);
+  auto limitIndices = cast<DenseI64ArrayAttr>(limitIndicesAttr);
+  auto strides = cast<DenseI64ArrayAttr>(stridesAttr);
   p << "[";
   // Let's be safe if we're printing invalid IR somehow: this can't be parsed
   // back!
   if (startIndices.size() != limitIndices.size() ||
       startIndices.size() != strides.size()) {
     p << "start_indices: ";
-    llvm::interleaveComma(startIndices, p);
+    llvm::interleaveComma(startIndices.asArrayRef(), p);
     p << ", limit_indices: ";
-    llvm::interleaveComma(limitIndices, p);
+    llvm::interleaveComma(limitIndices.asArrayRef(), p);
     p << ", strides: ";
-    llvm::interleaveComma(strides, p);
+    llvm::interleaveComma(strides.asArrayRef(), p);
     p << "]";
     return;
   }
 
-  llvm::interleaveComma(llvm::zip(startIndices, limitIndices, strides), p,
-                        [&](std::tuple<int64_t, int64_t, int64_t> pack) {
-                          auto [start, limit, stride] = pack;
-                          p << start << ":" << limit;
-                          if (stride != 1) {
-                            p << ":" << stride;
-                          }
-                        });
+  llvm::interleaveComma(
+      llvm::zip(startIndices.asArrayRef(), limitIndices.asArrayRef(),
+                strides.asArrayRef()),
+      p, [&](std::tuple<int64_t, int64_t, int64_t> pack) {
+        auto [start, limit, stride] = pack;
+        p << start << ":" << limit;
+        if (stride != 1) {
+          p << ":" << stride;
+        }
+      });
   p << "]";
 }
 
-ParseResult parseSliceRanges(OpAsmParser& parser,
-                             DenseI64ArrayAttr& startIndices,
-                             DenseI64ArrayAttr& limitIndices,
-                             DenseI64ArrayAttr& strides) {
+ParseResult parseSliceRanges(OpAsmParser& parser, Attribute& startIndices,
+                             Attribute& limitIndices, Attribute& strides) {
   if (parser.parseLSquare()) return failure();
   // Parse groups of comma-separated: `start`:`limit`[:`stride`]
   // If the stride isn't provided it'll be 1.
@@ -747,6 +758,17 @@
   return success();
 }
 
+void printDenseI64Array(OpAsmPrinter& p, Operation* op, Attribute attr) {
+  cast<DenseI64ArrayAttr>(attr).print(p);
+}
+
+ParseResult parseDenseI64Array(OpAsmParser& parser, Attribute& attr) {
+  if ((attr = DenseI64ArrayAttr::parse(parser, Type{}))) {
+    return success();
+  }
+  return failure();
+}
+
 ParseResult dimSizeFromString(AsmParser& parser, int64_t& result) {
   if (succeeded(parser.parseOptionalQuestion())) {
     result = ShapedType::kDynamic;
diff --ruN a/stablehlo/stablehlo/dialect/AssemblyFormat.h b/stablehlo/stablehlo/dialect/AssemblyFormat.h
--- stablehlo/stablehlo/dialect/AssemblyFormat.h
+++ stablehlo/stablehlo/dialect/AssemblyFormat.h
@@ -219,16 +219,25 @@
 // Attribute Printers and Parsers
 //===----------------------------------------------------------------------===//
 
+// TODO(#2216) Cleanup Attribute -> DenseArrayAttr for print/parse.
 // SliceRanges - Used to print multi-dimensional ranges for slice.
 void printSliceRanges(OpAsmPrinter& p, Operation* op,
-                      ArrayRef<int64_t> startIndices,
-                      ArrayRef<int64_t> limitIndices,
-                      ArrayRef<int64_t> strides);
-
-ParseResult parseSliceRanges(OpAsmParser& parser,
-                             DenseI64ArrayAttr& startIndices,
-                             DenseI64ArrayAttr& limitIndices,
-                             DenseI64ArrayAttr& strides);
+                      Attribute startIndices,
+                      Attribute limitIndices,
+                      Attribute strides);
+
+ParseResult parseSliceRanges(OpAsmParser& parser, Attribute& startIndices,
+                             Attribute& limitIndices, Attribute& strides);
+
+// GenericI64DenseArray - Used to print an attr that can be either
+//
+//   Dense elements:
+//     { dense<[1, 2]> : tensor<2xi64> }
+//   Array:
+//     { array<i64: 1, 2> }
+void printDenseI64Array(OpAsmPrinter& p, Operation* op, Attribute attr);
+
+ParseResult parseDenseI64Array(OpAsmParser& parser, Attribute& attr);
 
 // DimSizes - Print an array of ints. Dynamic dimensions printed as `?`.
 //
diff --ruN a/stablehlo/stablehlo/dialect/StablehloAttrs.td b/stablehlo/stablehlo/dialect/StablehloAttrs.td
--- stablehlo/stablehlo/dialect/StablehloAttrs.td
+++ stablehlo/stablehlo/dialect/StablehloAttrs.td
@@ -24,6 +24,24 @@
   let parser = "parseDimSizes($_parser)";
   let printer = "printDimSizes($_printer, $_self)";
 }
+
+def GenericDenseI64ArrayAttr : Attr<DenseI64ArrayAttr.predicate, "DenseI64ArrayAttr with generic Attribute storage"> {
+  let storageType = "Attribute";
+  let valueType = DenseI64ArrayAttr.valueType;
+  let returnType = "::llvm::ArrayRef<int64_t>";
+  let baseAttr = DenseI64ArrayAttr;
+  let convertFromStorage = "$_self.cast<DenseI64ArrayAttr>().asArrayRef()";
+  let constBuilderCall = "$_builder.getDenseI64ArrayAttr($0)";
+}
+
+def GenericDenseBoolArrayAttr : Attr<DenseBoolArrayAttr.predicate, "DenseBoolArrayAttr with generic Attribute storage"> {
+  let storageType = "Attribute";
+  let valueType = DenseBoolArrayAttr.valueType;
+  let returnType = "::llvm::ArrayRef<bool>";
+  let convertFromStorage = "$_self.cast<DenseBoolArrayAttr>().asArrayRef()";
+  let constBuilderCall = "$_builder.getDenseBoolArrayAttr($0)";
+}
+
 
 def StableHLO_ScatterDimensionNumbers : AttrDef<StableHLO_Dialect, "ScatterDimensionNumbers"> {
   let mnemonic = "scatter";
@@ -182,15 +200,15 @@
 def StableHLO_ConvolutionAttributes {
   dag attributes = (ins
     // Default value: one for each of the spatial dimension.
-    OptionalAttr<DenseI64ArrayAttr>:$window_strides,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$window_strides,
     // Default value: two zeros for each of the spatial dimension.
     OptionalAttr<I64ElementsAttr>:$padding,
     // Default value: one for each of the spatial dimension.
-    OptionalAttr<DenseI64ArrayAttr>:$lhs_dilation,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$lhs_dilation,
     // Default value: one for each of the spatial dimension.
-    OptionalAttr<DenseI64ArrayAttr>:$rhs_dilation,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$rhs_dilation,
     // Default value: false for each of the spatial dimension.
-    OptionalAttr<DenseBoolArrayAttr>:$window_reversal,
+    OptionalAttr<GenericDenseBoolArrayAttr>:$window_reversal,
     StableHLO_ConvDimensionNumbers:$dimension_numbers,
     I64Attr:$feature_group_count,
     I64Attr:$batch_group_count,
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -2346,6 +2346,7 @@
 
 using mlir::hlo::parseComplexOpType;
 using mlir::hlo::parseCustomCallTarget;
+using mlir::hlo::parseDenseI64Array;
 using mlir::hlo::parseDotDimensionNumbers;
 using mlir::hlo::parseExponentMantissa;
 using mlir::hlo::parsePairwiseOpType;
@@ -2357,6 +2358,7 @@
 using mlir::hlo::parseVariadicSameOperandsAndResultType;
 using mlir::hlo::printComplexOpType;
 using mlir::hlo::printCustomCallTarget;
+using mlir::hlo::printDenseI64Array;
 using mlir::hlo::printDotDimensionNumbers;
 using mlir::hlo::printExponentMantissa;
 using mlir::hlo::printPairwiseOpType;
@@ -3028,11 +3030,11 @@
 }  // namespace
 
 void printWindowAttributes(OpAsmPrinter& p, Operation* /*op*/,
-                           std::optional<DenseI64ArrayAttr> windowStrides,
+                           std::optional<Attribute> windowStrides,
                            std::optional<DenseIntElementsAttr> padding,
-                           std::optional<DenseI64ArrayAttr> lhsDilation,
-                           std::optional<DenseI64ArrayAttr> rhsDilation,
-                           std::optional<DenseBoolArrayAttr> windowReversal) {
+                           std::optional<Attribute> lhsDilation,
+                           std::optional<Attribute> rhsDilation,
+                           std::optional<Attribute> windowReversal) {
   using pair_t = std::pair<Attribute, StringRef>;
   std::array<pair_t, 5> printedAttributes = {{
       {windowStrides ? *windowStrides : nullptr, "stride"},
@@ -3065,11 +3067,11 @@
 }
 
 ParseResult parseWindowAttributes(OpAsmParser& parser,
-                                  DenseI64ArrayAttr& windowStrides,
+                                  Attribute& windowStrides,
                                   DenseIntElementsAttr& padding,
-                                  DenseI64ArrayAttr& lhsDilation,
-                                  DenseI64ArrayAttr& rhsDilation,
-                                  DenseBoolArrayAttr& windowReversal) {
+                                  Attribute& lhsDilation,
+                                  Attribute& rhsDilation,
+                                  Attribute& windowReversal) {
   StringRef attributeName;
 
   llvm::StringSet<> allowedAttributeNames{
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h
--- stablehlo/stablehlo/dialect/StablehloOps.h
+++ stablehlo/stablehlo/dialect/StablehloOps.h
@@ -120,20 +120,21 @@
 ParseResult parseConvolutionDimensions(AsmParser &parser,
                                        ConvDimensionNumbersAttr &dimNums);
 
+// TODO(#2216) Cleanup Attribute -> DenseArrayAttr for print/parse.
 // Custom formatting for convolution window attributes.
 void printWindowAttributes(OpAsmPrinter &p, Operation *op,
-                           std::optional<DenseI64ArrayAttr> windowStrides,
+                           std::optional<Attribute> windowStrides,
                            std::optional<DenseIntElementsAttr> padding,
-                           std::optional<DenseI64ArrayAttr> lhsDilation,
-                           std::optional<DenseI64ArrayAttr> rhsDilation,
-                           std::optional<DenseBoolArrayAttr> windowReversal);
+                           std::optional<Attribute> lhsDilation,
+                           std::optional<Attribute> rhsDilation,
+                           std::optional<Attribute> windowReversal);
 
 ParseResult parseWindowAttributes(OpAsmParser &parser,
-                                  DenseI64ArrayAttr &windowStrides,
+                                  Attribute &windowStrides,
                                   DenseIntElementsAttr &padding,
-                                  DenseI64ArrayAttr &lhsDilation,
-                                  DenseI64ArrayAttr &rhsDilation,
-                                  DenseBoolArrayAttr &windowReversal);
+                                  Attribute &lhsDilation,
+                                  Attribute &rhsDilation,
+                                  Attribute &windowReversal);
 
 }  // end namespace stablehlo
 }  // end namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -38,7 +38,6 @@
 
   let useDefaultAttributePrinterParser = 0;
   let useDefaultTypePrinterParser = 0;
-  let usePropertiesForAttributes = 0;
 }
 
 class StableHLO_Op<string mnemonic, list<Trait> traits = []> :
@@ -1515,7 +1514,7 @@
   let arguments = (ins
     Variadic<HLO_Tensor>:$inputs, /*reduce_i1*/
     Variadic<HLO_Tensor>:$init_values, /*reduce_i2*/
-    DenseI64ArrayAttr:$dimensions /*reduce_i3*/
+    GenericDenseI64ArrayAttr:$dimensions /*reduce_i3*/
   );
   let regions = (region SizedRegion<1>:$body /*reduce_i4*/);
 
@@ -1665,9 +1664,9 @@
 
   let arguments = (ins
     HLO_Tensor:$operand,
-    DenseI64ArrayAttr:$start_indices,
-    DenseI64ArrayAttr:$limit_indices,
-    DenseI64ArrayAttr:$strides
+    GenericDenseI64ArrayAttr:$start_indices,
+    GenericDenseI64ArrayAttr:$limit_indices,
+    GenericDenseI64ArrayAttr:$strides
   );
 
   let assemblyFormat = [{
@@ -1698,14 +1697,14 @@
   let arguments = (ins
     HLO_Tensor:$operand /*dynamic_slice_i1*/,
     Variadic<HLO_ScalarIntTensor>:$start_indices /*dynamic_slice_i2*/,
-    DenseI64ArrayAttr:$slice_sizes /*dynamic_slice_i3*/
+    GenericDenseI64ArrayAttr:$slice_sizes /*dynamic_slice_i3*/
   );
 
   let results = (outs HLO_Tensor:$result);
 
   let assemblyFormat = [{
     $operand `,` custom<VariadicOperandWithAttribute>($start_indices)
-      `sizes` `=` $slice_sizes
+      `sizes` `=` custom<DenseI64Array>($slice_sizes)
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -1900,13 +1899,13 @@
   }];
   let arguments = (ins
     HLO_Tensor:$operand,
-    DenseI64ArrayAttr:$broadcast_sizes
+    GenericDenseI64ArrayAttr:$broadcast_sizes
   );
 
   let results = (outs HLO_TensorOrPerAxisQuantizedTensor);
 
   let assemblyFormat = [{
-    $operand `,` `sizes` `=` $broadcast_sizes
+    $operand `,` `sizes` `=` custom<DenseI64Array>($broadcast_sizes)
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -1928,7 +1927,7 @@
   }];
   let arguments = (ins
     HLO_TensorOrPerAxisQuantizedTensor:$operand /*broadcast_in_dim_i1*/,
-    DenseI64ArrayAttr:$broadcast_dimensions /*broadcast_in_dim_i2*/
+    GenericDenseI64ArrayAttr:$broadcast_dimensions /*broadcast_in_dim_i2*/
   );
 
   let results = (outs HLO_StaticShapeTensorOrPerAxisQuantizedTensor);
@@ -1936,7 +1935,7 @@
   let hasVerifier = 1;
 
   let assemblyFormat = [{
-    $operand `,` `dims` `=` $broadcast_dimensions
+    $operand `,` `dims` `=` custom<DenseI64Array>($broadcast_dimensions)
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -1961,9 +1960,9 @@
   let arguments = (ins
     HLO_TensorOrPerAxisQuantizedTensor:$operand,
     HLO_StaticDimensionTensor:$output_dimensions,
-    DenseI64ArrayAttr:$broadcast_dimensions,
-    OptionalAttr<DenseI64ArrayAttr>:$known_expanding_dimensions,
-    OptionalAttr<DenseI64ArrayAttr>:$known_nonexpanding_dimensions
+    GenericDenseI64ArrayAttr:$broadcast_dimensions,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$known_expanding_dimensions,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$known_nonexpanding_dimensions
   );
 
   let results = (outs HLO_TensorOrPerAxisQuantizedTensor);
@@ -1981,7 +1980,7 @@
   let hasVerifier = 1;
 
   let assemblyFormat = [{
-    $operand `,` $output_dimensions `,` `dims` `=` $broadcast_dimensions
+    $operand `,` $output_dimensions `,` `dims` `=` custom<DenseI64Array>($broadcast_dimensions)
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -2239,8 +2238,8 @@
 
   let extraClassDeclaration = [{
     bool hasWindowReversal() {
-      auto reversal = getWindowReversalAttr();
-      return reversal && llvm::any_of(reversal.asArrayRef(), [](bool v) { return v; });
+      auto reversal = getWindowReversal();
+      return reversal.has_value() && llvm::any_of(reversal.value(), [](bool v) { return v; });
     }
   }];
 
@@ -2472,13 +2471,13 @@
   let arguments = (ins
     HLO_FpOrComplexTensor:$operand,
     StableHLO_FftTypeAttr:$fft_type,
-    DenseI64ArrayAttr:$fft_length
+    GenericDenseI64ArrayAttr:$fft_length
   );
 
   let results = (outs HLO_FpOrComplexTensor);
 
   let assemblyFormat = [{
-    $operand `,` `type` `=` $fft_type `,` `length` `=` $fft_length
+    $operand `,` `type` `=` $fft_type `,` `length` `=` custom<DenseI64Array>($fft_length)
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -2513,7 +2512,7 @@
     HLO_Tensor:$operand /*gather_i1*/,
     HLO_IntTensor:$start_indices /*gather_i2*/,
     StableHLO_GatherDimensionNumbers:$dimension_numbers /*gather_i3, gather_i4, gather_i5, gather_i6*/,
-    DenseI64ArrayAttr:$slice_sizes /*gather_i7*/,
+    GenericDenseI64ArrayAttr:$slice_sizes /*gather_i7*/,
     DefaultValuedOptionalAttr<BoolAttr, "false">:$indices_are_sorted /*gather_i8*/
   );
 
@@ -2578,7 +2577,7 @@
   }];
   let arguments = (ins
     Variadic<HLO_Tensor>:$inputs /*map_i1*/,
-    DenseI64ArrayAttr:$dimensions /*map_i2*/
+    GenericDenseI64ArrayAttr:$dimensions /*map_i2*/
   );
   let regions = (region SizedRegion<1>:$computation /*map_i3*/);
   let results = (outs HLO_Tensor);
@@ -2758,8 +2757,8 @@
     HLO_Tensor:$operand, /*select_and_scatter_i1*/
     HLO_Tensor:$source, /*select_and_scatter_i2*/
     HLO_Tensor:$init_value, /*select_and_scatter_i3*/
-    OptionalAttr<DenseI64ArrayAttr>:$window_dimensions, /*select_and_scatter_i4*/
-    OptionalAttr<DenseI64ArrayAttr>:$window_strides, /*select_and_scatter_i5*/
+    OptionalAttr<GenericDenseI64ArrayAttr>:$window_dimensions, /*select_and_scatter_i4*/
+    OptionalAttr<GenericDenseI64ArrayAttr>:$window_strides, /*select_and_scatter_i5*/
     OptionalAttr<I64ElementsAttr>:$padding /*select_and_scatter_i6*/
   );
 
@@ -2861,7 +2860,7 @@
   }];
   let arguments = (ins
     HLO_Tensor:$operand,
-    DenseI64ArrayAttr:$dimensions
+    GenericDenseI64ArrayAttr:$dimensions
   );
 
   let hasVerifier = 1;
@@ -2869,7 +2868,7 @@
   let results = (outs HLO_Tensor:$result);
 
   let assemblyFormat = [{
-    $operand `,` `dims` `=` $dimensions
+    $operand `,` `dims` `=` custom<DenseI64Array>($dimensions)
       attr-dict `:` custom<SameOperandsAndResultType>(type($operand), type($result))
   }];
 }
@@ -2897,18 +2896,18 @@
   let arguments = (ins
     HLO_Tensor:$operand /*pad_i1*/,
     HLO_Tensor:$padding_value /*pad_i2*/,
-    DenseI64ArrayAttr:$edge_padding_low /*pad_i3*/,
-    DenseI64ArrayAttr:$edge_padding_high /*pad_i4*/,
-    DenseI64ArrayAttr:$interior_padding /*pad_i5*/
+    GenericDenseI64ArrayAttr:$edge_padding_low /*pad_i3*/,
+    GenericDenseI64ArrayAttr:$edge_padding_high /*pad_i4*/,
+    GenericDenseI64ArrayAttr:$interior_padding /*pad_i5*/
   );
 
   let results = (outs HLO_Tensor);
 
   let assemblyFormat = [{
     $operand `,` $padding_value `,`
-      `low` `=` $edge_padding_low `,`
-      `high` `=` $edge_padding_high `,`
-      `interior` `=` $interior_padding
+      `low` `=` custom<DenseI64Array>($edge_padding_low) `,`
+      `high` `=` custom<DenseI64Array>($edge_padding_high) `,`
+      `interior` `=` custom<DenseI64Array>($interior_padding)
       attr-dict `:` functional-type(operands, results)
   }];
 }
@@ -2954,14 +2953,14 @@
   }];
   let arguments = (ins
     HLO_TensorOrPerAxisQuantizedTensor:$operand,
-    DenseI64ArrayAttr:$permutation
+    GenericDenseI64ArrayAttr:$permutation
   );
   let results = (outs HLO_TensorOrPerAxisQuantizedTensor:$result);
 
   let hasVerifier = 1;
 
   let assemblyFormat = [{
-    $operand `,` `dims` `=` $permutation
+    $operand `,` `dims` `=` custom<DenseI64Array>($permutation)
       attr-dict `:` functional-type(operands, results)
   }];
 
@@ -3041,13 +3040,13 @@
   let arguments = (ins
     Variadic<HLO_Tensor>:$inputs /*reduce_window_i1*/,
     Variadic<HLO_Tensor>:$init_values /*reduce_window_i2*/,
-    DenseI64ArrayAttr:$window_dimensions /*reduce_window_i3*/,
+    GenericDenseI64ArrayAttr:$window_dimensions /*reduce_window_i3*/,
     // If strides or dilations attributes are missing then the default value is
     // one for each of the operand dimensions. Similarly, padding values are zero
     // for both low and high in each of the dimensions, if not specified.
-    OptionalAttr<DenseI64ArrayAttr>:$window_strides /*reduce_window_i4*/,
-    OptionalAttr<DenseI64ArrayAttr>:$base_dilations /*reduce_window_i5*/,
-    OptionalAttr<DenseI64ArrayAttr>:$window_dilations /*reduce_window_i6*/,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$window_strides /*reduce_window_i4*/,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$base_dilations /*reduce_window_i5*/,
+    OptionalAttr<GenericDenseI64ArrayAttr>:$window_dilations /*reduce_window_i6*/,
     OptionalAttr<I64ElementsAttr>:$padding /*reduce_window_i7*/
   );
 
diff --ruN a/stablehlo/stablehlo/experimental/BUILD.bazel b/stablehlo/stablehlo/experimental/BUILD.bazel
--- stablehlo/stablehlo/experimental/BUILD.bazel
+++ stablehlo/stablehlo/experimental/BUILD.bazel
@@ -0,0 +1,115 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+cc_library(
+    name = "experimental_base",
+    srcs = [
+        "dialect/Base.cpp",
+    ],
+    hdrs = [
+        "dialect/Base.h",
+    ],
+    deps = [
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+    ],
+)
+
+cc_library(
+    name = "experimental_stablehlo_ops",
+    srcs = [
+        "dialect/StablehloOps.cpp",
+    ],
+    hdrs = [
+        "dialect/StablehloOps.h",
+    ],
+    deps = [
+        ":experimental_base",
+        "//:stablehlo_ops",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+gentbl_cc_library(
+    name = "experimental_stablehlo_pass_inc_gen",
+    tbl_outs = [
+        (
+            [
+                "-gen-pass-decls",
+            ],
+            "transforms/Passes.h.inc",
+        ),
+    ],
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "transforms/Passes.td",
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+cc_library(
+    name = "experimental_stablehlo_passes",
+    srcs = [
+        "transforms/ChloRecomposeOps.cpp",
+        "transforms/StablehloCanonicalizeDynamism.cpp",
+        "transforms/StablehloRefineShapes.cpp",
+    ],
+    hdrs = [
+        "transforms/Passes.h",
+    ],
+    deps = [
+        ":experimental_stablehlo_ops",
+        ":experimental_stablehlo_pass_inc_gen",
+        "//:base",
+        "//:chlo_ops",
+        "//:stablehlo_ops",
+        "//:stablehlo_ops_inc_gen",
+        "//:stablehlo_passes",
+        "//:stablehlo_type_inference",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:InferTypeOpInterface",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+    ],
+)
+
+cc_binary(
+    name = "experimental-stablehlo-opt",
+    srcs = [
+        "tools/StablehloOptMain.cpp",
+    ],
+    deps = [
+        ":experimental_stablehlo_passes",
+        "//:interpreter_ops",
+        "//:register",
+        "//:stablehlo_passes",
+        "//:test_utils",
+        "//:tosa_passes",
+        "@llvm-project//mlir:AllExtensions",
+        "@llvm-project//mlir:AllPassesAndDialects",
+        "@llvm-project//mlir:MlirOptLib",
+        "@llvm-project//mlir:TosaDialect",
+    ],
+)
diff --ruN a/stablehlo/stablehlo/experimental/CMakeLists.txt b/stablehlo/stablehlo/experimental/CMakeLists.txt
--- stablehlo/stablehlo/experimental/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/CMakeLists.txt
@@ -0,0 +1,18 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_subdirectory(dialect)
+add_subdirectory(tests)
+add_subdirectory(tools)
+add_subdirectory(transforms)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.cpp b/stablehlo/stablehlo/experimental/dialect/Base.cpp
--- stablehlo/stablehlo/experimental/dialect/Base.cpp
+++ stablehlo/stablehlo/experimental/dialect/Base.cpp
@@ -0,0 +1,39 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/Base.h"
+
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext* context,
+                                    ArrayRef<int64_t> values) {
+  return DenseIntElementsAttr::get(
+      RankedTensorType::get({static_cast<int64_t>(values.size()) / 2, 2},
+                            IntegerType::get(context, 64)),
+      values);
+}
+
+DenseIntElementsAttr getPaddingAttr(Builder* builder,
+                                    ArrayRef<int64_t> values) {
+  return getPaddingAttr(builder->getContext(), values);
+}
+
+}  // namespace hlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.h b/stablehlo/stablehlo/experimental/dialect/Base.h
--- stablehlo/stablehlo/experimental/dialect/Base.h
+++ stablehlo/stablehlo/experimental/dialect/Base.h
@@ -0,0 +1,35 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+
+#include "llvm/ADT/ArrayRef.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext *context,
+                                    ArrayRef<int64_t> value);
+DenseIntElementsAttr getPaddingAttr(Builder *builder, ArrayRef<int64_t> value);
+
+}  // namespace hlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
diff --ruN a/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt b/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
--- stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
@@ -0,0 +1,42 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_mlir_library(ExperimentalStablehloBase
+  PARTIAL_SOURCES_INTENDED
+  Base.cpp
+
+  LINK_LIBS PUBLIC
+  MLIRIR
+)
+
+add_mlir_dialect_library(ExperimentalStablehloOps
+  PARTIAL_SOURCES_INTENDED
+  StablehloOps.cpp
+
+  DEPENDS
+  StablehloOpsIncGen
+
+  LINK_LIBS PUBLIC
+  ExperimentalStablehloBase
+  MLIRFuncDialect
+  MLIRIR
+  MLIRSupport
+  StablehloOps
+)
+
+target_include_directories(ExperimentalStablehloOps INTERFACE
+  $<BUILD_INTERFACE:${STABLEHLO_SOURCE_DIR}>
+  $<BUILD_INTERFACE:${STABLEHLO_BINARY_DIR}>
+)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp b/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
@@ -0,0 +1,506 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+
+#include <cstdint>
+#include <optional>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/Types.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+LogicalResult DynamicReduceWindowOpAdaptor::verify() {
+  // Before checking the constraints inherited from ReduceWindowOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2 * op_->getNumResults() + 5)
+    return op_.emitError("expects size(operands) = 2 * size(results) + 5");
+  if (op_->getNumResults() == 0)
+    return op_.emitError("expects size(results) > 0");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_reduce_window".
+    // called_computations carries the body.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "called_computations")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_reduce_window")
+    return op_.emitError() << "expects @stablehlo.dynamic_reduce_window";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto numInputs = getInputs().size();
+  auto inputs = op_.getInputs().slice(0, numInputs);
+  auto initValues = op_.getInputs().slice(numInputs, numInputs);
+  auto windowDimensions = op_.getInputs()[op_.getInputs().size() - 5];
+  auto windowStrides = op_.getInputs()[op_.getInputs().size() - 4];
+  auto baseDilations = op_.getInputs()[op_.getInputs().size() - 3];
+  auto windowDilations = op_.getInputs()[op_.getInputs().size() - 2];
+  auto padding = op_.getInputs()[op_.getInputs().size() - 1];
+  auto results = op_.getResults();
+
+  // reduce_window_c1
+  // This constraint hold automatically thanks to the checks that we have
+  // performed above.
+
+  // reduce_window_i1
+  SmallVector<ShapedType> inputTypes;
+  for (auto [index, input] : llvm::enumerate(inputs)) {
+    auto inputType = input.getType().dyn_cast<ShapedType>();
+    inputTypes.push_back(inputType);
+    if (!inputType)
+      return op_.emitError()
+             << "expects inputs (e.g. operand #" << index << ") to be tensors";
+  }
+
+  // reduce_window_i2
+  SmallVector<ShapedType> initValueTypes;
+  for (auto [index, initValue] : llvm::enumerate(initValues)) {
+    auto initValueType = initValue.getType().dyn_cast<ShapedType>();
+    initValueTypes.push_back(initValueType);
+    if (!initValueType || !initValueType.hasRank() ||
+        initValueType.getRank() != 0)
+      return op_.emitError() << "expects init_values (e.g. operand #"
+                             << numInputs + index << ") "
+                             << "to be 0-dimensional tensors";
+  }
+
+  // reduce_window_i3...reduce_window_i7
+  auto checkRank = [&](StringRef name, int64_t index, Value dynamicAttr,
+                       int64_t expectedRank) -> LogicalResult {
+    auto type = dynamicAttr.getType().dyn_cast<ShapedType>();
+    if (!type || !type.hasRank() || type.getRank() != expectedRank ||
+        !type.getElementType().isIntOrIndex()) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to be a " << expectedRank << "-dimensional tensor "
+             << "of integer or index type";
+    }
+    return success();
+  };
+  if (failed(checkRank("window_dimensions", -5, windowDimensions, 1)) ||
+      failed(checkRank("window_strides", -4, windowStrides, 1)) ||
+      failed(checkRank("base_dilations", -3, baseDilations, 1)) ||
+      failed(checkRank("window_dilations", -2, windowDilations, 1)) ||
+      failed(checkRank("padding", -1, padding, 2)))
+    return failure();
+
+  // reduce_window_i7
+  auto paddingType = getPadding().getType().dyn_cast<ShapedType>();
+  if (!paddingType || !paddingType.hasRank() || paddingType.getRank() != 2 ||
+      paddingType.getDimSize(1) != 2 ||
+      !paddingType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects padding_type (operand #" << op_.getNumOperands() - 1
+           << ") to be a 2-dimensional tensor of integer or index type";
+
+  // reduce_window_c2
+  std::optional<ArrayRef<int64_t>> inputShape;
+  for (auto inputType : inputTypes) {
+    if (!inputType.hasRank()) continue;
+    if (!inputShape) inputShape = inputType.getShape();
+    if (failed(verifyCompatibleShape(inputType.getShape(), *inputShape)))
+      return op_.emitError() << "expects all inputs (operands 0.." << numInputs
+                             << ") to have compatible shapes";
+  }
+
+  // reduce_window_c3
+  for (auto [inputType, initValueType] :
+       llvm::zip(inputTypes, initValueTypes)) {
+    if (inputType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects inputs (operands 0.." << numInputs
+                             << ") and init_values (operands " << numInputs
+                             << ".." << numInputs * 2 << ") to have pairwise "
+                             << "the same element types";
+  }
+
+  // reduce_window_c4...reduce_window_c12
+  // In this range, we only verify the constraints with even numbers.
+  // Verifying the constraints with odd numbers would require knowing the
+  // actual values of window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+  auto checkShape = [&](StringRef name, int64_t index, Value dynamicAttr,
+                        ArrayRef<int64_t> expectedShape) -> LogicalResult {
+    auto type = dynamicAttr.getType().cast<ShapedType>();
+    if (type.getShape() != expectedShape) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to have shape [" << expectedShape << "]";
+    }
+    return success();
+  };
+  if (inputShape) {
+    auto inputRank = static_cast<int64_t>(inputShape->size());
+    if (failed(checkShape("window_dimensions", -5, windowDimensions,
+                          {inputRank})) ||
+        failed(checkShape("window_strides", -4, windowStrides, {inputRank})) ||
+        failed(checkShape("base_dilations", -3, baseDilations, {inputRank})) ||
+        failed(
+            checkShape("window_dilations", -2, windowDilations, {inputRank})) ||
+        failed(checkShape("padding", -1, padding, {inputRank, 2})))
+      return failure();
+  }
+
+  // reduce_window_c13
+  if (op_.getCalledComputations().size() != 1)
+    return op_.emitError() << "expects called_computations to have 1 element";
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  if (!bodyFunc)
+    return op_.emitError() << "expects called_computations to refer to "
+                           << "a function that exists within a parent module";
+
+  // reduce_window_c13
+  SmallVector<Type> expectedBodyInputs;
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  SmallVector<Type> expectedBodyOutputs;
+  llvm::append_range(expectedBodyOutputs, initValueTypes);
+  auto expectedBodyType = FunctionType::get(
+      op_.getContext(), expectedBodyInputs, expectedBodyOutputs);
+  if (bodyFunc.getFunctionType() != expectedBodyType)
+    return op_.emitError() << "expects body to have type " << expectedBodyType;
+
+  // reduce_window_c14
+  SmallVector<ShapedType> resultTypes;
+  std::optional<ArrayRef<int64_t>> resultShape;
+  for (auto result : results) {
+    auto resultType = result.getType().dyn_cast<ShapedType>();
+    resultTypes.push_back(resultType);
+    if (!resultType) return op_.emitError() << "expects results to be tensors";
+
+    if (!resultType.hasRank()) continue;
+    if (!resultShape) resultShape = resultType.getShape();
+    if (failed(verifyCompatibleShape(resultType.getShape(), *resultShape)))
+      return op_.emitError() << "expects all results to have compatible shapes";
+  }
+
+  // reduce_window_c15
+  // Verifying this constraint would require knowing the actual values of
+  // window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+
+  // reduce_window_c16
+  for (auto [resultType, initValueType] :
+       llvm::zip(resultTypes, initValueTypes)) {
+    if (resultType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects results and init_values (operands "
+                             << numInputs << ".." << numInputs * 2 << ") "
+                             << "to have pairwise the same element types";
+  }
+
+  return success();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInputs() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(0, numInputs);
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInitValues() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(numInputs, numInputs);
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDimensions() {
+  return op_.getInputs()[op_.getInputs().size() - 5]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowStrides() {
+  return op_.getInputs()[op_.getInputs().size() - 4]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getBaseDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 3]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 2]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getPadding() {
+  return op_.getInputs()[op_.getInputs().size() - 1]
+      .cast<TypedValue<ShapedType>>();
+}
+
+Region& DynamicReduceWindowOpAdaptor::getBody() {
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  return bodyFunc.getBody();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getResults() {
+  return op_.getResults();
+}
+
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_reduce_window") return {};
+  return DynamicReduceWindowOpAdaptor(op);
+}
+
+LogicalResult DynamicRngBitGeneratorOpAdaptor::verify() {
+  // Before checking the constraints inherited from RngBitGeneratorOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_rng_bit_generator".
+    // rng_algorithm comes from the operation.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "rng_algorithm")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return op_.emitError() << "expects @stablehlo.dynamic_rng_bit_generator";
+  if (!op_->hasAttr("rng_algorithm"))
+    return op_.emitError() << "expects an rng_algorithm";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto rngAlgorithmAttr = op_->getDiscardableAttr("rng_algorithm");
+  auto initialState = op_.getInputs()[0];
+  auto outputShape = op_.getInputs()[1];
+  auto outputState = op_.getResults()[0];
+  auto output = op_.getResults()[1];
+
+  // dynamic_rng_bit_generator_i1
+  if (!rngAlgorithmAttr.isa<RngAlgorithmAttr>())
+    return op_.emitError()
+           << "expects a #stablehlo<rng_algorithm ...> rng_algorithm";
+
+  // dynamic_rng_bit_generator_i2
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto initialStateType = initialState.getType().dyn_cast<ShapedType>();
+  if (!initialStateType || !initialStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects initial_state (operand #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_i3
+  auto outputShapeType = outputShape.getType().dyn_cast<ShapedType>();
+  if (!outputShapeType || !outputShapeType.hasRank() ||
+      outputShapeType.getRank() != 1 ||
+      !outputShapeType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects output_shape (operand #1) "
+           << "to be a 1-dimensional tensor of integer or index type";
+
+  // dynamic_rng_bit_generator_o1
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto outputStateType = outputState.getType().dyn_cast<ShapedType>();
+  if (!outputStateType || !outputStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output_state (result #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_o2
+  auto outputType = output.getType().dyn_cast<ShapedType>();
+  if (!outputType || !outputType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output (result #1) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_c1
+  if (!hlo::isCompatibleForHloTypeInference(initialStateType, outputStateType))
+    return op_.emitError()
+           << "expects initial_state (operand #0) and output_state (result #0) "
+           << "to have compatible shapes";
+
+  // dynamic_rng_bit_generator_c2
+  // TODO(#486): Verify rng_algorithm in RngBitGeneratorOp.
+
+  // dynamic_rng_bit_generator_c3
+  if (!hlo::isCompatibleForHloTypeInference(outputShape, outputType))
+    return op_.emitError() << "expects output (result #1) to have shape  "
+                           << "compatible with output_shape (operand #2)";
+
+  return success();
+}
+
+RngAlgorithm DynamicRngBitGeneratorOpAdaptor::getRngAlgorithm() {
+  return op_->getDiscardableAttr("rng_algorithm")
+      .cast<RngAlgorithmAttr>()
+      .getValue();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getInitialState() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputShape() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputState() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutput() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return {};
+  return DynamicRngBitGeneratorOpAdaptor(op);
+}
+
+LogicalResult DynamicTopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_top_k".
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_top_k")
+    return op_.emitError() << "expects @stablehlo.dynamic_top_k";
+
+  auto operand = op_.getInputs()[0];
+  auto k = op_.getInputs()[1];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+
+  // dynamic_top_k_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_i2
+  auto kType = k.getType().dyn_cast<ShapedType>();
+  if (!kType || !kType.hasRank() || kType.getRank() != 0 ||
+      !kType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects k (operand #1) "
+           << "to be a 0-dimensional tensor of integer or index type";
+
+  // dynamic_top_k_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // dynamic_top_k_c1
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] =
+      valuesType.getDimSize(valuesType.getRank() - 1);
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension";
+
+  // dynamic_top_k_c2
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // dynamic_top_k_c3
+  if (!operandType.isDynamicDim(operandLastDim) &&
+      !valuesType.isDynamicDim(operandLastDim) &&
+      operandType.getDimSize(operandLastDim) <
+          valuesType.getDimSize(operandLastDim))
+    return op_.emitError() << "expects the values last dimension to have size "
+                              "at least as large "
+                           << "as operand last dimension";
+
+  // dynamic_top_k_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getK() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_top_k") return {};
+  return DynamicTopKOpAdaptor(op);
+}
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.h b/stablehlo/stablehlo/experimental/dialect/StablehloOps.h
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.h
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.h
@@ -0,0 +1,230 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+
+// This file supports XLA-specific experiments with the StableHLO opset.
+// These experiments are not yet ready to be upstreamed to openxla/stablehlo
+// and are incubating towards the respective StableHLO RFCs.
+//
+// Custom calls (which are the implementation vehicle of these experiments)
+// don't have compatibility guarantees within the StableHLO process, but
+// the StableHLO team at Google provides out-of-band guarantees for these
+// custom calls, with the same compatibility window as StableHLO upstream.
+
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LogicalResult.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/Base.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+// The DynamicReduceWindowOp experiment provides a dynamic version of
+// ReduceWindowOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicReduceWindowOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_reduce_window` custom call.
+// This custom call has the following operands which represent a dynamic version
+// of operands and attributes of ReduceWindowOp:
+//   * [0:N]   => inputs
+//   * [N:2*N] => init_values
+//   * [-5]    => window_dimensions
+//   * [-4]    => window_strides
+//   * [-3]    => base_dilations
+//   * [-2]    => window_dilations
+//   * [-1]    => padding
+// Additionally, to represent the body of DynamicReduceWindowOp, the custom call
+// has a satellite function attached to the custom call via called_computations.
+//
+// Semantics of DynamicReduceWindowOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window
+// with the following exceptions:
+//   1) All tensor constants, i.e. window_dimensions, window_strides,
+//      base_dilations, window_dilations and padding, become tensors of
+//      integer type.
+//   2) As a result, some of the constraints can no longer be validated
+//      statically. However, this operation still expects these constraints
+//      to hold dynamically, and if they don't hold, the behavior is undefined.
+class DynamicReduceWindowOpAdaptor {
+ public:
+  DynamicReduceWindowOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::ReduceWindowOp, except that all the
+  // std::optional<DenseIntElementsAttr> attributes have turned into values.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  ValueRange getInputs();
+  ValueRange getInitValues();
+  TypedValue<ShapedType> getWindowDimensions();
+  TypedValue<ShapedType> getWindowStrides();
+  TypedValue<ShapedType> getBaseDilations();
+  TypedValue<ShapedType> getWindowDilations();
+  TypedValue<ShapedType> getPadding();
+  Region& getBody();
+  ValueRange getResults();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicReduceWindowOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_reduce_window".
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op);
+
+// The DynamicRngBitGeneratorOp experiment provides a dynamic version of
+// RngBitGeneratorOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicRngBitGeneratorOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator` custom call.
+// This custom call has the regular operand of RngBitGeneratorOp plus an
+// additional `output_shape` operand that determines the shape of the output:
+//   * [0] => initial_state
+//   * [1] => output_shape
+//
+// Semantics of DynamicRngBitGeneratorOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator
+// extended with an additional input (I3) and an additional constraint (C3):
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `rng_algorithm` | enum of `DEFAULT`, `THREE_FRY`, and `PHILOX` |
+// | (I2)  | `initial_state` | 1-dimensional tensor of type `ui64`          |
+// | (I3)  | `output_shape`  | 1-dimensional tensor of integer type         |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `output_state` | 1-dimensional tensor of type `ui64`      |
+// | `output`       | tensor of integer or floating-point type |
+//
+// #### Constraints
+//
+// * (C1) `type(initial_state) = type(output_state)`.
+// * (C2) `size(initial_state)` is defined as:
+//   * implementation-defined if `rng_algorithm = DEFAULT`.
+//   * `2` if `rng_algorithm = THREE_FRY`.
+//   * `2` or `3` if `rng_algorithm = PHILOX`.
+// * (C3) `shape(output) = output_shape`.
+class DynamicRngBitGeneratorOpAdaptor {
+ public:
+  DynamicRngBitGeneratorOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::RngBitGeneratorOp, extended with the
+  // additional `output_shape` operand.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  RngAlgorithm getRngAlgorithm();
+  TypedValue<ShapedType> getInitialState();
+  TypedValue<ShapedType> getOutputShape();
+  TypedValue<ShapedType> getOutputState();
+  TypedValue<ShapedType> getOutput();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicRngBitGeneratorOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_rng_bit_generator".
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op);
+
+// The DynamicTopKOp experiment provides a dynamic version of
+// TopKOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicTopKOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_top_k` custom call.
+// This custom call has the regular operand of TopKOp plus an
+// additional `k` operand that determines the shape of the output.
+//
+// Semantics of DynamicTopKOp are inherited from semantics of Chlo.TopKOp.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | 0-dimensional tensor of integer or index type|
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `element_type(values) = element_type(operand)`
+// * (C3) `shape(values)[-1] <= shape(operand)[-1]`
+// * (C4) `shape(indices) = shape(values)`
+class DynamicTopKOpAdaptor {
+ public:
+  DynamicTopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getK();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicTopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_top_k".
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op);
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
diff --ruN a/stablehlo/stablehlo/experimental/tests/BUILD.bazel b/stablehlo/stablehlo/experimental/tests/BUILD.bazel
--- stablehlo/stablehlo/experimental/tests/BUILD.bazel
+++ stablehlo/stablehlo/experimental/tests/BUILD.bazel
@@ -0,0 +1,59 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@bazel_skylib//rules:expand_template.bzl", "expand_template")
+load("@llvm-project//llvm:lit_test.bzl", "lit_test", "package_path")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+# Equivalent of configure_lit_site_cfg from CMakeLists.txt.
+expand_template(
+    name = "lit_site_cfg_py_gen",
+    testonly = True,
+    out = "lit.site.cfg.py",
+    substitutions = {
+        "@LIT_SITE_CFG_IN_HEADER@": "# Autogenerated, do not edit.",
+        "@LLVM_TOOLS_DIR@": package_path("@llvm-project//llvm:BUILD"),
+        "\"@STABLEHLO_TOOLS_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+        "\"@STABLEHLO_SOURCE_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+    },
+    template = "lit.site.cfg.py.in",
+)
+
+# Equivalent of add_lit_testsuite from CMakeLists.txt.
+[
+    lit_test(
+        name = "%s.test" % src,
+        size = "small",
+        srcs = [src],
+        data = [
+            "lit.cfg.py",
+            "lit.site.cfg.py",
+            "//:stablehlo-opt",
+            "//:stablehlo-translate",
+            "//stablehlo/experimental:experimental-stablehlo-opt",
+            "@llvm-project//llvm:FileCheck",
+            "@llvm-project//llvm:not",
+        ] + glob(["%s.bc" % src]),
+        tags = ["stablehlo_tests"],
+    )
+    for src in glob(["**/*.mlir"])
+]
+
+test_suite(
+    name = "experimental_stablehlo_tests",
+    tags = ["experimental_stablehlo_tests"],
+)
diff --ruN a/stablehlo/stablehlo/experimental/tests/CMakeLists.txt b/stablehlo/stablehlo/experimental/tests/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tests/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tests/CMakeLists.txt
@@ -0,0 +1,29 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+configure_lit_site_cfg(
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.site.cfg.py.in
+  ${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg.py
+  MAIN_CONFIG
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.cfg.py
+)
+add_lit_testsuite(check-experimental-stablehlo-tests "Running the experimental/tests/ suite"
+  ${CMAKE_CURRENT_BINARY_DIR}
+  DEPENDS
+  FileCheck
+  experimental-stablehlo-opt
+  stablehlo-translate
+)
+add_dependencies(check-stablehlo-quick check-experimental-stablehlo-tests)
diff --ruN a/stablehlo/stablehlo/experimental/tests/chlo_recompose_ops.mlir b/stablehlo/stablehlo/experimental/tests/chlo_recompose_ops.mlir
--- stablehlo/stablehlo/experimental/tests/chlo_recompose_ops.mlir
+++ stablehlo/stablehlo/experimental/tests/chlo_recompose_ops.mlir
@@ -0,0 +1,51 @@
+// RUN: experimental-stablehlo-opt --experimental-chlo-recompose-ops --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// -----
+
+// CHECK-LABEL: func @recompose_topk
+func.func @recompose_topk(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // CHECK: %values, %indices = chlo.top_k(%arg0, k = 4) {largest = true} : tensor<5x16xf32> -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = true}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @recompose_topk_invalid_attr
+func.func @recompose_topk_invalid_attr(%arg0: tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>) {
+  // CHECK: stablehlo.custom_call @mhlo.topk
+  %0:2 = stablehlo.custom_call @mhlo.topk(%arg0) {
+    mhlo.attributes = { k = 4 : i64, largest = false}
+  } : (tensor<5x16xf32>) -> (tensor<?x?xf32>, tensor<?x?xi32>)
+  return %0#0, %0#1 : tensor<?x?xf32>, tensor<?x?xi32>
+}
+
+// -----
+
+// CHECK-LABEL: @recompose_tan
+func.func @recompose_tan(%arg0: tensor<16xf32>) -> tensor<?xf32> {
+  // CHECK: %0 = chlo.tan %arg0 : tensor<16xf32> -> tensor<?xf32>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "mhlo.tan",
+    mhlo.attributes = {},
+    mhlo.version = 1 : i64
+  } : (tensor<16xf32>) -> tensor<?xf32>
+  func.return %0 : tensor<?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: @recompose_erf
+func.func @recompose_erf(%arg0: tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16> {
+  // CHECK: %0 = chlo.erf %arg0 : tensor<3x20x20xbf16> -> tensor<?x20x20xbf16>
+  %0 = "stablehlo.custom_call"(%arg0) {
+    backend_config = "",
+    call_target_name = "mhlo.erf",
+    mhlo.attributes = {},
+    mhlo.version = 1 : i64
+  } : (tensor<3x20x20xbf16>) -> tensor<?x20x20xbf16>
+  func.return %0 : tensor<?x20x20xbf16>
+}
+
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.cfg.py b/stablehlo/stablehlo/experimental/tests/lit.cfg.py
--- stablehlo/stablehlo/experimental/tests/lit.cfg.py
+++ stablehlo/stablehlo/experimental/tests/lit.cfg.py
@@ -0,0 +1,46 @@
+"""Lit configuration to drive test in this repo."""
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# -*- Python -*-
+# pylint: disable=undefined-variable
+
+import os
+
+import lit.formats
+from lit.llvm import llvm_config
+
+# Populate Lit configuration with the minimal required metadata.
+# Some metadata is populated in lit.site.cfg.py.in.
+config.name = 'STABLEHLO_TESTS_SUITE'
+config.test_format = lit.formats.ShTest(not llvm_config.use_lit_shell)
+config.suffixes = ['.mlir']
+config.test_source_root = os.path.dirname(__file__)
+
+# Disallow reusing variables across CHECK-LABEL matches.
+# A variable can eschew this (be made "global") by prefixing its name with $.
+config.environment['FILECHECK_OPTS'] = '-enable-var-scope'
+
+# Make LLVM and StableHLO tools available in RUN directives
+tools = [
+  'FileCheck',
+  'experimental-stablehlo-opt',
+  'stablehlo-translate',
+  'not',
+]
+tool_dirs = [
+  config.llvm_tools_dir,
+  config.stablehlo_tools_dir,
+]
+llvm_config.add_tool_substitutions(tools, tool_dirs)
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in b/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
--- stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
+++ stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
@@ -0,0 +1,21 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+@LIT_SITE_CFG_IN_HEADER@
+
+import lit.llvm
+lit.llvm.initialize(lit_config, config)
+config.llvm_tools_dir = "@LLVM_TOOLS_DIR@"
+config.stablehlo_tools_dir = "@STABLEHLO_TOOLS_DIR@"
+lit_config.load_config(config, "@STABLEHLO_SOURCE_DIR@" + "/stablehlo/experimental/tests/lit.cfg.py")
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
@@ -0,0 +1,344 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-canonicalize-dynamism --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_static_result_type
+func.func @dynamic_reduce_window_success_static_result_type(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<2x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) <{
+  //          CHECK-SAME:   base_dilations = array<i64: 2, 1>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = array<i64: 3, 1>,
+  //          CHECK-SAME:   window_dimensions = array<i64: 2, 1>,
+  //          CHECK-SAME:   window_strides = array<i64: 4, 1>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) : (tensor<3x2xf32>, tensor<f32>) -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %5 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_dynamic_result_type
+func.func @dynamic_reduce_window_success_dynamic_result_type(%arg0: tensor<?x2xf32>, %arg1: tensor<f32>) -> tensor<?x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) <{
+  //          CHECK-SAME:   base_dilations = array<i64: 2, 1>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = array<i64: 3, 1>,
+  //          CHECK-SAME:   window_dimensions = array<i64: 2, 1>,
+  //          CHECK-SAME:   window_strides = array<i64: 4, 1>
+  //          CHECK-SAME: }> ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) : (tensor<?x2xf32>, tensor<f32>) -> tensor<?x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<?x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<?x2xf32>
+  func.return %5 : tensor<?x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_reduce_window.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %arg2, %0, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_strides
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_strides(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %arg2, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_base_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_base_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %arg2, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %arg2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_padding
+func.func @dynamic_reduce_window_inapplicable_dynamic_padding(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2x2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %arg2) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_success
+func.func @dynamic_rng_bit_generator_success(%arg0: tensor<2xui64>) -> tensor<1x4xf32> {
+  // CHECK-NOT: stablehlo.dynamic_rng_bit_generator
+  // CHECK: stablehlo.rng_bit_generator %arg0, algorithm = DEFAULT : (tensor<2xui64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_rng_bit_generator.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape(%arg0: tensor<2xui64>, %arg1: tensor<2xi64>) -> tensor<1x4xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %arg1) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type(%arg0: tensor<2xui64>) -> tensor<?x?xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<?x?xf32>)
+  return %1#1 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_success
+func.func @dynamic_top_k_success(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: chlo.top_k
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_failure_k_mismatch
+func.func @dynamic_top_k_failure_k_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: @stablehlo.dynamic_top_k
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_not_float
+func.func @dynamic_top_k_error_operand_not_float(%arg0: tensor<16xcomplex<f64>>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xcomplex<f64>>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_unranked
+func.func @dynamic_top_k_error_operand_unranked(%arg0: tensor<*xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<*xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_scalar_operand
+func.func @dynamic_top_k_error_scalar_operand(%arg0: tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<f32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_integer
+func.func @dynamic_top_k_error_k_not_integer(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3.> : tensor<f32>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_scalar
+func.func @dynamic_top_k_error_k_not_scalar(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3> : tensor<1xui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<1xui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O1
+// CHECK-LABEL: func @dynamic_top_k_error_values_not_float
+func.func @dynamic_top_k_error_values_not_float(%arg0: tensor<16xf32>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects values (result #0) to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O2
+// CHECK-LABEL: func @dynamic_top_k_error_indices_not_i32
+func.func @dynamic_top_k_error_indices_not_i32(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi64>) {
+  // expected-error@+2{{expects indices (result #1) to be a tensor of si32}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi64>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi64>
+}
+
+// -----
+
+// dynamic_top_k C1
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_rank
+func.func @dynamic_top_k_error_values_bad_rank(%arg0: tensor<16xf32>) -> (tensor<3x4xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values shape to match the operand shape in all but the last dimension}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3x4xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3x4xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C2
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_element_type
+func.func @dynamic_top_k_error_values_bad_element_type(%arg0: tensor<16xf32>) -> (tensor<3xf64>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values element type to be the same as the operand element type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf64>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf64>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C3
+// CHECK-LABEL: func @dynamic_top_k_error_values_last_dim_too_large
+func.func @dynamic_top_k_error_values_last_dim_too_large(%arg0: tensor<16xf32>) -> (tensor<17xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values last dimension to have size at least as large as operand last dimension}}
+  %k = stablehlo.constant dense<17> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<17xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<17xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C4
+// CHECK-LABEL: func @dynamic_top_k_error_indices_shape_mismatch
+func.func @dynamic_top_k_error_indices_shape_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<4xi32>) {
+  // expected-error@+2{{expects the indices shape to match the values shape}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<4xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<4xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
@@ -0,0 +1,42 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-refine-shapes --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: @main
+func.func @main(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<?x?xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window{{.*}} -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<?x?xf32>
+  func.return %5 : tensor<?x?xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: @refine_dynamic_rng_bit_generator
+func.func @refine_dynamic_rng_bit_generator(%arg0: tensor<2xui64>) -> (tensor<?xui64>, tensor<?x?xf32>) {
+  // CHECK: stablehlo.dynamic_rng_bit_generator{{.*}} -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<?xui64>, tensor<?x?xf32>)
+  func.return %1#0, %1#1 : tensor<?xui64>, tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_dynamic_top_k
+func.func @refine_dynamic_top_k(%arg0: tensor<16xf32>) -> (tensor<?xf32>, tensor<?xi32>) {
+  // CHECK: stablehlo.dynamic_top_k{{.*}} -> (tensor<4xf32>, tensor<4xi32>)
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<?xf32>, tensor<?xi32>)
+  return %1#0, %1#1 : tensor<?xf32>, tensor<?xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tools/CMakeLists.txt b/stablehlo/stablehlo/experimental/tools/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tools/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tools/CMakeLists.txt
@@ -0,0 +1,41 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_OPTIONAL_SOURCES
+  StablehloOptMain.cpp
+)
+
+# stablehlo-opt
+get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)
+get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)
+get_property(extension_libs GLOBAL PROPERTY MLIR_EXTENSION_LIBS)
+set(LIBS
+        ${dialect_libs}
+        ${conversion_libs}
+        ${extension_libs}
+        ExperimentalStablehloPasses
+        MLIROptLib
+        StablehloRegister
+        StablehloTestUtils
+        StablehloPasses
+        InterpreterOps
+        StablehloTOSATransforms
+        )
+add_llvm_executable(experimental-stablehlo-opt StablehloOptMain.cpp)
+llvm_update_compile_flags(experimental-stablehlo-opt)
+target_link_libraries(experimental-stablehlo-opt PRIVATE ${LIBS})
+
+mlir_check_all_link_libraries(experimental-stablehlo-opt)
+
diff --ruN a/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp b/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
--- stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
+++ stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
@@ -0,0 +1,46 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/Dialect/Tosa/Transforms/Passes.h"
+#include "mlir/InitAllDialects.h"
+#include "mlir/InitAllExtensions.h"
+#include "mlir/InitAllPasses.h"
+#include "mlir/Tools/mlir-opt/MlirOptMain.h"
+#include "stablehlo/conversions/tosa/transforms/Passes.h"
+#include "stablehlo/dialect/Register.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/reference/InterpreterOps.h"
+#include "stablehlo/tests/TestUtils.h"
+#include "stablehlo/transforms/Passes.h"
+
+int main(int argc, char **argv) {
+  mlir::registerAllPasses();
+  mlir::hlo::registerAllTestPasses();
+  mlir::stablehlo::registerPassPipelines();
+  mlir::stablehlo::registerPasses();
+  mlir::stablehlo::experimental::registerPasses();
+  mlir::tosa::registerStablehloLegalizeToTosaPassPass();
+  mlir::tosa::registerStablehloPrepareForTosaPassPass();
+
+  mlir::DialectRegistry registry;
+  mlir::registerAllDialects(registry);
+  mlir::registerAllExtensions(registry);
+  mlir::stablehlo::registerAllDialects(registry);
+  registry.insert<mlir::stablehlo::interpreter::InterpreterDialect>();
+
+  return failed(
+      mlir::MlirOptMain(argc, argv, "Experimental StableHLO optimizer driver\n", registry));
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt b/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
--- stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
@@ -0,0 +1,40 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_TARGET_DEFINITIONS Passes.td)
+mlir_tablegen(Passes.h.inc -gen-pass-decls)
+add_public_tablegen_target(ExperimentalPassesIncGen)
+
+add_mlir_dialect_library(ExperimentalStablehloPasses
+  PARTIAL_SOURCES_INTENDED
+  ChloRecomposeOps.cpp
+  StablehloCanonicalizeDynamism.cpp
+  StablehloRefineShapes.cpp
+
+  DEPENDS
+  ExperimentalPassesIncGen
+
+  LINK_LIBS PUBLIC
+  ChloOps
+  MLIRFuncDialect
+  MLIRIR
+  MLIRInferTypeOpInterface
+  MLIRSupport
+  MLIRTransformUtils
+  ExperimentalStablehloOps
+  StablehloBase
+  StablehloOps
+  StablehloPasses
+  StablehloTypeInference
+)
diff --ruN a/stablehlo/stablehlo/experimental/transforms/ChloRecomposeOps.cpp b/stablehlo/stablehlo/experimental/transforms/ChloRecomposeOps.cpp
--- stablehlo/stablehlo/experimental/transforms/ChloRecomposeOps.cpp
+++ stablehlo/stablehlo/experimental/transforms/ChloRecomposeOps.cpp
@@ -0,0 +1,180 @@
+/* Copyright 2024 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/Casting.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_CHLORECOMPOSEOPSPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+FailureOr<DictionaryAttr> getCustomCallOpAttributes(CustomCallOp op,
+                                                    PatternRewriter& rewriter) {
+  auto attrs = llvm::dyn_cast_or_null<DictionaryAttr>(
+      op->getDiscardableAttr("mhlo.attributes"));
+  if (!attrs)
+    return rewriter.notifyMatchFailure(
+        op, "Expected mhlo.attributes dictionary attribute.");
+  return attrs;
+}
+
+LogicalResult verifyCustomCallOpAttributes(
+    CustomCallOp op, PatternRewriter& rewriter,
+    std::function<LogicalResult(NamedAttribute)> verifyFn) {
+  auto attrs = getCustomCallOpAttributes(op, rewriter);
+  if (failed(attrs)) return failure();
+
+  for (auto attr : attrs->getValue()) {
+    if (failed(verifyFn(attr))) return failure();
+  }
+  return success();
+}
+
+// Experimental and public ops in MHLO that do not exist yet in StableHLO
+// can be encoded as a StableHLO CustomCallOp to allow round-tripping
+// between dialects. Some of these ops are CHLO ops that are accelerated by XLA.
+// For these ops we can recompose to CHLO.
+//
+// Example:
+//  %0 = stablehlo.custom_call @mhlo.topk(...) {...}
+//  ==>
+//   %0 = "chlo.topk"(...) {...}
+template <typename OpType>
+LogicalResult recomposeChloOpFromCustomCall(stablehlo::CustomCallOp op,
+                                            PatternRewriter& rewriter) {
+  // Only call_target_name, backend_config, called_computations, mhlo.version,
+  // and mhlo.attributes are compatible with the extensibility protocol.
+  auto isSupportedAttrName = [](NamedAttribute attr) {
+    auto name = attr.getName();
+    return name == "call_target_name" || name == "backend_config" ||
+           name == "called_computations" || name == "mhlo.attributes" ||
+           name == "mhlo.version";
+  };
+  if (!llvm::all_of(op->getAttrs(), isSupportedAttrName) ||
+      !op.getBackendConfig().empty()) {
+    return rewriter.notifyMatchFailure(
+        op, "CHLO Recompose custom call did not have required attributes.");
+  }
+  if (!op.getCalledComputations().empty())
+    return rewriter.notifyMatchFailure(op, "Ops with regions not supported.");
+
+  auto attrs = getCustomCallOpAttributes(op, rewriter);
+  if (failed(attrs)) return failure();
+
+  rewriter.replaceOpWithNewOp<OpType>(op, op->getResultTypes(),
+                                      op->getOperands(), attrs->getValue());
+  return success();
+}
+
+struct TopKOpRecomposePattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp op,
+                                PatternRewriter& rewriter) const override {
+    if (op.getCallTargetName() != "mhlo.topk") return failure();
+    auto res = verifyCustomCallOpAttributes(
+        op, rewriter, [&](NamedAttribute attr) -> LogicalResult {
+          if (attr.getName() != "largest") return success();
+          if (attr.getValue().cast<BoolAttr>().getValue() == false)
+            return rewriter.notifyMatchFailure(
+                op, "largest = false is not supported.");
+          return success();
+        });
+    if (failed(res)) return failure();
+    return recomposeChloOpFromCustomCall<chlo::TopKOp>(op, rewriter);
+  }
+};
+
+struct TanOpRecomposePattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp op,
+                                PatternRewriter& rewriter) const override {
+    if (op.getCallTargetName() != "mhlo.tan") return failure();
+    return recomposeChloOpFromCustomCall<chlo::TanOp>(op, rewriter);
+  }
+};
+
+struct ErfOpRecomposePattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp op,
+                                PatternRewriter& rewriter) const override {
+    if (op.getCallTargetName() != "mhlo.erf") return failure();
+    return recomposeChloOpFromCustomCall<chlo::ErfOp>(op, rewriter);
+  }
+};
+
+}  // namespace
+
+struct ChloRecomposeOpsPass
+    : public impl::ChloRecomposeOpsPassBase<ChloRecomposeOpsPass> {
+  using ChloRecomposeOpsPassBase::ChloRecomposeOpsPassBase;
+
+  void runOnOperation() override {
+    // Do a single traversal to recompose CustomCallOp to CHLO ops.
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 1;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::ExistingOps;
+
+    RewritePatternSet patterns(&getContext());
+    patterns.add<TopKOpRecomposePattern>(&getContext());
+    patterns.add<TanOpRecomposePattern>(&getContext());
+    patterns.add<ErfOpRecomposePattern>(&getContext());
+
+    // Only apply to CustomCallOps
+    auto moduleOp = getOperation();
+    llvm::SmallVector<Operation*> candidateOps;
+    moduleOp.walk([&](CustomCallOp op) { candidateOps.push_back(op); });
+
+    if (failed(applyOpPatternsAndFold(candidateOps, std::move(patterns),
+                                      config))) {
+      moduleOp.emitError("Failed to converge ChloRecomposeOps in ")
+          << config.maxIterations << " iterations";
+      return signalPassFailure();
+    }
+  }
+};
+
+void createChloLegalizeToStablehloPipeline(OpPassManager& pm) {
+  pm.addPass(mlir::stablehlo::experimental::createChloRecomposeOpsPass());
+  pm.addNestedPass<mlir::func::FuncOp>(
+      mlir::stablehlo::createChloLegalizeToStablehloPass());
+  pm.addNestedPass<mlir::func::FuncOp>(
+      mlir::stablehlo::createShapeLegalizeToStablehloPass());
+}
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.h b/stablehlo/stablehlo/experimental/transforms/Passes.h
--- stablehlo/stablehlo/experimental/transforms/Passes.h
+++ stablehlo/stablehlo/experimental/transforms/Passes.h
@@ -0,0 +1,38 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+#define STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+
+#include <memory>
+
+#include "mlir/Pass/Pass.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DECL
+#define GEN_PASS_REGISTRATION
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+void createChloLegalizeToStablehloPipeline(OpPassManager &pm);
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.td b/stablehlo/stablehlo/experimental/transforms/Passes.td
--- stablehlo/stablehlo/experimental/transforms/Passes.td
+++ stablehlo/stablehlo/experimental/transforms/Passes.td
@@ -0,0 +1,39 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def StablehloCanonicalizeDynamismPass : Pass<"experimental-stablehlo-canonicalize-dynamism", "func::FuncOp"> {
+  let summary = "(Experimental) Canonicalizes dynamic StableHLO ops into static ops.";
+  let description = [{
+    Experimental version of the --stablehlo-canonicalize-dynamism pass.
+  }];
+  let dependentDialects = ["mlir::chlo::ChloDialect"];
+}
+
+def StablehloRefineShapesPass : Pass<"experimental-stablehlo-refine-shapes", "ModuleOp"> {
+  let summary = "(Experimental) Refines shapes across a StableHLO program.";
+  let description = [{
+    Experimental version of the --stablehlo-refine-shapes pass.
+  }];
+}
+
+def ChloRecomposeOpsPass : Pass<"experimental-chlo-recompose-ops", "ModuleOp"> {
+  let summary = "(Experimental) Recompose CHLO ops serialized as custom calls.";
+  let description = [{
+    Experimental version of CHLO serialization support.
+  }];
+  let dependentDialects = ["chlo::ChloDialect"];
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
@@ -0,0 +1,171 @@
+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2023 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOCANONICALIZEDYNAMISMPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+struct CanonicalizeDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // ReduceWindowOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op, "expected static window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op, "expected static base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected static padding");
+    auto newOp = rewriter.create<ReduceWindowOp>(
+        op->getLoc(), op->getResultTypes(), op.getInputs(), op.getInitValues(),
+        rewriter.getDenseI64ArrayAttr(windowDimensions),
+        rewriter.getDenseI64ArrayAttr(windowStrides),
+        rewriter.getDenseI64ArrayAttr(baseDilations),
+        rewriter.getDenseI64ArrayAttr(windowDilations),
+        hlo::getPaddingAttr(&rewriter, padding));
+
+    // Inline the called computation into newOp.
+    // This is somewhat annoying because we also have to rewrite the original
+    // func::ReturnOp into stablehlo::ReturnOp.
+    rewriter.cloneRegionBefore(op.getBody(), newOp.getBody(),
+                               newOp.getBody().end());
+    auto funcReturnOp =
+        cast<func::ReturnOp>(newOp.getBody().front().getTerminator());
+    rewriter.setInsertionPointToEnd(&newOp.getBody().front());
+    rewriter.replaceOpWithNewOp<stablehlo::ReturnOp>(
+        funcReturnOp, funcReturnOp.getOperands());
+    rewriter.replaceOp(op, newOp->getResults());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // This pattern ignores and discards the output_shape operand. We rely on
+    // the verifier to make sure that its value is consistent with result type.
+    if (!succeeded(hlo::matchInts(op.getOutputShape())))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getOutput().getType().cast<ShapedType>().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static output type");
+    rewriter.replaceOpWithNewOp<RngBitGeneratorOp>(
+        op, op->getResultTypes(), op.getRngAlgorithm(), op.getInitialState());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicTopKOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(impl, "expected constant k");
+
+    // We rely on many of the properties checked by verification.
+    auto valuesType = op.getValues().getType().cast<ShapedType>();
+    auto valuesLastDimSize = valuesType.getShape()[valuesType.getRank() - 1];
+    if (hlo::isDynamicDimSize(valuesLastDimSize) ||
+        valuesLastDimSize != k[0])
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected value of k to match the values last dimension size of "
+          "static values type (result #0)");
+
+    rewriter.replaceOpWithNewOp<chlo::TopKOp>(
+        op, op->getResultTypes(), op.getOperand(), k[0]);
+    return success();
+  }
+};
+
+struct StablehloCanonicalizeDynamismPass
+    : public impl::StablehloCanonicalizeDynamismPassBase<
+          StablehloCanonicalizeDynamismPass> {
+  using StablehloCanonicalizeDynamismPassBase::
+      StablehloCanonicalizeDynamismPassBase;
+
+  void runOnOperation() override {
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 2;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    populateStablehloCanonicalizeDynamismPatterns(&patterns, &getContext());
+    patterns.add<CanonicalizeDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicTopKOpPattern>(&getContext());
+
+    auto funcOp = getOperation();
+    if (failed(applyPatternsAndFoldGreedily(funcOp, std::move(patterns),
+                                            config))) {
+      funcOp.emitError("Failed to converge StablehloCanonicalizeDynamism in ")
+          << config.maxIterations << " iterations";
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
@@ -0,0 +1,170 @@
+/* Copyright 2022 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/transforms/StablehloRefineShapes.h"
+
+#include <cstdint>
+
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/Base.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/dialect/TypeInference.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOREFINESHAPESPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+struct RefineDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected constant padding");
+
+    SmallVector<ShapedTypeComponents> inferredReturnTypes;
+    if (failed(hlo::inferReduceWindowOp(
+            /*location=*/{}, op.getInputs(), op.getInitValues(),
+            llvm::to_vector(rewriter.getI64TensorAttr(windowDimensions)
+                                .getValues<int64_t>()),
+            llvm::to_vector(
+                rewriter.getI64TensorAttr(windowStrides).getValues<int64_t>()),
+            llvm::to_vector(
+                rewriter.getI64TensorAttr(baseDilations).getValues<int64_t>()),
+            llvm::to_vector(rewriter.getI64TensorAttr(windowDilations)
+                                .getValues<int64_t>()),
+            hlo::getPaddingAttr(&rewriter, padding), op.getBody(),
+            inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferReduceWindowOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
+struct RefineDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    auto initialStateType = op.getInitialState().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape;
+    if (failed(hlo::matchInts(op.getOutputShape(), outputShape)))
+      return rewriter.notifyMatchFailure(op, "expected constant output_shape");
+
+    // We only need to refine the shape of `output` (the second result).
+    // The shape of `output_state` (the first result) is determined by the shape
+    // of `initial_state`, so we ignore it and provide an empty refinement.
+    return refineReturnTypes(rewriter, op, {{initialStateType}, {outputShape}});
+  }
+};
+
+struct RefineDynamicTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(op, "expected constant k");
+
+    outputShape[operandType.getRank() - 1] = k[0];
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
+  }
+};
+
+struct StablehloRefineShapesPass
+    : public impl::StablehloRefineShapesPassBase<StablehloRefineShapesPass> {
+  using StablehloRefineShapesPassBase::StablehloRefineShapesPassBase;
+
+  void runOnOperation() override {
+    auto func = getStablehloRefineShapesTarget(getOperation());
+    if (!func) return signalPassFailure();
+
+    // The algorithm behind this pass consists of a single traversal of the
+    // function. This is sufficient because we only support one function per
+    // program at the moment.
+    // TODO(#1048): Find out why .maxIterations = 1 no longer works.
+    // There have been recent refactors to applyPatternsAndFoldGreedily
+    // upstream, and that might be the reason.
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 3;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    populateStablehloRefineShapesPatterns(&patterns, &getContext());
+    patterns.add<RefineDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<RefineDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<RefineDynamicTopKOpPattern>(&getContext());
+    if (failed(
+            applyPatternsAndFoldGreedily(func, std::move(patterns), config))) {
+      func.emitError()
+          << "Greedy rewriter in StablehloRefineShapes does not converge after "
+          << config.maxIterations << " iterations.";
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/reference/Ops.cpp b/stablehlo/stablehlo/reference/Ops.cpp
--- stablehlo/stablehlo/reference/Ops.cpp
+++ stablehlo/stablehlo/reference/Ops.cpp
@@ -16,6 +16,7 @@
 #include "stablehlo/reference/Ops.h"
 
 #include <algorithm>
+#include <cstdint>
 
 #include "llvm/ADT/APFloat.h"
 #include "llvm/ADT/APInt.h"
@@ -517,8 +518,8 @@
       auto rank = lhs.getRank();
 
       SmallVector<int64_t> windowStrides(rank - 2, 1);
-      if (auto windowStridesAttr = convolutionOp.getWindowStridesAttr())
-        windowStrides = SmallVector<int64_t>(windowStridesAttr.asArrayRef());
+      if (auto windowStridesAttr = convolutionOp.getWindowStrides())
+        windowStrides = SmallVector<int64_t>(windowStridesAttr.value());
 
       SmallVector<std::pair<int64_t, int64_t>> padding(rank - 2, {0, 0});
       if (auto paddingAttr = convolutionOp.getPaddingAttr()) {
@@ -529,16 +530,16 @@
       }
 
       SmallVector<int64_t> lhsDilation(rank - 2, 1);
-      if (auto lhsDilationAttr = convolutionOp.getLhsDilationAttr())
-        lhsDilation = SmallVector<int64_t>(lhsDilationAttr.asArrayRef());
+      if (auto lhsDilationAttr = convolutionOp.getLhsDilation())
+        lhsDilation = SmallVector<int64_t>(lhsDilationAttr.value());
 
       SmallVector<int64_t> rhsDilation(rank - 2, 1);
-      if (auto rhsDilationAttr = convolutionOp.getRhsDilationAttr())
-        rhsDilation = SmallVector<int64_t>(rhsDilationAttr.asArrayRef());
+      if (auto rhsDilationAttr = convolutionOp.getRhsDilation())
+        rhsDilation = SmallVector<int64_t>(rhsDilationAttr.value());
 
       SmallVector<bool> windowReversal(rank - 2, false);
-      if (auto windowReversalAttr = convolutionOp.getWindowReversalAttr())
-        windowReversal = SmallVector<bool>(windowReversalAttr.asArrayRef());
+      if (auto windowReversalAttr = convolutionOp.getWindowReversal())
+        windowReversal = SmallVector<bool>(windowReversalAttr.value());
 
       auto dimensionNumbers = convolutionOp.getDimensionNumbers();
       auto result = evalConvolutionOp(
@@ -1428,23 +1429,23 @@
                         inputSpatialDimensions.end());
   lhsPermutation.push_back(inputFeatureDimension);
 
-  auto lhsWindowDimensions =
-      concatAndPermute(lhs.getShape()[inputBatchDimension],
-                       extractElements(rhs.getShape(), kernelSpatialDimensions),
-                       lhs.getShape()[inputFeatureDimension], lhsPermutation);
-
-  auto lhsWindowStrides =
-      concatAndPermute(1L, llvm::to_vector(windowStrides), 1L, lhsPermutation);
+  auto lhsWindowDimensions = concatAndPermute<int64_t>(
+      lhs.getShape()[inputBatchDimension],
+      extractElements(rhs.getShape(), kernelSpatialDimensions),
+      lhs.getShape()[inputFeatureDimension], lhsPermutation);
+
+  auto lhsWindowStrides = concatAndPermute<int64_t>(
+      1L, llvm::to_vector(windowStrides), 1L, lhsPermutation);
 
   auto lhsBaseDilations =
-      concatAndPermute(0L, Sizes(lhsDilation) - 1, 0L, lhsPermutation);
-
-  auto lhsWindowDilations =
-      concatAndPermute(1L, llvm::to_vector(rhsDilation), 1L, lhsPermutation);
+      concatAndPermute<int64_t>(0L, Sizes(lhsDilation) - 1, 0L, lhsPermutation);
+
+  auto lhsWindowDilations = concatAndPermute<int64_t>(
+      1L, llvm::to_vector(rhsDilation), 1L, lhsPermutation);
 
   Sizes lhsPaddingLow, lhsPaddingHigh;
-  for (auto paddingPair : concatAndPermute({0, 0}, llvm::to_vector(padding),
-                                           {0, 0}, lhsPermutation)) {
+  for (auto paddingPair : concatAndPermute<std::pair<int64_t, int64_t>>(
+           {0, 0}, llvm::to_vector(padding), {0, 0}, lhsPermutation)) {
     lhsPaddingLow.push_back(paddingPair.first);
     lhsPaddingHigh.push_back(paddingPair.second);
   }
@@ -1461,8 +1462,8 @@
   for (; outputSpatialIndexIt != outputSpatialIndexItEnd;
        ++outputSpatialIndexIt) {
     Sizes lhsWindowStart;
-    for (auto [i, offset] : llvm::enumerate(
-             concatAndPermute(0L, *outputSpatialIndexIt, 0L, lhsPermutation)))
+    for (auto [i, offset] : llvm::enumerate(concatAndPermute<int64_t>(
+             0L, *outputSpatialIndexIt, 0L, lhsPermutation)))
       lhsWindowStart.push_back(lhsWindowStrides[i] * offset);
 
     Sizes limitIndices;
@@ -1507,9 +1508,9 @@
     for (auto dotProductIt = dotProduct.index_begin();
          dotProductIt != dotProduct.index_end();
          ++dotProductIt, ++resultNonSpatialIt) {
-      Index resultIndex(
-          concatAndPermute((*resultNonSpatialIt)[0], *outputSpatialIndexIt,
-                           (*resultNonSpatialIt)[1], resultPermutation));
+      Index resultIndex(concatAndPermute<int64_t>(
+          (*resultNonSpatialIt)[0], *outputSpatialIndexIt,
+          (*resultNonSpatialIt)[1], resultPermutation));
       result.set(resultIndex, dotProduct.get(*dotProductIt));
     }
   }
diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo.mlir
@@ -1317,8 +1317,8 @@
   // CHECK: %[[TMP_33:.*]] = stablehlo.add %[[TMP_30]], %[[TMP_3]]
   // CHECK: %[[TMP_34:.*]] = stablehlo.power %[[TMP_33]], %[[TMP_4]]
   // CHECK: %[[TMP_35:.*]] = stablehlo.constant dense<1.000000e+00>
-  // CHECK: %[[TMP_36:.*]] = stablehlo.multiply %[[TMP_34]], %[[TMP_33]]
-  // CHECK: %[[TMP_37:.*]] = stablehlo.subtract %[[TMP_0]], %[[TMP_35]]
+  // CHECK-DAG: %[[TMP_36:.*]] = stablehlo.multiply %[[TMP_34]], %[[TMP_33]]
+  // CHECK-DAG: %[[TMP_37:.*]] = stablehlo.subtract %[[TMP_0]], %[[TMP_35]]
   // CHECK: %[[TMP_38:.*]] = stablehlo.divide %[[TMP_36]], %[[TMP_37]]
   // CHECK: %[[TMP_39:.*]] = stablehlo.multiply %[[TMP_33]], %[[TMP_33]]
   // CHECK: %[[TMP_40:.*]] = stablehlo.divide %[[TMP_3]], %[[TMP_39]]
@@ -1465,7 +1465,7 @@
 
 // -----
 
-// CHECK: @polygamma_f32
+// CHECK-LABEL: @polygamma_f32
 // CHECK-SAME: (%[[ARG0:.*]]: tensor<f32>, %[[ARG1:.*]]: tensor<f32>)
 func.func @polygamma_f32(%lhs : tensor<f32>, %rhs : tensor<f32>) -> tensor<f32> {
   // CHECK-DAG: %[[TMP_0:.*]] = stablehlo.constant dense<1.000000e+00>
@@ -1592,8 +1592,8 @@
   // CHECK: %[[TMP_121:.*]] = stablehlo.add %[[TMP_118]], %[[TMP_91]]
   // CHECK: %[[TMP_122:.*]] = stablehlo.power %[[TMP_121]], %[[TMP_92]]
   // CHECK: %[[TMP_123:.*]] = stablehlo.constant dense<1.000000e+00>
-  // CHECK: %[[TMP_124:.*]] = stablehlo.multiply %[[TMP_122]], %[[TMP_121]]
-  // CHECK: %[[TMP_125:.*]] = stablehlo.subtract %[[TMP_5]], %[[TMP_123]]
+  // CHECK-DAG: %[[TMP_124:.*]] = stablehlo.multiply %[[TMP_122]], %[[TMP_121]]
+  // CHECK-DAG: %[[TMP_125:.*]] = stablehlo.subtract %[[TMP_5]], %[[TMP_123]]
   // CHECK: %[[TMP_126:.*]] = stablehlo.divide %[[TMP_124]], %[[TMP_125]]
   // CHECK: %[[TMP_127:.*]] = stablehlo.multiply %[[TMP_121]], %[[TMP_121]]
   // CHECK: %[[TMP_128:.*]] = stablehlo.divide %[[TMP_91]], %[[TMP_127]]
@@ -1602,7 +1602,7 @@
   // CHECK: %[[TMP_131:.*]] = stablehlo.constant dense<2.100000e+01>
   // CHECK: %[[TMP_132:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_131]]
   // CHECK: %[[TMP_133:.*]] = stablehlo.multiply %[[TMP_130]], %[[TMP_132]]
-  // CHECK: %[[TMP_134:.*]] = stablehlo.constant dense<-1.39544646E-19>
+  // CHECK: %[[TMP_134:.*]] = stablehlo.constant
   // CHECK: %[[TMP_135:.*]] = stablehlo.add %[[TMP_90]], %[[TMP_134]]
   // CHECK: %[[TMP_136:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_135]]
   // CHECK: %[[TMP_137:.*]] = stablehlo.multiply %[[TMP_133]], %[[TMP_136]]
@@ -1611,7 +1611,7 @@
   // CHECK: %[[TMP_140:.*]] = stablehlo.constant dense<1.900000e+01>
   // CHECK: %[[TMP_141:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_140]]
   // CHECK: %[[TMP_142:.*]] = stablehlo.multiply %[[TMP_139]], %[[TMP_141]]
-  // CHECK: %[[TMP_143:.*]] = stablehlo.constant dense<5.50900303E-18>
+  // CHECK: %[[TMP_143:.*]] = stablehlo.constant
   // CHECK: %[[TMP_144:.*]] = stablehlo.add %[[TMP_137]], %[[TMP_143]]
   // CHECK: %[[TMP_145:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_144]]
   // CHECK: %[[TMP_146:.*]] = stablehlo.multiply %[[TMP_142]], %[[TMP_145]]
@@ -1620,7 +1620,7 @@
   // CHECK: %[[TMP_149:.*]] = stablehlo.constant dense<1.700000e+01>
   // CHECK: %[[TMP_150:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_149]]
   // CHECK: %[[TMP_151:.*]] = stablehlo.multiply %[[TMP_148]], %[[TMP_150]]
-  // CHECK: %[[TMP_152:.*]] = stablehlo.constant dense<-2.17486866E-16>
+  // CHECK: %[[TMP_152:.*]] = stablehlo.constant
   // CHECK: %[[TMP_153:.*]] = stablehlo.add %[[TMP_146]], %[[TMP_152]]
   // CHECK: %[[TMP_154:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_153]]
   // CHECK: %[[TMP_155:.*]] = stablehlo.multiply %[[TMP_151]], %[[TMP_154]]
@@ -1629,7 +1629,7 @@
   // CHECK: %[[TMP_158:.*]] = stablehlo.constant dense<1.500000e+01>
   // CHECK: %[[TMP_159:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_158]]
   // CHECK: %[[TMP_160:.*]] = stablehlo.multiply %[[TMP_157]], %[[TMP_159]]
-  // CHECK: %[[TMP_161:.*]] = stablehlo.constant dense<8.58606213E-15>
+  // CHECK: %[[TMP_161:.*]] = stablehlo.constant
   // CHECK: %[[TMP_162:.*]] = stablehlo.add %[[TMP_155]], %[[TMP_161]]
   // CHECK: %[[TMP_163:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_162]]
   // CHECK: %[[TMP_164:.*]] = stablehlo.multiply %[[TMP_160]], %[[TMP_163]]
@@ -1638,7 +1638,7 @@
   // CHECK: %[[TMP_167:.*]] = stablehlo.constant dense<1.300000e+01>
   // CHECK: %[[TMP_168:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_167]]
   // CHECK: %[[TMP_169:.*]] = stablehlo.multiply %[[TMP_166]], %[[TMP_168]]
-  // CHECK: %[[TMP_170:.*]] = stablehlo.constant dense<-3.3896803E-13>
+  // CHECK: %[[TMP_170:.*]] = stablehlo.constant
   // CHECK: %[[TMP_171:.*]] = stablehlo.add %[[TMP_164]], %[[TMP_170]]
   // CHECK: %[[TMP_172:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_171]]
   // CHECK: %[[TMP_173:.*]] = stablehlo.multiply %[[TMP_169]], %[[TMP_172]]
@@ -1647,7 +1647,7 @@
   // CHECK: %[[TMP_176:.*]] = stablehlo.constant dense<1.100000e+01>
   // CHECK: %[[TMP_177:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_176]]
   // CHECK: %[[TMP_178:.*]] = stablehlo.multiply %[[TMP_175]], %[[TMP_177]]
-  // CHECK: %[[TMP_179:.*]] = stablehlo.constant dense<1.33825364E-11>
+  // CHECK: %[[TMP_179:.*]] = stablehlo.constant
   // CHECK: %[[TMP_180:.*]] = stablehlo.add %[[TMP_173]], %[[TMP_179]]
   // CHECK: %[[TMP_181:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_180]]
   // CHECK: %[[TMP_182:.*]] = stablehlo.multiply %[[TMP_178]], %[[TMP_181]]
@@ -1656,7 +1656,7 @@
   // CHECK: %[[TMP_185:.*]] = stablehlo.constant dense<9.000000e+00>
   // CHECK: %[[TMP_186:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_185]]
   // CHECK: %[[TMP_187:.*]] = stablehlo.multiply %[[TMP_184]], %[[TMP_186]]
-  // CHECK: %[[TMP_188:.*]] = stablehlo.constant dense<-5.28419031E-10>
+  // CHECK: %[[TMP_188:.*]] = stablehlo.constant
   // CHECK: %[[TMP_189:.*]] = stablehlo.add %[[TMP_182]], %[[TMP_188]]
   // CHECK: %[[TMP_190:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_189]]
   // CHECK: %[[TMP_191:.*]] = stablehlo.multiply %[[TMP_187]], %[[TMP_190]]
@@ -1665,7 +1665,7 @@
   // CHECK: %[[TMP_194:.*]] = stablehlo.constant dense<7.000000e+00>
   // CHECK: %[[TMP_195:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_194]]
   // CHECK: %[[TMP_196:.*]] = stablehlo.multiply %[[TMP_193]], %[[TMP_195]]
-  // CHECK: %[[TMP_197:.*]] = stablehlo.constant dense<2.08767563E-8>
+  // CHECK: %[[TMP_197:.*]] = stablehlo.constant
   // CHECK: %[[TMP_198:.*]] = stablehlo.add %[[TMP_191]], %[[TMP_197]]
   // CHECK: %[[TMP_199:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_198]]
   // CHECK: %[[TMP_200:.*]] = stablehlo.multiply %[[TMP_196]], %[[TMP_199]]
@@ -1674,7 +1674,7 @@
   // CHECK: %[[TMP_203:.*]] = stablehlo.constant dense<5.000000e+00>
   // CHECK: %[[TMP_204:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_203]]
   // CHECK: %[[TMP_205:.*]] = stablehlo.multiply %[[TMP_202]], %[[TMP_204]]
-  // CHECK: %[[TMP_206:.*]] = stablehlo.constant dense<-8.26719599E-7>
+  // CHECK: %[[TMP_206:.*]] = stablehlo.constant
   // CHECK: %[[TMP_207:.*]] = stablehlo.add %[[TMP_200]], %[[TMP_206]]
   // CHECK: %[[TMP_208:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_207]]
   // CHECK: %[[TMP_209:.*]] = stablehlo.multiply %[[TMP_205]], %[[TMP_208]]
@@ -1683,7 +1683,7 @@
   // CHECK: %[[TMP_212:.*]] = stablehlo.constant dense<3.000000e+00>
   // CHECK: %[[TMP_213:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_212]]
   // CHECK: %[[TMP_214:.*]] = stablehlo.multiply %[[TMP_211]], %[[TMP_213]]
-  // CHECK: %[[TMP_215:.*]] = stablehlo.constant dense<3.30687835E-5>
+  // CHECK: %[[TMP_215:.*]] = stablehlo.constant
   // CHECK: %[[TMP_216:.*]] = stablehlo.add %[[TMP_209]], %[[TMP_215]]
   // CHECK: %[[TMP_217:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_216]]
   // CHECK: %[[TMP_218:.*]] = stablehlo.multiply %[[TMP_214]], %[[TMP_217]]
@@ -1692,13 +1692,13 @@
   // CHECK: %[[TMP_221:.*]] = stablehlo.constant dense<1.000000e+00>
   // CHECK: %[[TMP_222:.*]] = stablehlo.add %[[TMP_5]], %[[TMP_221]]
   // CHECK: %[[TMP_223:.*]] = stablehlo.multiply %[[TMP_220]], %[[TMP_222]]
-  // CHECK: %[[TMP_224:.*]] = stablehlo.constant dense<-0.00138888892>
+  // CHECK: %[[TMP_224:.*]] = stablehlo.constant
   // CHECK: %[[TMP_225:.*]] = stablehlo.add %[[TMP_218]], %[[TMP_224]]
   // CHECK: %[[TMP_226:.*]] = stablehlo.multiply %[[TMP_128]], %[[TMP_225]]
   // CHECK: %[[TMP_227:.*]] = stablehlo.multiply %[[TMP_223]], %[[TMP_226]]
   // CHECK: %[[TMP_228:.*]] = stablehlo.constant dense<5.000000e-01>
   // CHECK: %[[TMP_229:.*]] = stablehlo.divide %[[TMP_5]], %[[TMP_121]]
-  // CHECK: %[[TMP_230:.*]] = stablehlo.constant dense<0.0833333358>
+  // CHECK: %[[TMP_230:.*]] = stablehlo.constant
   // CHECK: %[[TMP_231:.*]] = stablehlo.add %[[TMP_230]], %[[TMP_227]]
   // CHECK: %[[TMP_232:.*]] = stablehlo.multiply %[[TMP_229]], %[[TMP_231]]
   // CHECK: %[[TMP_233:.*]] = stablehlo.add %[[TMP_228]], %[[TMP_232]]
@@ -1707,11 +1707,11 @@
   // CHECK: %[[TMP_236:.*]] = stablehlo.add %[[TMP_235]], %[[TMP_234]]
   // CHECK: %[[TMP_237:.*]] = stablehlo.abs %[[TMP_122]]
   // CHECK: %[[TMP_238:.*]] = stablehlo.abs %[[TMP_120]]
-  // CHECK: %[[TMP_239:.*]] = stablehlo.constant dense<1.401300e-45>
+  // CHECK: %[[TMP_239:.*]] = stablehlo.constant
   // CHECK: %[[TMP_240:.*]] = stablehlo.multiply %[[TMP_238]], %[[TMP_239]]
   // CHECK: %[[TMP_241:.*]] = stablehlo.compare LT, %[[TMP_237]], %[[TMP_240]], NOTYPE
   // CHECK: %[[TMP_242:.*]] = stablehlo.select %[[TMP_241]], %[[TMP_120]], %[[TMP_236]]
-  // CHECK: %[[TMP_243:.*]] = stablehlo.constant dense<0x7FC00000>
+  // CHECK: %[[TMP_243:.*]] = stablehlo.constant
   // CHECK: %[[TMP_244:.*]] = stablehlo.compare LT, %[[TMP_5]], %[[TMP_123]], NOTYPE
   // CHECK: %[[TMP_245:.*]] = stablehlo.select %[[TMP_244]], %[[TMP_243]], %[[TMP_242]]
   // CHECK: %[[TMP_246:.*]] = stablehlo.compare LE, %[[ARG1]], %[[TMP_90]], NOTYPE
@@ -1719,7 +1719,7 @@
   // CHECK: %[[TMP_248:.*]] = stablehlo.compare NE, %[[TMP_5]], %[[TMP_247]], NOTYPE
   // CHECK: %[[TMP_249:.*]] = stablehlo.and %[[TMP_246]], %[[TMP_248]]
   // CHECK: %[[TMP_250:.*]] = stablehlo.select %[[TMP_249]], %[[TMP_243]], %[[TMP_245]]
-  // CHECK: %[[TMP_251:.*]] = stablehlo.constant dense<0x7F800000>
+  // CHECK: %[[TMP_251:.*]] = stablehlo.constant
   // CHECK: %[[TMP_252:.*]] = stablehlo.floor %[[ARG1]]
   // CHECK: %[[TMP_253:.*]] = stablehlo.compare EQ, %[[ARG1]], %[[TMP_252]], NOTYPE
   // CHECK: %[[TMP_254:.*]] = stablehlo.and %[[TMP_246]], %[[TMP_253]]
@@ -1744,8 +1744,8 @@
   // CHECK: %[[TMP_273:.*]] = stablehlo.subtract %[[ARG1]], %[[TMP_272]]
   // CHECK: %[[TMP_274:.*]] = stablehlo.select %[[TMP_270]], %[[TMP_271]], %[[TMP_273]]
   // CHECK-DAG: %[[TMP_275:.*]] = stablehlo.constant dense<0.000000e+00>
-  // CHECK-DAG: %[[TMP_276:.*]] = stablehlo.constant dense<1.000000e+00>
-  // CHECK-DAG: %[[TMP_277:.*]] = stablehlo.constant dense<676.520386>
+  // CHECK-DAG: %[[TMP_276:.*]] = stablehlo.constant
+  // CHECK-DAG: %[[TMP_277:.*]] = stablehlo.constant
   // CHECK-DAG: %[[TMP_278:.*]] = stablehlo.constant dense<1.000000e+00>
   // CHECK: %[[TMP_279:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_278]]
   // CHECK: %[[TMP_280:.*]] = stablehlo.multiply %[[TMP_279]], %[[TMP_279]]
@@ -1753,7 +1753,7 @@
   // CHECK: %[[TMP_282:.*]] = stablehlo.subtract %[[TMP_275]], %[[TMP_281]]
   // CHECK: %[[TMP_283:.*]] = stablehlo.divide %[[TMP_277]], %[[TMP_279]]
   // CHECK: %[[TMP_284:.*]] = stablehlo.add %[[TMP_276]], %[[TMP_283]]
-  // CHECK-DAG: %[[TMP_285:.*]] = stablehlo.constant dense<-1259.13916>
+  // CHECK-DAG: %[[TMP_285:.*]] = stablehlo.constant
   // CHECK-DAG: %[[TMP_286:.*]] = stablehlo.constant dense<2.000000e+00>
   // CHECK: %[[TMP_287:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_286]]
   // CHECK: %[[TMP_288:.*]] = stablehlo.multiply %[[TMP_287]], %[[TMP_287]]
@@ -1761,7 +1761,7 @@
   // CHECK: %[[TMP_290:.*]] = stablehlo.subtract %[[TMP_282]], %[[TMP_289]]
   // CHECK: %[[TMP_291:.*]] = stablehlo.divide %[[TMP_285]], %[[TMP_287]]
   // CHECK: %[[TMP_292:.*]] = stablehlo.add %[[TMP_284]], %[[TMP_291]]
-  // CHECK-DAG: %[[TMP_293:.*]] = stablehlo.constant dense<771.323425>
+  // CHECK-DAG: %[[TMP_293:.*]] = stablehlo.constant
   // CHECK-DAG: %[[TMP_294:.*]] = stablehlo.constant dense<3.000000e+00>
   // CHECK: %[[TMP_295:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_294]]
   // CHECK: %[[TMP_296:.*]] = stablehlo.multiply %[[TMP_295]], %[[TMP_295]]
@@ -1769,15 +1769,15 @@
   // CHECK: %[[TMP_298:.*]] = stablehlo.subtract %[[TMP_290]], %[[TMP_297]]
   // CHECK: %[[TMP_299:.*]] = stablehlo.divide %[[TMP_293]], %[[TMP_295]]
   // CHECK: %[[TMP_300:.*]] = stablehlo.add %[[TMP_292]], %[[TMP_299]]
-  // CHECK-DAG: %[[TMP_301:.*]] = stablehlo.constant dense<-176.615036>
-  // CHECK-DAG: %[[TMP_302:.*]] = stablehlo.constant dense<4.000000e+00>
+  // CHECK-DAG: %[[TMP_301:.*]] = stablehlo.constant
+  // CHECK-DAG: %[[TMP_302:.*]] = stablehlo.constant
   // CHECK: %[[TMP_303:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_302]]
   // CHECK: %[[TMP_304:.*]] = stablehlo.multiply %[[TMP_303]], %[[TMP_303]]
   // CHECK: %[[TMP_305:.*]] = stablehlo.divide %[[TMP_301]], %[[TMP_304]]
   // CHECK: %[[TMP_306:.*]] = stablehlo.subtract %[[TMP_298]], %[[TMP_305]]
   // CHECK: %[[TMP_307:.*]] = stablehlo.divide %[[TMP_301]], %[[TMP_303]]
   // CHECK: %[[TMP_308:.*]] = stablehlo.add %[[TMP_300]], %[[TMP_307]]
-  // CHECK-DAG: %[[TMP_309:.*]] = stablehlo.constant dense<12.5073433>
+  // CHECK-DAG: %[[TMP_309:.*]] = stablehlo.constant
   // CHECK-DAG: %[[TMP_310:.*]] = stablehlo.constant dense<5.000000e+00>
   // CHECK: %[[TMP_311:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_310]]
   // CHECK: %[[TMP_312:.*]] = stablehlo.multiply %[[TMP_311]], %[[TMP_311]]
@@ -1785,7 +1785,7 @@
   // CHECK: %[[TMP_314:.*]] = stablehlo.subtract %[[TMP_306]], %[[TMP_313]]
   // CHECK: %[[TMP_315:.*]] = stablehlo.divide %[[TMP_309]], %[[TMP_311]]
   // CHECK: %[[TMP_316:.*]] = stablehlo.add %[[TMP_308]], %[[TMP_315]]
-  // CHECK-DAG: %[[TMP_317:.*]] = stablehlo.constant dense<-0.138571098>
+  // CHECK-DAG: %[[TMP_317:.*]] = stablehlo.constant
   // CHECK-DAG: %[[TMP_318:.*]] = stablehlo.constant dense<6.000000e+00>
   // CHECK: %[[TMP_319:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_318]]
   // CHECK: %[[TMP_320:.*]] = stablehlo.multiply %[[TMP_319]], %[[TMP_319]]
@@ -1793,7 +1793,7 @@
   // CHECK: %[[TMP_322:.*]] = stablehlo.subtract %[[TMP_314]], %[[TMP_321]]
   // CHECK: %[[TMP_323:.*]] = stablehlo.divide %[[TMP_317]], %[[TMP_319]]
   // CHECK: %[[TMP_324:.*]] = stablehlo.add %[[TMP_316]], %[[TMP_323]]
-  // CHECK-DAG: %[[TMP_325:.*]] = stablehlo.constant dense<9.98436917E-6>
+  // CHECK-DAG: %[[TMP_325:.*]] = stablehlo.constant
   // CHECK-DAG: %[[TMP_326:.*]] = stablehlo.constant dense<7.000000e+00>
   // CHECK: %[[TMP_327:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_326]]
   // CHECK: %[[TMP_328:.*]] = stablehlo.multiply %[[TMP_327]], %[[TMP_327]]
@@ -1801,7 +1801,7 @@
   // CHECK: %[[TMP_330:.*]] = stablehlo.subtract %[[TMP_322]], %[[TMP_329]]
   // CHECK: %[[TMP_331:.*]] = stablehlo.divide %[[TMP_325]], %[[TMP_327]]
   // CHECK: %[[TMP_332:.*]] = stablehlo.add %[[TMP_324]], %[[TMP_331]]
-  // CHECK-DAG: %[[TMP_333:.*]] = stablehlo.constant dense<1.50563267E-7>
+  // CHECK-DAG: %[[TMP_333:.*]] = stablehlo.constant
   // CHECK-DAG: %[[TMP_334:.*]] = stablehlo.constant dense<8.000000e+00>
   // CHECK: %[[TMP_335:.*]] = stablehlo.add %[[TMP_274]], %[[TMP_334]]
   // CHECK: %[[TMP_336:.*]] = stablehlo.multiply %[[TMP_335]], %[[TMP_335]]
@@ -1811,7 +1811,7 @@
   // CHECK: %[[TMP_340:.*]] = stablehlo.add %[[TMP_332]], %[[TMP_339]]
   // CHECK: %[[TMP_341:.*]] = stablehlo.constant dense<7.500000e+00>
   // CHECK: %[[TMP_342:.*]] = stablehlo.add %[[TMP_341]], %[[TMP_274]]
-  // CHECK: %[[TMP_343:.*]] = stablehlo.constant dense<2.01490307>
+  // CHECK: %[[TMP_343:.*]] = stablehlo.constant
   // CHECK: %[[TMP_344:.*]] = stablehlo.divide %[[TMP_274]], %[[TMP_341]]
   // CHECK: %[[TMP_345:.*]] = stablehlo.log_plus_one %[[TMP_344]]
   // CHECK: %[[TMP_346:.*]] = stablehlo.add %[[TMP_343]], %[[TMP_345]]
@@ -1825,7 +1825,7 @@
   // CHECK: %[[TMP_354:.*]] = stablehlo.floor %[[TMP_353]]
   // CHECK: %[[TMP_355:.*]] = stablehlo.abs %[[TMP_354]]
   // CHECK: %[[TMP_356:.*]] = stablehlo.add %[[ARG1]], %[[TMP_355]]
-  // CHECK: %[[TMP_357:.*]] = stablehlo.constant dense<3.14159274>
+  // CHECK: %[[TMP_357:.*]] = stablehlo.constant
   // CHECK: %[[TMP_358:.*]] = stablehlo.multiply %[[TMP_357]], %[[TMP_356]]
   // CHECK: %[[TMP_359:.*]] = stablehlo.cosine %[[TMP_358]]
   // CHECK: %[[TMP_360:.*]] = stablehlo.sine %[[TMP_358]]
@@ -1837,14 +1837,14 @@
   // CHECK: %[[TMP_366:.*]] = stablehlo.floor %[[ARG1]]
   // CHECK: %[[TMP_367:.*]] = stablehlo.compare EQ, %[[ARG1]], %[[TMP_366]], NOTYPE
   // CHECK: %[[TMP_368:.*]] = stablehlo.and %[[TMP_365]], %[[TMP_367]]
-  // CHECK: %[[TMP_369:.*]] = stablehlo.constant dense<0x7FC00000>
+  // CHECK: %[[TMP_369:.*]] = stablehlo.constant
   // CHECK: %[[TMP_370:.*]] = stablehlo.select %[[TMP_368]], %[[TMP_369]], %[[TMP_364]]
   // CHECK: %[[TMP_371:.*]] = stablehlo.select %[[TMP_268]], %[[TMP_370]], %[[TMP_266]]
   // CHECK: %[[TMP_372:.*]] = stablehlo.floor %[[ARG0]]
   // CHECK: %[[TMP_373:.*]] = stablehlo.compare NE, %[[ARG0]], %[[TMP_372]], NOTYPE
   // CHECK: %[[TMP_374:.*]] = stablehlo.compare LT, %[[ARG0]], %[[TMP_267]], NOTYPE
   // CHECK: %[[TMP_375:.*]] = stablehlo.or %[[TMP_373]], %[[TMP_374]]
-  // CHECK: %[[TMP_376:.*]] = stablehlo.constant dense<0x7FC00000>
+  // CHECK: %[[TMP_376:.*]] = stablehlo.constant
   // CHECK: %[[TMP_377:.*]] = stablehlo.select %[[TMP_375]], %[[TMP_376]], %[[TMP_371]]
   %1 = chlo.polygamma %lhs, %rhs : tensor<f32>, tensor<f32> -> tensor<f32>
   func.return %1 : tensor<f32>
@@ -1852,7 +1852,7 @@
 
 // -----
 
-// CHECK: @polygamma_f64
+// CHECK-LABEL: @polygamma_f64
 // CHECK-SAME: (%[[ARG0:.*]]: tensor<f64>, %[[ARG1:.*]]: tensor<f64>)
 func.func @polygamma_f64(%lhs : tensor<f64>, %rhs : tensor<f64>) -> tensor<f64> {
   // CHECK-DAG: %[[TMP_0:.*]] = stablehlo.constant dense<1.000000e+00>
@@ -1979,8 +1979,8 @@
   // CHECK: %[[TMP_121:.*]] = stablehlo.add %[[TMP_118]], %[[TMP_91]]
   // CHECK: %[[TMP_122:.*]] = stablehlo.power %[[TMP_121]], %[[TMP_92]]
   // CHECK: %[[TMP_123:.*]] = stablehlo.constant dense<1.000000e+00>
-  // CHECK: %[[TMP_124:.*]] = stablehlo.multiply %[[TMP_122]], %[[TMP_121]]
-  // CHECK: %[[TMP_125:.*]] = stablehlo.subtract %[[TMP_5]], %[[TMP_123]]
+  // CHECK-DAG: %[[TMP_124:.*]] = stablehlo.multiply %[[TMP_122]], %[[TMP_121]]
+  // CHECK-DAG: %[[TMP_125:.*]] = stablehlo.subtract %[[TMP_5]], %[[TMP_123]]
   // CHECK: %[[TMP_126:.*]] = stablehlo.divide %[[TMP_124]], %[[TMP_125]]
   // CHECK: %[[TMP_127:.*]] = stablehlo.multiply %[[TMP_121]], %[[TMP_121]]
   // CHECK: %[[TMP_128:.*]] = stablehlo.divide %[[TMP_91]], %[[TMP_127]]
@@ -2492,11 +2492,11 @@
 // CHECK-SAME: (%[[ARG:.*]]: tensor<16x16xf32>)
 func.func @top_k(%arg : tensor<16x16xf32>) -> (tensor<16x8xf32>, tensor<16x8xi32>) {
   // CHECK:      %[[IOTA:.*]] = stablehlo.iota dim = 1 : tensor<16x16xi32>
-  // CHECK-NEXT: %[[SORT:.*]]:2 = "stablehlo.sort"(%[[ARG]], %[[IOTA]]) ({
+  // CHECK-NEXT: %[[SORT:.*]]:2 = "stablehlo.sort"(%[[ARG]], %[[IOTA]]) <{dimension = 1 : i64, is_stable = true}> ({
   // CHECK-NEXT: ^{{.*}}(%[[LHS:.*]]: tensor<f32>, %[[RHS:.*]]: tensor<f32>, %{{.*}}: tensor<i32>, %{{.*}}: tensor<i32>):
   // CHECK-NEXT:   %[[CMP:.*]] = stablehlo.compare GT, %[[LHS]], %[[RHS]], TOTALORDER
   // CHECK-NEXT:   stablehlo.return %[[CMP]]
-  // CHECK-NEXT: }) {dimension = 1 : i64, is_stable = true} : (tensor<16x16xf32>, tensor<16x16xi32>) -> (tensor<16x16xf32>, tensor<16x16xi32>)
+  // CHECK-NEXT: }) : (tensor<16x16xf32>, tensor<16x16xi32>) -> (tensor<16x16xf32>, tensor<16x16xi32>)
   // CHECK-NEXT: %[[VAL:.*]] = stablehlo.slice %[[SORT]]#0 [0:16, 0:8] : (tensor<16x16xf32>) -> tensor<16x8xf32>
   // CHECK-NEXT: %[[IDX:.*]] = stablehlo.slice %[[SORT]]#1 [0:16, 0:8] : (tensor<16x16xi32>) -> tensor<16x8xi32>
   // CHECK-NEXT: return %[[VAL]], %[[IDX]]
@@ -2521,11 +2521,11 @@
   // CHECK-NEXT: [[K_I32x1:%.*]] = stablehlo.reshape [[K_I32]] : (tensor<i32>) -> tensor<1xi32>
   // CHECK-NEXT: [[RESULT_SHAPE:%.*]] = stablehlo.concatenate [[DIM_0_I32x1]], [[DIM_1_I32x1]], [[K_I32x1]], dim = 0 : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<3xi32>
   // CHECK-NEXT: [[IOTA:%.*]] = stablehlo.dynamic_iota [[IOTA_SHAPE]], dim = 2 : (tensor<3xi32>) -> tensor<?x5x?xi32>
-  // CHECK-NEXT: [[SORT:%.*]]:2 = "stablehlo.sort"([[ARG]], [[IOTA]]) ({
+  // CHECK-NEXT: [[SORT:%.*]]:2 = "stablehlo.sort"([[ARG]], [[IOTA]]) <{dimension = 2 : i64, is_stable = true}> ({
   // CHECK-NEXT: ^bb0([[ARG_1:%.*]]: tensor<i1>, [[ARG_2:%.*]]: tensor<i1>, [[ARG_3:%.*]]: tensor<i32>, [[ARG_4:%.*]]: tensor<i32>):
   // CHECK-NEXT:   [[CMP:%.*]] = stablehlo.compare  GT, [[ARG_1]], [[ARG_2]],  NOTYPE : (tensor<i1>, tensor<i1>) -> tensor<i1>
   // CHECK-NEXT:   stablehlo.return [[CMP]] : tensor<i1>
-  // CHECK-NEXT: }) {dimension = 2 : i64, is_stable = true} : (tensor<?x5x?xi1>, tensor<?x5x?xi32>) -> (tensor<?x5x?xi1>, tensor<?x5x?xi32>)
+  // CHECK-NEXT: }) : (tensor<?x5x?xi1>, tensor<?x5x?xi32>) -> (tensor<?x5x?xi1>, tensor<?x5x?xi32>)
   // CHECK-NEXT: [[STARTS:%.*]] = stablehlo.constant dense<0> : tensor<3xi64>
   // CHECK-NEXT: [[LIMITS:%.*]] = stablehlo.convert [[RESULT_SHAPE]] : (tensor<3xi32>) -> tensor<3xi64>
   // CHECK-NEXT: [[STRIDES:%.*]] = stablehlo.constant dense<1> : tensor<3xi64>
diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo.mlir b/stablehlo/stablehlo/tests/ops_stablehlo.mlir
--- stablehlo/stablehlo/tests/ops_stablehlo.mlir
+++ stablehlo/stablehlo/tests/ops_stablehlo.mlir
@@ -2014,7 +2014,7 @@
 
 // CHECK-LABEL: func @rng_normal
 func.func @rng_normal(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<2x3x5xf32> {
-  %cst = "stablehlo.constant"() {value = dense<[2, 3, 5]> : tensor<3xi64>} : () -> tensor<3xi64>
+  %cst = stablehlo.constant dense<[2, 3, 5]> : tensor<3xi64>
   %0 = "stablehlo.rng"(%arg0, %arg1, %cst) {rng_distribution = #stablehlo<rng_distribution NORMAL>}: (tensor<f32>, tensor<f32>, tensor<3xi64>) -> tensor<2x3x5xf32>
   func.return %0 : tensor<2x3x5xf32>
 }
@@ -2030,7 +2030,7 @@
 // -----
 
 func.func @rng_normal_invalid_shape(%arg0: tensor<f32>, %arg1: tensor<f32>) {
-  %cst = "stablehlo.constant"() {value = dense<7> : tensor<1xi64>} : () -> tensor<1xi64>
+  %cst = stablehlo.constant dense<7> : tensor<1xi64>
   // expected-error@+2 {{failed to infer returned types}}
   // expected-error @+1 {{inferred type(s) 'tensor<7xf32>' are incompatible with return type(s) of operation 'tensor<12xf32>'}}
   %0 = "stablehlo.rng"(%arg0, %arg1, %cst) {rng_distribution = #stablehlo<rng_distribution NORMAL>}: (tensor<f32>, tensor<f32>, tensor<1xi64>) -> tensor<12xf32>
@@ -2067,7 +2067,7 @@
 // -----
 
 func.func @rng_normal_invalid_type(%arg0: tensor<complex<f32>>, %arg1: tensor<f32>) {
-  %cst = "stablehlo.constant"() {value = dense<7> : tensor<1xi64>} : () -> tensor<1xi64>
+  %cst = stablehlo.constant dense<7> : tensor<1xi64>
   // expected-error @+1 {{#0 must be 0D tensor of pred (AKA boolean or 1-bit integer) or 4/8/16/32/64-bit signless integer or 4/8/16/32/64-bit unsigned integer or f8E4M3B11FNUZ type or f8E4M3FN type or f8E4M3FNUZ type or f8E5M2 type or f8E5M2FNUZ type or 16-bit float or 32-bit float or 64-bit float or bfloat16 type values, but got 'tensor<complex<f32>>'}}
   %0 = "stablehlo.rng"(%arg0, %arg1, %cst) {rng_distribution = #stablehlo<rng_distribution NORMAL>}: (tensor<complex<f32>>, tensor<f32>, tensor<1xi64>) -> tensor<7xf32>
   func.return
@@ -2691,7 +2691,7 @@
 // CHECK-LABEL: func @constants
 func.func @constants() -> () {
   // CHECK: stablehlo.constant dense<0> : tensor<i32>
-  %0 = "stablehlo.constant"() {value = dense<0> : tensor<i32>} : () -> (tensor<i32>)
+  %0 = "stablehlo.constant"() <{value = dense<0> : tensor<i32>}> : () -> (tensor<i32>)
 
   // CHECK: stablehlo.constant {extra_attr = 3 : i32} dense<0> : tensor<i32>
   %1 = "stablehlo.constant"() {extra_attr = 3 : i32, value = dense<0> : tensor<i32>} : () -> (tensor<i32>)
@@ -2703,7 +2703,7 @@
 func.func @constant_invalid() -> () {
   // expected-error@+2 {{failed to infer returned types}}
   // expected-error@+1 {{'stablehlo.constant' op inferred type(s) 'tensor<i32>' are incompatible with return type(s) of operation 'tensor<3xi32>'}}
-  %0 = "stablehlo.constant"() {value = dense<0> : tensor<i32>} : () -> (tensor<3xi32>)
+  %0 = "stablehlo.constant"() <{value = dense<0> : tensor<i32>}> : () -> (tensor<3xi32>)
   func.return
 }
 
@@ -2711,7 +2711,7 @@
 
 func.func @constant_invalid() -> () {
   // expected-error@+1 {{op result #0 must be statically shaped tensor}}
-  %0 = "stablehlo.constant"() {value = dense<1> : tensor<i32>} : () -> tensor<?xi32>
+  %0 = "stablehlo.constant"() <{value = dense<1> : tensor<i32>}> : () -> tensor<?xi32>
   func.return
 }
 
@@ -2719,7 +2719,7 @@
 
 func.func @constant_invalid() -> () {
   // expected-error@+1 {{elements literal type must have static shape}}
-  %0 = "stablehlo.constant"() {value = dense<1> : tensor<?xi32>} : () -> tensor<?xi32>
+  %0 = "stablehlo.constant"() <{value = dense<1> : tensor<?xi32>}> : () -> tensor<?xi32>
   func.return
 }
 
@@ -4872,7 +4872,7 @@
   %3 = stablehlo.uniform_quantize %2 : (tensor<2xf32>) -> tensor<2x!quant.uniform<i8:f32, 2.0:15>>
   %4 = stablehlo.uniform_quantize %1 : (tensor<2xf32>) -> tensor<2x!quant.uniform<ui8:f32, 34.0:16>>
   func.return %0, %4, %3 : tensor<2x!quant.uniform<i8:f32, 2.0:15>>, tensor<2x!quant.uniform<ui8:f32, 34.0:16>>, tensor<2x!quant.uniform<i8:f32, 2.0:15>>
-  // CHECK: stablehlo.constant() {value = dense<[1, 2]> : tensor<2xi8>} : () -> tensor<2x!quant.uniform<i8:f32, 2.000000e+00:15>>
+  // CHECK: stablehlo.constant() <{value = dense<[1, 2]> : tensor<2xi8>}> : () -> tensor<2x!quant.uniform<i8:f32, 2.000000e+00:15>>
   // CHECK-NEXT: stablehlo.constant dense<[1.000000e+01, 1.200000e+01]> : tensor<2xf32>
   // CHECK-NEXT: stablehlo.constant dense<[3.000000e+00, 1.000000e+02]> : tensor<2xf32>
 }
diff --ruN a/stablehlo/stablehlo/tests/print_stablehlo.mlir b/stablehlo/stablehlo/tests/print_stablehlo.mlir
--- stablehlo/stablehlo/tests/print_stablehlo.mlir
+++ stablehlo/stablehlo/tests/print_stablehlo.mlir
@@ -59,6 +59,20 @@
 func.func @zero_output_ret0(%arg0 : tensor<3xi64>) -> () {
   // CHECK:     stablehlo.return
   "stablehlo.return"() : () -> ()
+}
+
+func.func @constants() -> () {
+  // CHECK:      %c = stablehlo.constant dense<-1> : tensor<1xi64>
+  // CHECK-NEXT: %c_0 = stablehlo.constant {attr = 1 : i32} dense<[-2, 4]> : tensor<2xi64>
+  // CHECK-NEXT: %cst = stablehlo.constant() <{value = dense<[1, 2]> : tensor<2xi8>}> : () -> tensor<2x!quant.uniform<i8:f32, 2.000000e+00:15>>
+  // CHECK-NEXT: %cst_1 = stablehlo.constant() <{value = dense<3> : tensor<1xi8>}> : () -> tensor<1x!quant.uniform<i8:f32, 2.000000e+00:15>>
+  // CHECK-NEXT: %cst_2 = stablehlo.constant() <{value = dense<4> : tensor<1xi8>}> {attr = 1 : i32} : () -> tensor<1x!quant.uniform<i8:f32, 2.000000e+00:15>>
+  %cst = "stablehlo.constant"() <{value = dense<[-1]> : tensor<1xi64>}> : () -> tensor<1xi64>
+  %cst_attrs = "stablehlo.constant"() <{value = dense<[-2, 4]> : tensor<2xi64>}> {attr = 1 : i32} : () -> tensor<2xi64>
+  %cst_q = "stablehlo.constant"() {value = dense<[1, 2]> : tensor<2xi8>} : () -> tensor<2x!quant.uniform<i8:f32, 2.000000e+00:15>>
+  %cst_q_attr = stablehlo.constant() {value = dense<[3]> : tensor<1xi8>} : () -> tensor<1x!quant.uniform<i8:f32, 2.000000e+00:15>>
+  %cst_q_attrs = stablehlo.constant() {value = dense<[4]> : tensor<1xi8>, attr = 1 : i32} : () -> tensor<1x!quant.uniform<i8:f32, 2.000000e+00:15>>
+  return
 }
 
 // CHECK-LABEL: func @unary_ops
diff --ruN a/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/tests/stablehlo_canonicalize_dynamism.mlir
@@ -272,7 +272,7 @@
 // CHECK-LABEL: @dynamic_gather_success_static_result_type
 func.func @dynamic_gather_success_static_result_type(%arg0 : tensor<2x4x9xi32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x8xi32> {
   //  CHECK-NOT: stablehlo.dynamic_gather
-  //      CHECK: "stablehlo.gather"(%arg0, %arg1) {
+  //      CHECK: "stablehlo.gather"(%arg0, %arg1) <{
   // CHECK-SAME:   dimension_numbers = #stablehlo.gather<
   // CHECK-SAME:     offset_dims = [2],
   // CHECK-SAME:     collapsed_slice_dims = [0, 1],
@@ -280,7 +280,7 @@
   // CHECK-SAME:     index_vector_dim = 2
   // CHECK-SAME:   >,
   // CHECK-SAME:   slice_sizes = array<i64: 1, 1, 8>
-  // CHECK-SAME: } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
+  // CHECK-SAME: }> : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x8xi32>
   %0 = stablehlo.constant dense<[1, 1, 8]> : tensor<3xi32>
   %1 = "stablehlo.dynamic_gather"(%arg0, %arg1, %0) {
     dimension_numbers = #stablehlo.gather<
@@ -298,7 +298,7 @@
 // CHECK-LABEL: @dynamic_gather_success_dynamic_result_type
 func.func @dynamic_gather_success_dynamic_result_type(%arg0 : tensor<2x4x9xi32>, %arg1 : tensor<1x5x2xi32>) -> tensor<1x5x?xi32> {
   //  CHECK-NOT: stablehlo.dynamic_gather
-  //      CHECK: "stablehlo.gather"(%arg0, %arg1) {
+  //      CHECK: "stablehlo.gather"(%arg0, %arg1) <{
   // CHECK-SAME:   dimension_numbers = #stablehlo.gather<
   // CHECK-SAME:     offset_dims = [2],
   // CHECK-SAME:     collapsed_slice_dims = [0, 1],
@@ -306,16 +306,16 @@
   // CHECK-SAME:     index_vector_dim = 2
   // CHECK-SAME:   >,
   // CHECK-SAME:   slice_sizes = array<i64: 1, 1, 8>
-  // CHECK-SAME: } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x?xi32>
+  // CHECK-SAME: }> : (tensor<2x4x9xi32>, tensor<1x5x2xi32>) -> tensor<1x5x?xi32>
   %0 = stablehlo.constant dense<[1, 1, 8]> : tensor<3xi32>
-  %1 = "stablehlo.dynamic_gather"(%arg0, %arg1, %0) {
+  %1 = "stablehlo.dynamic_gather"(%arg0, %arg1, %0) <{
     dimension_numbers = #stablehlo.gather<
       collapsed_slice_dims = [0, 1],
       index_vector_dim = 2,
       offset_dims = [2],
       start_index_map = [0, 1]
     >
-  } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x?xi32>
+  }> : (tensor<2x4x9xi32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x?xi32>
   return %1 : tensor<1x5x?xi32>
 }
 
@@ -324,14 +324,14 @@
 // CHECK-LABEL: @dynamic_gather_inapplicable_dynamic_slice_sizes
 func.func @dynamic_gather_inapplicable_dynamic_slice_sizes(%arg0 : tensor<2x4x9xi32>, %arg1 : tensor<1x5x2xi32>, %arg2 : tensor<3xi32>) -> tensor<1x5x8xi32> {
   // CHECK: stablehlo.dynamic_gather
-  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) {
+  %0 = "stablehlo.dynamic_gather"(%arg0, %arg1, %arg2) <{
     dimension_numbers = #stablehlo.gather<
       collapsed_slice_dims = [0, 1],
       index_vector_dim = 2,
       offset_dims = [2],
       start_index_map = [0, 1]
     >
-  } : (tensor<2x4x9xi32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x8xi32>
+  }> : (tensor<2x4x9xi32>, tensor<1x5x2xi32>, tensor<3xi32>) -> tensor<1x5x8xi32>
   return %0 : tensor<1x5x8xi32>
 }
 
diff --ruN a/stablehlo/stablehlo/transforms/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/StablehloAggressiveSimplification.cpp
--- stablehlo/stablehlo/transforms/StablehloAggressiveSimplification.cpp
+++ stablehlo/stablehlo/transforms/StablehloAggressiveSimplification.cpp
@@ -771,9 +771,9 @@
       newInitVals.push_back(op.getOperand(i + numOperandPairs));
     }
 
-    auto newOp =
-        rewriter.create<ReduceOp>(op.getLoc(), newInputs, newInitVals,
-                                  op.getDimensionsAttr(), newElementTypes);
+    auto newOp = rewriter.create<ReduceOp>(
+        op.getLoc(), newInputs, newInitVals,
+        op.getDimensionsAttr().cast<DenseI64ArrayAttr>(), newElementTypes);
     Block *newReducerBlock = rewriter.createBlock(&newOp.getBody());
 
     IRMapping mapper;

