# Deploying code intelligence services

Most of the code intelligence logic lives inside of the [enterprise frontend](https://github.com/sourcegraph/sourcegraph/tree/main/enterprise/cmd/frontend), [precise-code-intel-worker](https://github.com/sourcegraph/sourcegraph/tree/main/enterprise/cmd/precise-code-intel-worker), and [executor-queue](https://github.com/sourcegraph/sourcegraph/tree/main/enterprise/cmd/executor-queue) services. These services deploy with the rest of the enterprise instance via [docker](https://github.com/sourcegraph/deploy-sourcegraph-docker), [docker-compose](https://github.com/sourcegraph/deploy-sourcegraph-docker/tree/master/docker-compose), or [Kubernetes configuration](https://github.com/sourcegraph/deploy-sourcegraph).

The [executor](https://github.com/sourcegraph/sourcegraph/tree/main/enterprise/cmd/executor) service, which runs user-supplied code to produce and upload precise code intel indexes, is deployed directly onto compute nodes in its own GCP project. This services requires certain Linux kernel extensions to operate, which are not available within a Kubernetes cluster. The [deployment](https://github.com/sourcegraph/infrastructure/tree/main/code-intel) for this service is managed through terraform.
